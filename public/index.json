[
{
	"uri": "/",
	"title": "Amazon Sagemaker Workshop",
	"tags": [],
	"description": "",
	"content": " Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow to label and prepare your data, choose an algorithm, train the model, tune and optimize it for deployment, make predictions, and take action. Your models get to production faster with much less effort and lower cost.\n"
},
{
	"uri": "/personalize/lite/introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Prerequisites AWS Account User with administrator access to the AWS Account Process First you will deploy a CloudFormation template that will do the following: Create an S3 bucket for all of your data storage. Create a SageMaker Notebook Instance for you to complete the workshop. Create the IAM policies needed for your notebook. Clone this repository into the notebook so you are ready to work. Open the notebook and follow the instructions below. "
},
{
	"uri": "/cleanup/sagemaker.html",
	"title": "SageMaker Resources",
	"tags": [],
	"description": "",
	"content": "To avoid charges for resources you no longer need when you\u0026rsquo;re done with this workshop, you can delete them or, in the case of your notebook instance, stop them. Here are the resources you should consider:\nEndpoints: these are the clusters of one or more instances serving inferences from your models. If you did not delete them from within a notebook, you can delete them via the SageMaker console. To do so:\nClick the Endpoints link in the left panel.\nThen, for each endpoint, click the radio button next to it, then select Delete from the Actions drop down menu.\nYou can follow a similar procedure to delete the related Models and Endpoint configurations.\nNotebook instance: you have two options if you do not want to keep the notebook instance running. If you would like to save it for later, you can stop rather than deleting it.\nTo stop a notebook instance: click the Notebook instances link in the left pane of the SageMaker console home page. Next, click the Stop link under the \u0026lsquo;Actions\u0026rsquo; column to the left of your notebook instance\u0026rsquo;s name. After the notebook instance is stopped, you can start it again by clicking the Start link. Keep in mind that if you stop rather than delete it, you will be charged for the storage associated with it.\nTo delete a notebook instance: first stop it per the instruction above. Next, click the radio button next to your notebook instance, then select Delete from the Actions drop down menu.\nS3 Bucket: if you retain the S3 bucket created for this workshop, you will be charged for storage. To avoid these charges if you no longer wish to use the bucket, you may delete it. To delete the bucket, go to the S3 service console, and locate your bucket\u0026rsquo;s name in the bucket table. Next, click in the bucket table row for your bucket to highlight the table row. At the top of the table, the Delete Bucket button should now be enabled, so click it and then click the Confirm button in the resulting pop-up to complete the deletion.\n"
},
{
	"uri": "/security_for_sysops/scenario.html",
	"title": "Scenario",
	"tags": [],
	"description": "",
	"content": "You are a member of a Cloud Platform Engineering team that has been tasked with enabling your business\u0026rsquo;s data scientists to deliver machine learning-based projects that are trained on highly sensitive company and customer data. The project teams are constrained by shared on-premise resources so you have been tasked with determining how the business can leverage the cloud to provision environments for the data science teams. The environment must be secure, protecting the sensitive data, while also enabling the data science teams to self-service.\nDuring this series of labs, you will be creating a secure environment for a team of data scientists with self-service tools to manage the environment and deliver an ML project.\nThere are three roles involved across these labs:\nCloud Platform Engineering - responsible for managing the cloud environments Project Administrators - responsible for managing resources to support the data science teams and their projects Data Scientist - a member of a project team tasked with delivering an ML or data science project These 3 roles will work together to create a secure cloud environment with appropriate guard rails, provision a secure data science environment, and deliver an ML project working with sensitive data. You will start as a member of the cloud platform engineering team to define a secure mechanism for delivering resources on demand at the request of the project administration team. Then, as a project administrator, you will use this mechanism to provide the data science team with a way to request the tools they need to deliver their project. Finally, as a member of the data science team, you will use the mechanisms provided to self-service and provision a Jupyter Notebook server. Using that server you will then develop and train a model while exploring the security controls in the data science environment.\nTo do this, the roles will work together to configure environments and iterate to improve the security posture across 5 labs.\nLab 1: Deploy the base infrastructure\nAs the cloud platform engineer, create a shared service VPC to host a PyPI mirror for approved Python packages and an AWS Service Catalog portfolio to provide a self-service mechanism to the project administration team.\nLab 2: Deploy the a project team\u0026rsquo;s resources\nAs a project administrator, deploy a project-specific data science environment that protects against data exfiltration using a VPC with no Internet connectivity. Restrict access to this environment using an IAM role for the data science project team, and provide a project-specific self-service mechanism using a Service Catalog portfolio so the data scientists can obtain just-in-time on-demand resources.\nLab 3: Deploy an Amazon SageMaker notebook\nAs a data scientist, use your project\u0026rsquo;s Service Catalog to deploy an Amazon SageMaker notebook.\nLab 4: Create a training job in line with security policy\nAs a data scientist, observe your training job\u0026rsquo;s performance as security controls respond to incorrect configuration parameters.\nLab 5: Improve security controls\nAs the project administrator, alter the IAM policies governing the data science environment to deliver preventive controls to guard your sensitive data.\nNext, let\u0026rsquo;s review the tools you\u0026rsquo;ll need to complete these labs.\n"
},
{
	"uri": "/security_for_users/scenario.html",
	"title": "Scenario",
	"tags": [],
	"description": "",
	"content": "You are a data scientist or ML engineer who works at a company that wishes to enable their data scientists to deliver machine learning-based projects that are trained on highly sensitive company data. The project teams are constrained by shared on-premise resources so your sysops admins have created all the infrastructure-as-code templates needed to provision a secure environment, which protects the sensitive data while also enabling the data science teams to self-service.\nYou as a data scientist or engineer will need to quickly set this environment up for yourself, so you can start working on exploring your data, training, deploying and monitoring your models. The key takeaways of this workshop are:\nCompute and Network Isolation\nDeploy Amazon SageMaker in a secure, customer-managed VPC.\nAuthentication and Authorization\nProvide single user access to Jupyter over IAM.\nArtifact Management\nEnable private Git integration, lifecycle config, and versioning.\nData Encryption\nEncrypt data in motion and at rest across the entire ML workflow.\nTraceability and Auditability\nTrace model lineage, and audit all API calls and data events.\nExplainability and Interpretability\nExplain predictions with feature importance and SHAP values.\nReal-time Model Monitoring\nMonitor the performance of a productionized model.\nReproducibility\nReproduce the model and results based on saved artifacts.\nIn these notebooks you will see some recommended practices on how to implement these requirements using Amazon SageMaker. Note that while these are recommended practices and guidelines, the information included in these notebooks is for illustrative purposes only. Nothing in this notebook is intended to provide you legal, compliance, or regulatory guidance.\nThe specific features and functionalities that you will become familiar with are:\nImporting custom libraries using pip without having public internet connectivity\nTraining a model with and without VPC and implementation of preventative controls to avoid training without VPC.\nImporting networking configurations, KMS keys directly in the notebook without data scientist having to know what they are.\nUsing SageMaker Processing to run scikit-learn data pre-processing jobs in Python.\nUsing SageMaker training on spot instances to save on cost\nModel Explainability using SHAP\nPushing/pulling code to private AWS CodeCommit Repository\nDeploying a trained model and monitoring it for data drift with SageMaker ModelMonitor\nSecurely running training, processing jobs using KMS keys to ensure encryption at rest and PrivateLink to support encryption in transit.\nUsing SageMaker Experiments to maintain lineage and traceability of model artifacts.\nIn the following labs you will quickly provision a collection of mechanisms and guard rails to enable you, as a project team member, to provisoin infrastructure and tools to support your project. You will then work through the machine learning lifecycle in the secure environment to see how project teams can be enabled to work in an agile manner at speed.\n"
},
{
	"uri": "/custom/code.html",
	"title": "Submit custom code",
	"tags": [],
	"description": "",
	"content": "In this section, you will train a neural network locally on the location from where this notebook is run (typically the SageMaker Notebook instance) using MXNet. You will then see how to create an endpoint from the trained MXNet model and deploy it on SageMaker. You will then inference from the newly created SageMaker endpoint. For this section, you\u0026rsquo;ll be using the MNIST dataset.\nRunning the notebook Download the mxnet_mnist_byom.zip file. This archive contains the Python script to train your model and the Jupyter notebook to step through the process. Unzip this on your local environment.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;mxnet-mnist-byom\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;mxnet_mnist_byom.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name. Repeat this step for the \u0026lsquo;mnist.py\u0026rsquo; and the \u0026lsquo;input.html\u0026rsquo; files.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "/builtin/xgboost.html",
	"title": "Video Game Sales Prediction with XGBoost",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, we\u0026rsquo;ll use SageMaker\u0026rsquo;s version of XGBoost, a popular and efficient open-source implementation of the gradient boosted trees algorithm.\nGradient boosting is a supervised learning algorithm that attempts to predict a target variable by combining the estimates of a set of simpler, weaker models. XGBoost has done remarkably well in machine learning competitions because it robustly handles a wide variety of data types, relationships, and distributions. It often is a useful, go-to algorithm in working with structured data, such as data that might be found in relational databases and flat files.\nThis section also shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console.\nExploratory Data Analysis Download the video-game-sales-xgboost.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;video-game-sales\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;video-game-sales-xgboost.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;your_s3_bucket_name_here\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;your_s3_bucket_name_here\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. When it is time to set up a training job, return from the notebook to these instructions.\nModel Training Now that you have your data in S3, you can begin training a model. You\u0026rsquo;ll use SageMaker\u0026rsquo;s built-in version of the XGBoost algorithm, and the AWS CLI to run the training job. XGBoost has many tunable hyperparameters. Some of these hyperparameters are listed below; initially we\u0026rsquo;ll only use a few of them. Many of the hyperparameters are used to prevent overfitting, which prevents a model from generalizing to new observations. max_depth: Maximum depth of a tree. As a cautionary note, a value too small could underfit the data, while increasing it will make the model more complex and thus more likely to overfit the data (in other words, the classic bias-variance tradeoff).\neta: Step size shrinkage used in updates to prevent overfitting.\neval_metric: Evaluation metric(s) for validation data. For data sets such as this one with imbalanced classes, we\u0026rsquo;ll use the AUC metric.\nscale_pos_weight: Controls the balance of positive and negative weights, again useful for data sets having imbalanced classes.\nYou\u0026rsquo;ll be using the AWS CLI and a Bash script to run the training job. Using the AWS CLI and scripts is an excellent way to automate machine learning pipelines and repetitive tasks, such as periodic training jobs. As a reminder, in the Introduction module we recommended the use of AWS Cloud 9 for access to the AWS CLI and Bash environments. If you haven\u0026rsquo;t done so already, please set up and open your Cloud9 environment now as described in Cloud9 Setup. Below is a screenshot of what your Cloud9 environment should look like as you create the first script below and run the related commands.\nCreate a text file named videogames.sh. If you haven\u0026rsquo;t done so already, open a terminal/command window that supports Bash to enter commands. In the terminal window, change to the directory in which you created the file (if you\u0026rsquo;re not already there), then run the following command:\nchmod +x videogames.sh Paste the bash script below into the videogames.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets.\narn_role: To get the value for this variable, go to the SageMaker console, click Notebook instances in the left pane, then in the \u0026lsquo;Notebook instances\u0026rsquo; table, click the name of the instance you created for this workshop. In the Notebook instance settings section, look for the \u0026lsquo;IAM role ARN\u0026rsquo; value, and copy its text. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: select one of the following, depending on the AWS Region where you are running this workshop.\nN. Virginia: 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest Oregon: 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest Ohio: 825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest Ireland: 685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest bucket: the name of the S3 bucket you used in your notebook. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/videogames_xgboost training_job_name=videogames-xgboost-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=1,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,ContentType=libsvm ChannelName=validation,DataSource=$eval_source,CompressionType=None,ContentType=libsvm \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters max_depth=3,eta=0.1,eval_metric=auc,scale_pos_weight=2.0,subsample=0.5,objective=binary:logistic,num_round=100 \\ --stopping-condition MaxRuntimeInSeconds=1800 In your terminal window, run the following command to start the training job. Total job duration may last up to about 5 minutes, including time for setting up the training cluster. In case the training job encounters problems and is stuck, you can set a stopping condition that times out, in this case after a half hour. ``` ./videogames.sh ``` In the SageMaker console, click Jobs in the left panel to check the status of the training job. When the job is complete, its Status column will change from InProgress to Complete. As a reminder, duration of this job can last up to about 5 minutes, including time for setting up the training cluster. \u0026gt; To check the actual training time (not including cluster setup) for a job when it is complete, click the training job name in the jobs table, then examine the **Training time** listed at the top right under **Job Settings**. Model Creation Now that you\u0026rsquo;ve trained your machine learning model, you\u0026rsquo;ll want to make predictions by setting up a hosted endpoint for it. The first step in doing that is to create a SageMaker model object that wraps the actual model artifact from training. To create the model object, you will point to the model.tar.gz that came from training and the inference code container, then create the hosting model object. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console home page, right click the Models link and open it in another tab of your browser. Click the Create Model button at the upper right above the \u0026lsquo;Models\u0026rsquo; table.\nFor the \u0026lsquo;Model name\u0026rsquo; field under Model Settings, enter videogames-xgboost.\nFor the \u0026lsquo;Location of inference code image\u0026rsquo; field under Primary Container, enter the name of the same Docker image you specified previously for the region where you\u0026rsquo;re running this workshop. For ease of reference, here are the image names again:\nN. Virginia: 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest Oregon: 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest Ohio: 825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest Ireland: 685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest For the \u0026lsquo;Location of model artifacts\u0026rsquo; field under Primary Container, enter the path to the output of your replicated training job. To find the path, go back to your first browser tab, click Jobs in the left pane, then find and click the replicated job name, which will look like videogames-xgboost-\u0026lt;date\u0026gt;. Scroll down to the Outputs section, then copy the path under \u0026lsquo;S3 model artifact\u0026rsquo;. Paste the path in the field; it should look like s3://smworkshop-john-smith/sagemaker/videogames_xgboost/videogames-xgboost-2018-04-17-20-40-13/output/model.tar.gz .\nClick Create model at the bottom of the page.\nModel Serving and Evaluation Once you\u0026rsquo;ve created your model, you can configure what your hosting endpoint should be. Here you specify the EC2 instance type to use for hosting, the initial number of instances, and your hosting model name. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoint configuration. Click the Create endpoint configuration button at the upper right above the \u0026lsquo;Endpoint configuration\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint configuration name\u0026rsquo; field under New endpoint configuration, enter videogames-xgboost.\nUnder Production variants, click Add model. From the Add model popup, select the videogames-xgboost model you created earlier, and click Save. Then click Create endpoint configuration at the bottom of the page.\nFor the final step in the process of setting up an endpoint, you\u0026rsquo;ll use the SageMaker console to do so (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoints. Click the Create endpoint button at the upper right above the \u0026lsquo;Endpoints\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint name\u0026rsquo; field under Endpoint, enter videogames-xgboost.\nUnder Attach endpoint configuration, leave \u0026lsquo;Use an existing endpoint configuration\u0026rsquo; selected, then under Endpoint configuration, select videogames-xgboost from the table, then click Select endpoint configuration at the bottom of the table. Then click Create endpoint at the bottom of the page.\nIn the Endpoints table, refer to the \u0026lsquo;Status\u0026rsquo; column, and wait for the endpoint status to change from \u0026lsquo;Creating\u0026rsquo; to \u0026lsquo;InService\u0026rsquo; before proceeding to the next step. It will take several minutes for endpoint creation, possibly as long as ten minutes.\nTo evaluate predictions from your model, let\u0026rsquo;s return to the notebook you used earlier. When you are finished, return here and proceed to the next section. Conclusion \u0026amp; Extensions This XGBoost model is just the starting point for predicting whether a game will be a hit based on reviews and other features. There are several possible avenues for improving the model\u0026rsquo;s performance. First, of course, would be to collect more data and, if possible, fill in the existing missing fields with actual information. Another possibility is further hyperparameter tuning, with Amazon SageMaker\u0026rsquo;s Hyperparameter Optimization service. And, although ensemble learners often do well with imbalanced data sets, it could be worth exploring techniques for mitigating imbalances such as downsampling, synthetic data augmentation, and other approaches.\n"
},
{
	"uri": "/conclusion/conclusion.html",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\nCreated a Notebook for exploratory analysis Creaetd an ETL to prepare training data Trained the model with Hyperparameter Optimization Putted \u0026ldquo;new data\u0026rdquo; through a preprocessing pipeline to get it ready for prediction Automatized batch predictions for new data using a combination of CloudWatch, Step Functions, Lambda, Glue and SageMaker. "
},
{
	"uri": "/personalize/lite/building.html",
	"title": "Building Your Environment",
	"tags": [],
	"description": "",
	"content": "As mentioned above, the first step is to deploy a CloudFormation template that will perform much of the initial setup for you. In another browser window login to your AWS account. Once you have done that open the link below in a new tab to start the process of deploying the items you need via CloudFormation.\nLaunch Cloudformation\n"
},
{
	"uri": "/builtin/resnet.html",
	"title": "Image Classification with ResNet",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, you\u0026rsquo;ll use SageMaker\u0026rsquo;s image classification algorithm, a supervised learning algorithm that takes an image as input and classifies it into one of multiple output categories. It uses a convolutional neural network (ResNet) that can be trained from scratch, or trained using transfer learning when a large number of training images are not available. In this section you\u0026rsquo;ll train the image classification algorithm from scratch on the Caltech-256 dataset.\nRunning the notebook Download the image-classification-fulltraining.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;image-classification-resnet\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;image-classification-fulltraining.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "/personalize/full/overview.html",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "This lab will walk you through the following:\nDeploy and configure a Video Recommendation application Setting up a Jupyter Notebook environment for the Amazon Personalize Service Preview Downloading and preparing training data, based on the Movie Lens data set Importing prepared data into Amazon Personalize Building an ML model based upon the Hierarchical Recurrent Neural Network algorithm (HRNN) Testing your model by deploying an Amazon Personalize campaign Adding your campaign to Video Recommendation application "
},
{
	"uri": "/security_for_sysops/tools.html",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "To work through these labs you will need:\nAn AWS account\nWith privileges to create IAM roles, attach IAM policies, create AWS VPCs, configure Service Catalog, create Amazon S3 buckets, and work with Amazon SageMaker.\nAccess to the AWS web console\nMany of the instructions will guide you through working with the various service consoles.\nJupyter cheat sheet\nIf you are unfamiliar with the Jupyter notebook interface or its keybindings a cheat sheet may help you navigate.\nOptional: AWS CLI\nYou may want to have the AWS CLI available for working with AWS services like Amazon S3.\nNow, let\u0026rsquo;s get started!\n"
},
{
	"uri": "/security_for_users/tools.html",
	"title": "Tools &amp; Knowledge Check",
	"tags": [],
	"description": "",
	"content": "To work through these labs you will need:\nAn AWS account\nWith privileges to create IAM roles, attach IAM policies, create AWS VPCs, configure Service Catalog, create Amazon S3 buckets, and work with Amazon SageMaker.\nAccess to the AWS web console\nMany of the instructions will guide you through working with the various service consoles.\nTo get the most of these labs it will be beneficial if you have prior experience working with the following technologies:\nPython\nPython is a programming language that is popular in data science communities. It has been used in the labs to work with the AWS services and the data being used to train machine learning models.\nGit\nThe labs make use of Git to manage the work you will complete. Git is a distributed version control system and you will use a few simple commands to interact with a Git repository during the labs.\nSageMaker SDK\nSageMaker SDK is a high-level Python SDK wrapped around Boto3 and designed to provide a familiar interface to data science users.\nBoto3\nBoto3 is a low-level Python SDK for interacting with the AWS APIs. Documentation on its many great classes and functionality can be found online\nPandas\nThe notebooks use Pandas in many different places to load, export, and manipulate data. If you are unfamiliar with the Pandas library it may be helpful to review some of their Getting Started materials.\nscikit-learn\nscikit-learn is a popular open source framework for data science and machine learning.\nXGBoost\nxgboost is one of the most popular and performant gradient boosting algorithms for supervised learning tasks.\nJupyter\nJupyter is a popular open source interactive computing environment with a user friendly notebook interface.\nYou will use Jupyter notebooks to complete these labs. If you have not used Jupyter before you may find a Jupyter cheat sheet to be useful. The cheat sheet walks through navigation of the Jupyter interface and how to use a notebook.\nSageMaker Experiments\nSageMaker Experiments APIs can be used to manage and track the metadata for your training, pre-processing, hyperparameter tuning jobs.\nSageMaker Model Monitor\nSageMaker ModelMonitor can be used to detect data drift during inference time on the payload sent to your endpoint. It can be connected to CloudWatch to send an alarm or notification when violations are detected and users need to be alerted.\nSageMaker Processing\nSageMaker Processing can be used to run your scripts for pre-processing, feature engineering in a managed way where Amazon SageMaker sets up the underlying infrastructure needed to run your job at scale, tearing down the instances once the job is complete.\nNow, let\u0026rsquo;s get started!\n"
},
{
	"uri": "/builtin/rcf.html",
	"title": "Anomaly Detection with Random Cut Forest",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll work your way through a Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, you\u0026rsquo;ll use SageMaker\u0026rsquo;s Random Cut Forest (RCF) algorithm, an algorithm designed to detect anomalous data points within a dataset. Examples of when anomalies are important to detect include when website activity uncharacteristically spikes, when temperature data diverges from a periodic behaviour, or when changes to public transit ridership reflect the occurrence of a special event.\nIn this notebook, we will use the SageMaker RCF algorithm to train a model on the Numenta Anomaly Benchmark (NAB) NYC Taxi dataset which records the amount New York City taxi ridership over the course of six months. We will then use this model to predict anomalous events by emitting an \u0026ldquo;anomaly score\u0026rdquo; for each data point.\nRunning the notebook Access the SageMaker notebook instance you created earlier. Open the SageMaker Examples tab.\nIn the Introduction to Amazon Algorithms section locate the random_cut_forest.ipynb notebook and create a copy by clicking on Use.\nYou are now ready to begin the notebook.\nIn the bucket = '\u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;\u0026lt;bucket_name\u0026gt;\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "/personalize/lite/cloudformation.html",
	"title": "Cloud Formation Wizard",
	"tags": [],
	"description": "",
	"content": "Follow along with the screenshots if you have any questions about these steps.\nCloud Formation Wizard Start by clicking Next at the bottom like shown:\nIn the next page you need to provide a unique S3 bucket name for your file storage, it is recommended to simply add your first name and last name to the end of the default option as shown below, after that update click Next again.\nThis page is a bit longer so scroll to the bottom to click Next.\nAgain scroll to the bottom, check the box to enable the template to create new IAM resources and then click Create Stack.\nFor a few minutes CloudFormation will be creating the resources described above on your behalf it will look like this while it is provisioning:\nOnce it has completed you\u0026rsquo;ll see green text like below indicating that the work has been completed:\nNow that you have your environment created, you need to save the name of your S3 bucket for future use, you can find it by clicking on the Outputs tab and then looking for the resource S3Bucket, once you find it copy and paste it to a text file for the time being.\n"
},
{
	"uri": "/personalize/full/deploy.html",
	"title": "Deploy the App",
	"tags": [],
	"description": "",
	"content": "Deploy the \u0026ldquo;Video Recommendation\u0026rdquo; Application Whilst this application could be deployed anywhere, it uses both an EC2 Amazon Machine Image (AMI) and RDS Snapshot that have been stored in the North Virgina Region of AWS (us-east-1). Hence, please make sure that the Region selected in the AWS Console is alway US East (N.Virginia), as shown in the following diagram. The workshop will only function correctly if the EC2 configuration, CloudFormation template executiion and SageMaker notebook are all using this AWS Region.\nThe application will run on an EC2 instance, but at some point we will need to connect to the server in order to carry out some configuration task. To do this we need to have an EC2 Key Pair configured on the server that you also have access to on your computer; hence, we need to create and download a new one. Click on EC2 from the list of all services by entering EC2 into the Find services box. This will bring you to the Amazon EC2 console home page.\nOn the left-hand menu scroll down until you see Key Pairs and select it, and in the resulting dialog click on the Create Key Pair button. This will bring up a Create Key Pair dialog, where you need to enter the name of a new key pair - call it myLabKey and hit Create. This should automatically download the file, or you may need to manually do so.\nWe are going to deploy a pre-built application via a CloudFormation template - this will be a fully-functioning recommendation system, allowing access to multiple Amazon Personalize features. But it has one drawback - there are no models built into it! So we will create them in this lab, and when they are ready we will re-configure this application to use them. But first we need to deploy this skeleton application:\nFollow this link to create the resources\nCreate Cloudformation Stack\nThe next screen asks for more configuration parameters, but only two of these are required: Stack name and KeyName. For Stack name enter something simple, such as LabStack, and select your previously-defined EC2 kay-pair, the click Next (not shown).\nThere then follows two more screens. The first is called Options, but we have none to enter so just click on Next. The second is the final Review screen - please verify that the KeyName is the one that you just downloaded, and then click on Create.\nThis will then go and create the environment, which will take around 10-15 minutes minutes. Unfortunately, we are creating IAM resources, so we cannot continue until it has completed - so read ahead and get a feel for what\u0026rsquo;s coming up next.\n"
},
{
	"uri": "/personalize/full/setup.html",
	"title": "Setup your Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "Click on Amazon SageMaker from the list of all services by entering Sagemaker into the Find services box. This will bring you to the Amazon SageMaker console homepage.\nGo to the SageMaker menu on the left and choose \u0026ldquo;Notebook instances\u0026rdquo; under the \u0026ldquo;Notebook\u0026rdquo; option.\nWait until the notebook instance status is InService, then click on Open Jupyter - whilst you\u0026rsquo;re waiting you can perform step #1 of the next section to copy some files from Git\nNow you can click on the notebook .ipynb file and the lab notebook will open, and you can now begin to work through the lab notebook.\nWorking Through a Jupyter Notebook A Notebook consisted of a number of cells; in SageMaker these will typically either be Code or Markdown cells. Markdown is used to allow for documentation to be defined inline with the code, giving the author a rich set of markdown formatting options. The first cell in this notebook, which is called Get the Personalize boto3 Client, is Markdown, and if you select any cell then the whole cell is highlighted.\nThe first Markdown cell describes what the following Code cell is going to do â€“ for the sake of this lab you do not have to understand the code that is being run in the Code cell, rather you should just appreciate what the notebook is doing and how you interact with a Jupyter notebook.\nTo the left of a Code module is a set of empty braces [ ]. By highlighting the cell and then selecting the Run command in the menu bar, the Jupyter notebook will execute this code, outputting and code outputs to the notebook screen and keeping any results data internally for re-use in future steps. Do this now to execute the first code cell.\nNote: if a Markdown cell is highlighted, then clicking Run will move the highlight to the next cell\nWhilst the code is executing the braces will change to be [*], indicating that it is executing, and once complete will change to [1]. Future cells will have increasing numbers inside the braces, and this helps you see the order in which cells have been exected within the notebook. Directly below the code, but still within the Code cell, is the output from the code execution - this will include any error messages that your code has thrown. In this example, the code execution successfully created the specified bucket in S3.\nNow let\u0026rsquo;s continue to work through the notebook lab - read the comments prior to each Code cell in order to get an understanding as to what is going on, as these explain why we are doing each step and how it ties in to using the Amazon Personalize service.\n"
},
{
	"uri": "/custom/algo.html",
	"title": "Use your own custom algorithms",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll create your own training script using TensorFlow and the building blocks provided in tf.layers, which will predict the ages of abalones based on their physical measurements. It\u0026rsquo;s possible to estimate the age of an abalone (sea snail) by the number of rings on its shell. In this section, you\u0026rsquo;ll be using the UCI Abalone dataset.\nWriting Custom TensorFlow Model Training and Inference Code To train a model on Amazon SageMaker using custom TensorFlow code and deploy it on Amazon SageMaker, you need to implement training and inference code interfaces in your code.\nYour TensorFlow training script must be a Python 2.7 source file. The current default TensorFlow version is 1.6. This training/inference script must contain the following functions:\nmodel_fn: Defines the model that will be trained. train_input_fn: Preprocess and load training data. eval_input_fn: Preprocess and load evaluation data. serving_input_fn: Defines the features to be passed to the model during prediction. For more information, see TensorFlow Model Training Code in the Amazon SageMaker documentation.\nDefining the model The model_fn is a function that contains all the logic to support training, evaluation, and prediction. The basic skeleton for a model_fn looks like this:\ndef model_fn(features, labels, mode, hyperparameters): # Logic to do the following: # 1. Configure the model via TensorFlow operations # 2. Define the loss function for training/evaluation # 3. Define the training operation/optimizer # 4. Generate predictions # 5. Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) The model_fn function must accept four positional arguments:\nfeatures: A dict containing the features passed to the model via train_input_fn in training mode, via eval_input_fn in evaluation mode, and via serving_input_fn in predict mode. labels: A Tensor containing the labels passed to the model via train_input_fn in training mode and eval_input_fn in evaluation mode. It will be empty for predict mode. mode: One of the following tf.estimator.ModeKeys string values indicating the context in which the model_fn was invoked: TRAIN: the model_fn was invoked in training mode. EVAL: the model_fn was invoked in evaluation mode. PREDICT: the model_fn was invoked in predict mode. hyperparameters: The hyperparameters passed to SageMaker TrainingJob that runs your TensorFlow training script. You can use this to pass hyperparameters to your training script. The model_fn function must return a tf.estimator.EstimatorSpec.\nMore details on how to create a model_fn can be find in Constructing the model_fn.\nTraining and Evaluation The train_input_fn function is used to pass features and labels to the model_fn in training mode. The eval_input_fn function is used to features and labels to the model_fn in evaluation mode.\nThe basic skeleton for the train_input_fn looks like this:\ndef train_input_fn(training_dir, hyperparameters): # Logic to the following: # 1. Reads the **training** dataset files located in training_dir # 2. Preprocess the dataset # 3. Return 1) a dict of feature names to Tensors with # the corresponding feature data, and 2) a Tensor containing labels return features, labels An eval_input_fn follows the same format:\ndef eval_input_fn(training_dir, hyperparameters): # Logic to the following: # 1. Reads the **evaluation** dataset files located in training_dir # 2. Preprocess the dataset # 3. Return 1) a dict of feature names to Tensors with # the corresponding feature data, and 2) a Tensor containing labels return features, labels Note: For TensorFlow 1.4 and 1.5, train_input_fn and eval_input_fn may also return a no-argument function which returns the tuple features, labels. This is no longer supported for TensorFlow 1.6 and up.\nMore details on how to create input functions can be find in Building Input Functions with tf.estimator.\nServing the Model The serving_input_fn function is used to define the shapes and types of the inputs the model accepts when the model is exported for Tensorflow Serving. It is optional, but required for deploying the trained model to a SageMaker endpoint.\nThe serving_input_fn function is called at the end of model training and is not called during inference.\nThe basic skeleton for the serving_input_fn looks like this:\ndef serving_input_fn(hyperparameters): # Logic to the following: # 1. Defines placeholders that TensorFlow serving will feed with inference requests # 2. Preprocess input data # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver, # which packages the placeholders and the resulting feature Tensors together. Note: For TensorFlow 1.4 and 1.5, serving_input_fn may also return a no-argument function which returns a tf.estimator.export.ServingInputReceiver ortf.estimator.export.TensorServingInputReceiver. This is no longer supported for TensorFlow 1.6 and up.\nMore details on how to create a serving_input_fn can be find in Preparing serving inputs.\nRunning the notebook Download the tensorflow_abalone_age_predictor.zip file. This archive contains the Python script for your model, the Jupyter notebook to step through the process, and the UCI Abalone dataset.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;tensorflow-abalone-byom\u0026rsquo;.\nClick the folder to enter it.\nClick the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;tensorflow_abalone_age_predictor.zip\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nClick the New button on the right and select Terminal. In the terminal window, type the following commands to unzip the archive:\ncd SageMaker unzip tensorflow_abalone_age_predictor.zip Close the Terminal window. You are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nFollow the directions in the notebook. The notebook will walk you through the data preparation, training, hosting, and validating the model with Amazon SageMaker. Once completed, return from the notebook to these instructions to move to the next Module.\n"
},
{
	"uri": "/personalize/lite/agenda.html",
	"title": "Agenda",
	"tags": [],
	"description": "",
	"content": "The steps below outline the process of building your own recommendation model, improving it, and then cleaning up all of your resources to prevent any unwanted charges. To get started executing these follow the steps in the next section.\nPersonalize_BuildCampaign.ipynb - Guides you through building your first campaign and recommendation algorithm. View_Campaign_And_Interactions.ipynb - Showcase how to generate a recommendation and how to modify it with real time intent. Cleanup.ipynb - Deletes anything that was created so you are not charged for additional resources. Personalize Recipes HRNN - Hierarchical recurrent neural network (HRNN), which is able to model the changes in user behavior.\nHRNN-Metadata - Similar to the HRNN recipe with additional features derived from contextual, user, and item metadata (Interactions, Users, and Items datasets, respectively). Provides accuracy benefits over non-metadata models when high quality metadata is available.\nHRNN-Coldstart - Similar to the HRNN-Metadata recipe, while adding personalized exploration of new items. Use this recipe when you are frequently adding new items to the Items dataset and require the items to immediately appear in the recommendations. Popularity-Count - Popularity-count returns the top popular items from a dataset. A popular item is defined by the number of times it occurs in the dataset. The recipe returns the same popular items for all users.\nPersonalized-Ranking - Provides a user with a ranked list of items.\nSIMS - Leverages user-item interaction data to recommend items similar to a given item. In the absence of sufficient user behavior data for an item, this recipe recommends popular items.\n"
},
{
	"uri": "/personalize/full/parallel.html",
	"title": "Creating Parallel Solutions",
	"tags": [],
	"description": "",
	"content": "Create Item-to-Item Similarities Solution Using the same methods as before, go to the Services drop-down in the console and navigate to the Amazon Personalize service in another tab, and select Dataset groups. You will see the dataset group that you created earlier, and click on the name of your dataset group.\nThe left-hand side, which will show you the solution that you\u0026rsquo;re currently creating via your notebook. Then, select Solutions and recipes, then click on the Create solution button.\nEnter a suitable name for this solution, such as similar-items-solutions and choose the aws-sims recipe and click Next - we don\u0026rsquo;t need to change anything in the advanced configuration section\nIn the \u0026ldquo;Solutions overview\u0026rdquo; screen just hit the Finish button and a new solution version will start to be created.\nCreate Personal Ranking Solution Let\u0026rsquo;s do exactly the same thing again, but this time we\u0026rsquo;ll create a ranking solition. From the Solutions and Recipes screen that you are on, click Create solution, give it a name like rankings-solution but this time select the aws-personalized-ranking recipe. Click Next and Finished as before.\nYou now have three solutions being built off of the same dataset, and all three will slot into the application later. Please now go back to the notebook and continue to build your recommendation campaign and do some quick testing - if the notebook solution still hasn\u0026rsquo;t completed then you may continue with the first part of the next section.\n"
},
{
	"uri": "/builtin/parallelized.html",
	"title": "Parallelized Data Distribution",
	"tags": [],
	"description": "",
	"content": "SageMaker makes it easy to train machine learning models across a cluster containing a large number of machines. This a non-trivial process, but SageMaker\u0026rsquo;s built-in algorithms and pre-built MXNet and TensorFlow containers hide most of the complexity from you. Nevertheless, there are decisions about how to structure data that will have implications regarding how the distributed training is carried out.\nIn this section, you will learn about how to take full advantage of distributed training clusters when using one of SageMaker\u0026rsquo;s built-in algorithms. This section also shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console.\nExploratory Data Analysis Download the data_distribution_types.ipynb notebook.\nAccess the SageMaker notebook instance you created earlier. Click the New button on the right and select Folder.\nClick the checkbox next to your new folder, click the Rename button above in the menu bar, and give the folder a name such as \u0026lsquo;distributed-data\u0026rsquo;.\nClick the folder to enter it.\nTo upload the notebook, click the Upload button on the right. Then in the file selection popup, select the file \u0026lsquo;data_distribution_types.ipynb\u0026rsquo; from the folder on your computer where you downloadeded it earlier. Click the blue Upload button that appears to the right of the notebook\u0026rsquo;s file name.\nYou are now ready to begin the notebook. Click the notebook\u0026rsquo;s file name to open it.\nIn the bucket = '\u0026lt;your_s3_bucket_name_here\u0026gt;' code line, paste the name of the S3 bucket you created in Module 1 to replace \u0026lt;your_s3_bucket_name_here\u0026gt;. The code line should now read similar to bucket = 'smworkshop-john-smith'. Do NOT paste the entire path (s3://\u0026hellip;\u0026hellip;.), just the bucket name.\nIn this workshop, you also will be accessing a S3 bucket that holds data from one of the AWS Public Data Sets.\nIf you followed the Creating a Notebook Instance module to create your notebook instance, you should be able to access this S3 bucket. Otherwise, if you are using your own notebook instance created elsewhere, you may need to modify the associated IAM role to add permissions for s3:ListBucket for arn:aws:s3:::gdelt-open-data, and s3:GetObject for arn:aws:s3:::gdelt-open-data/*.\nFollow the directions in the notebook. When it is time to set up a training job, return from the notebook to these instructions. Model Training Now that we have our data in S3, you can begin training. You\u0026rsquo;ll use SageMaker\u0026rsquo;s built-in Linear Learner algorithm. Since the focus of this module is data distribution to a training cluster, you\u0026rsquo;ll fit two models in order to compare data distribution types: In the first job, you\u0026rsquo;ll use FullyReplicated for your train channel. This will pass every file in the input S3 location to every machine (in this case you\u0026rsquo;ll be using 5 machines).\nIn the second job, you\u0026rsquo;ll use ShardedByS3Key for the train channel (note that you\u0026rsquo;ll keep FullyReplicated for the validation channel). So, for the training data, you\u0026rsquo;ll pass each S3 object to a separate machine. Since there are 5 files in the dataset (one for each year), you\u0026rsquo;ll train on 5 machines, meaning each machine will get a year\u0026rsquo;s worth of records.\nYou\u0026rsquo;ll be using the AWS CLI and Bash scripts to run the training jobs. Using the AWS CLI and scripts is an excellent way to automate machine learning pipelines and repetitive tasks, such as periodic training jobs. If you haven\u0026rsquo;t done so already, please set up and open your Cloud9 environment now as described in Cloud9 Setup. Below is a screenshot of what your Cloud9 environment should look like as you create the first script below and run the related commands. Step-by-step instructions follow.\nCreate a text file named replicated.sh. If you haven\u0026rsquo;t done so already, open a terminal/command window that supports Bash to enter commands. In the terminal window, change to the directory in which you created the file (if you\u0026rsquo;re not already there), then run the following command:\nchmod +x replicated.sh Paste the bash script below into the replicated.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets.\narn_role: To get the value for this variable, go to the SageMaker console, click Notebook instances in the left pane, then in the \u0026lsquo;Notebook instances\u0026rsquo; table, click the name of the instance you created for this workshop. In the Notebook instance settings section, look for the \u0026lsquo;IAM role ARN\u0026rsquo; value, and copy its text. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: select one of the following, depending on the AWS Region where you are running this workshop.\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest bucket: the name of the S3 bucket you used in your notebook. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/data_distribution_types training_job_name=linear-replicated-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=5,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,RecordWrapperType=None ChannelName=validation,DataSource=$eval_source,CompressionType=None,RecordWrapperType=None \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters feature_dim=25,mini_batch_size=500,predictor_type=regressor,epochs=2,num_models=32,loss=absolute_loss \\ --stopping-condition MaxRuntimeInSeconds=1800 Save your file, then in your terminal window, run the following command to start the training job. Total job duration may last up to about 10 minutes, including time for setting up the training cluster. In case the training job encounters problems and is stuck, you can set a stopping condition that times out, in this case after a half hour. Now, since you can run another job concurrently with this one, move onto the next step after you start this job. ``` ./replicated.sh ``` For the next training job with the ShardedByS3Key distribution type, please create a text file named sharded.sh. then run the following command in your terminal window:\nchmod +x sharded.sh Paste the bash script below into the sharded.sh file, and then change the text in the angle brackets (\u0026lt; \u0026gt;) as follows. Do NOT put quotes around the values you insert, or retain the brackets. All four values to change are the same as the values you changed for the previous script; they are noted again below for your ease of reference.\narn_role: same as for the previous script. It should look like the following: arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20171211T211964.\ntraining_image: same as the previous script; the image depends on the AWS Region where you are running this workshop. They are shown again here for convenience:\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest bucket: same as for the previous script. It should look like: s3://smworkshop-john-smith.\nregion: the region code for the region where you are running this workshop, either us-east-1 for N. Virginia, us-west-2 for Oregon, us-east-2 for Ohio, or eu-west-1 for Ireland.\n# Fill in the values of these four variables arn_role=\u0026lt;arn-of-your-notebook-role\u0026gt; training_image=\u0026lt;training-image-for-region\u0026gt; bucket=\u0026lt;name-of-your-s3-bucket\u0026gt; region=\u0026lt;your-region\u0026gt; prefix=/sagemaker/data_distribution_types training_job_name=linear-sharded-`date \u0026#39;+%Y-%m-%d-%H-%M-%S\u0026#39;` training_data=$bucket$prefix/train eval_data=$bucket$prefix/validation train_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=ShardedByS3Key,S3Uri=$training_data}} eval_source={S3DataSource={S3DataType=S3Prefix,S3DataDistributionType=FullyReplicated,S3Uri=$eval_data}} aws --region $region \\ sagemaker create-training-job \\ --role-arn $arn_role \\ --training-job-name $training_job_name \\ --algorithm-specification TrainingImage=$training_image,TrainingInputMode=File \\ --resource-config InstanceCount=5,InstanceType=ml.c4.2xlarge,VolumeSizeInGB=10 \\ --input-data-config ChannelName=train,DataSource=$train_source,CompressionType=None,RecordWrapperType=None ChannelName=validation,DataSource=$eval_source,CompressionType=None,RecordWrapperType=None \\ --output-data-config S3OutputPath=$bucket$prefix \\ --hyper-parameters feature_dim=25,mini_batch_size=500,predictor_type=regressor,epochs=2,num_models=32,loss=absolute_loss \\ --stopping-condition MaxRuntimeInSeconds=1800 Save your file, then in your terminal window, run the following command to start your second training job now, there is no need to wait for the first training job to complete: ``` ./sharded.sh ``` In the SageMaker console, click Jobs in the left panel to check the status of the training jobs, which run concurrently. When they are complete, their Status column will change from InProgress to Complete. As a reminder, duration of these jobs can last up to about 10 minutes, including time for setting up the training cluster, as shown in the Duration column of the Jobs table. \u0026gt; To check the actual training time (not including cluster setup) for each job when both are complete, click the training job name in the jobs table, then examine the **Training duration** listed at the top right under **Job Settings**. **Training duration** does not include the time related to cluster setup. As you can see, and might expect, the sharded distribution type trained substantially faster than the fully replicated type. This is a key differentiator to consider when preparing data and picking the distribution type. Model Creation Now that you\u0026rsquo;ve trained your machine learning models, you\u0026rsquo;ll want to make predictions by setting up a hosted endpoint for them. The first step in doing that is to create a SageMaker model object that wraps the actual model artifact from training. To create the model object, you will point to the model.tar.gz that came from training and the inference code container, then create the hosting model object. You\u0026rsquo;ll do this twice, once for each model you trained earlier. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console home page, right click the Models link and open it in another tab of your browser. Click the Create Model button at the upper right above the \u0026lsquo;Models\u0026rsquo; table.\nFor the \u0026lsquo;Model name\u0026rsquo; field under Model Settings, enter distributed-replicated.\nFor the \u0026lsquo;Location of inference code image\u0026rsquo; field under Primary Container, enter the name of the same Docker image you specified previously for the region where you\u0026rsquo;re running this workshop. For ease of reference, here are the image names again:\nN. Virginia: 382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest Oregon: 174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest Ohio: 404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest Ireland: 438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest For the \u0026lsquo;Location of model artifacts\u0026rsquo; field under Primary Container, enter the path to the output of your replicated training job. To find the path, go back to your first browser tab, click Jobs in the left pane, then find and click the replicated job name, which will look like linear-replicated-\u0026lt;date\u0026gt;. Scroll down to the Outputs section, then copy the path under \u0026lsquo;S3 model artifact\u0026rsquo;. Paste the path in the field; it should look like s3://sagemaker-projects-pdx/sagemaker/data_distribution_types/linear-replicated-2018-03-11-18-13-13/output/model.tar.gz.\nClick Create model at the bottom of the page.\nRepeat the above steps for the sharded training job model, except: for \u0026lsquo;Model name\u0026rsquo;, enter distributed-sharded, and for \u0026lsquo;Location of model artifacts\u0026rsquo;, enter the path for the sharded training job model artifact.\nModel Serving and Evaluation Once you\u0026rsquo;ve setup your models, you can configure what your hosting endpoints should be. Here you specify the EC2 instance type to use for hosting, the initial number of instances, and the hosting model name. Again, you\u0026rsquo;ll do this twice, once for each model we trained earlier. Here are the steps to do this via the SageMaker console (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoint configuration. Click the Create endpoint configuration button at the upper right above the \u0026lsquo;Endpoint configuration\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint configuration name\u0026rsquo; field under New endpoint configuration, enter distributed-replicated.\nUnder Production variants, click Add model. From the Add model popup, select the distributed-replicated model you created earlier, and click Save. Then click Create endpoint configuration at the bottom of the page.\nRepeat the above steps for the sharded training job model, except: for \u0026lsquo;Endpoint configuration name\u0026rsquo;, enter distributed-sharded, and for the Add model popup, select the distributed-sharded model.\nFor the final step in the process of setting up endpoints, you\u0026rsquo;ll use the SageMaker console to do so (see screenshot below for an example of all relevant fields filled in for the Oregon AWS Region): In the left pane of the SageMaker console, click Endpoints. Click the Create endpoint button at the upper right above the \u0026lsquo;Endpoints\u0026rsquo; table.\nFor the \u0026lsquo;Endpoint name\u0026rsquo; field under Endpoint, enter distributed-replicated.\nUnder Attach endpoint configuration, leave \u0026lsquo;Use an existing endpoint configuration\u0026rsquo; selected, then under Endpoint configuration, select distributed-replicated from the table, then click Select endpoint configuration at the bottom of the table. Then click Create endpoint at the bottom of the page.\nRepeat the above steps, except: for \u0026lsquo;Endpoint name\u0026rsquo;, enter distributed-sharded, and for the Endpoint configuration table, select the distributed-sharded endpoint configuration.\nIn the Endpoints table, refer to the \u0026lsquo;Status\u0026rsquo; column, and wait for both endpoints to change from \u0026lsquo;Creating\u0026rsquo; to \u0026lsquo;InService\u0026rsquo; before proceeding to the next step. It will take several minutes for endpoint creation, possibly as long as ten minutes.\nTo compare predictions from your two models, let\u0026rsquo;s return to the notebook you used earlier (jump to the \u0026lsquo;Evaluate\u0026rsquo; section). When you are finished, return here and proceed to the next section. Conclusion \u0026amp; Extensions In this module, you ran a regression on a relatively artificial example, and skipped some pre-processing steps along the way (like potentially transforming or winsorizing our target variable, looking for interations in our features, etc.). But the main point was to highlight the difference in training time and accuracy of a model trained through two different distribution methods.\nOverall, sharding data into separate files and sending them to separate training nodes will run faster, but may produce lower accuracy than a model that replicates the data across all nodes. Naturally, this can be influenced by training the sharded model longer, with more epochs. And it should be noted that we trained with a very small number of epochs to highlight this difference.\nDifferent algorithms can be expected to show variation in which distribution mechanism is most effective at achieving optimal compute spend per point of model accuracy. The message remains the same though, that the process of finding the right distribution type is another experiment in optimizing model training times.\n"
},
{
	"uri": "/personalize/full/videorecommendation.html",
	"title": "Configure the Video Recommendation App",
	"tags": [],
	"description": "",
	"content": "Django only allows access via pre-defined source IP addresses. Naturally, these could be open to the internet, but they recommend only exposing it the instance private IP address (for internal calls) and to your front-end load balancer. You already have a reference to the private IP address, so you now need to extract the Load Balancer DNS entry. Go back to the EC2 console screen, but this time select Load Balancers on the left-hand menu; select your Application Load Balancer and in the details screen that comes up select the DNS name and store it for later.\n"
},
{
	"uri": "/introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Sagemaker and Step Functions In this workshop you will explore the development cycle of machine learning model on AWS. In the first part, you will find a sample project fully developed in an ml.m4.4xlarge SageMaker notebook instance. On purpose, the notebooks are divided in different stages\nExploratory analysis ETL to prepare training data Training the model with Hyperparameter Optimization Putting \u0026ldquo;new data\u0026rdquo; through a preprocessing pipeline to get it ready for prediction Batch predictions for new data In the second part of this workshop we will implement this project in production automatizing it\u0026rsquo;s execution using a combination of CloudWatch, Step Functions, Lambda, Glue and SageMaker.\n"
},
{
	"uri": "/custom/containers.html",
	"title": "Overview of containers for Amazon SageMaker",
	"tags": [],
	"description": "",
	"content": "SageMaker makes extensive use of Docker containers to allow users to train and deploy algorithms. Containers allow developers and data scientists to package software into standardized units that run consistently on any platform that supports Docker. Containerization packages code, runtime, system tools, system libraries and settings all in the same place, isolating it from its surroundings, and insuring a consistent runtime regardless of where it is being run.\nWhen you develop a model in Amazon SageMaker, you can provide separate Docker images for the training code and the inference code, or you can combine them into a single Docker image.\nAnatomy of an Amazon SageMaker container Using this powerful container environment, developers can deploy any kind of code in the Amazon SageMaker ecosystem. You can also create a logical division of labor by creating a deployment team, such as a DevOps, security, and infrastructure teams (who maintains the container) and a data scientists team (who focuses on producing models that are later added to the container).\nExample container folder The Bring Your Own scikit Algorithm example provides a detailed walkthrough on how to package a scikit-learn algorithm for training and production-ready hosting using containers. Let\u0026rsquo;s take a look at the container folder structure to explain how Amazon SageMaker runs Docker for training and hosting your own algorithm.\ncontainer/ Dockerfile build_and_push.sh decision_trees/ nginx.conf predictor.py serve train wsgi.py Let\u0026rsquo;s discuss each of these in turn:\nDockerfile describes how to build your Docker container image. More details below. build_and_push.sh is a script that uses the Dockerfile to build your container images and then pushes it to Amazon ECR. decision_trees is the directory which contains the files that will be installed in the container. In this simple application, only five files are installed in the container. You may only need that many or, if you have many supporting routines, you may wish to install more. These five show the standard structure of our Python containers, although you are free to choose a different toolset and therefore could have a different layout. If you\u0026rsquo;re writing in a different programming language, you\u0026rsquo;ll certainly have a different layout depending on the frameworks and tools you choose.\nThe files that will be installed in the container are:\nnginx.conf is the configuration file for the nginx front-end. predictor.py is the program that actually implements the Flask web server and the decision tree predictions for this app. serve is the program started when the container is started for hosting. It simply launches the gunicorn server which runs multiple instances of the Flask app defined in predictor.py. train is the program that is invoked when the container is run for training. wsgi.py is a small wrapper used to invoke the Flask app. The Dockerfile The Dockerfile describes the image that you want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations.\nFor the Python science stack, we will start from a standard Ubuntu installation and run the normal tools to install the things needed by scikit-learn. Finally, we add the code that implements our specific algorithm to the container and set up the right environment to run under.\nAlong the way, we clean up extra space. This makes the container smaller and faster to start.\nLet\u0026rsquo;s look at the Dockerfile for the example:\n# Build an image that can do training and inference in SageMaker # This is a Python 2 image that uses the nginx, gunicorn, flask stack # for serving inferences in a stable way. FROM ubuntu:16.04 MAINTAINER Amazon AI \u0026lt;sage-learner@amazon.com\u0026gt; RUN apt-get -y update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ wget \\ python \\ nginx \\ ca-certificates \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # Here we get all python packages. # There\u0026#39;s substantial overlap between scipy and numpy that we eliminate by # linking them together. Likewise, pip leaves the install caches populated which uses # a significant amount of space. These optimizations save a fair amount of space in the # image, which reduces start up time. RUN wget https://bootstrap.pypa.io/get-pip.py \u0026amp;\u0026amp; python get-pip.py \u0026amp;\u0026amp; \\ pip install numpy scipy scikit-learn pandas flask gevent gunicorn \u0026amp;\u0026amp; \\ (cd /usr/local/lib/python2.7/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .) \u0026amp;\u0026amp; \\ rm -rf /root/.cache # Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard # output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE # keeps Python from writing the .pyc files which are unnecessary in this case. We also update # PATH so that the train and serve programs are found when the container is invoked. ENV PYTHONUNBUFFERED=TRUE ENV PYTHONDONTWRITEBYTECODE=TRUE ENV PATH=\u0026#34;/opt/program:${PATH}\u0026#34; # Set up the program in the image COPY decision_trees /opt/program WORKDIR /opt/program Running a container for Amazon SageMaker training Amazon SageMaker invokes the training code by running a version of the following command:\ndocker run \u0026lt;image\u0026gt; train This means that your Docker image should have an executable file in it that is called train. You will modify this program to implement your training algorithm. This can be in any language that is capable of running inside of the Docker environment, but the most common language options for data scientists include Python, R, Scala, and Java. For our Scikit example, we use Python.\nAt runtime, Amazon SageMaker injects the training data from an Amazon S3 location into the container. The training program ideally should produce a model artifact. The artifact is written, inside of the container, then packaged into a compressed tar archive and pushed to an Amazon S3 location by Amazon SageMaker.\nWhen Amazon SageMaker runs training, your train script is run just like any regular program. A number of files are laid out for your use, under the /opt/ml directory:\n/opt/ml â”œâ”€â”€ input â”‚ â”œâ”€â”€ config â”‚ â”‚ â”œâ”€â”€ hyperparameters.json â”‚ â”‚ â””â”€â”€ resourceConfig.json â”‚ â””â”€â”€ data â”‚ â””â”€â”€ \u0026lt;channel_name\u0026gt; â”‚ â””â”€â”€ \u0026lt;input data\u0026gt; â”œâ”€â”€ model â”‚ â””â”€â”€ \u0026lt;model files\u0026gt; â””â”€â”€ output â””â”€â”€ failure The input /opt/ml/input/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u0026rsquo;t support distributed training, we\u0026rsquo;ll ignore it here. /opt/ml/input/data/\u0026lt;channel_name\u0026gt;/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u0026rsquo;s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. /opt/ml/input/data/\u0026lt;channel_name\u0026gt;_\u0026lt;epoch_number\u0026gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch. The output /opt/ml/model/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result. /opt/ml/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored. Running a container for SageMaker hosting Amazon SageMaker invokes hosting service by running a version of the following command\ndocker run \u0026lt;image\u0026gt; serve This launches a RESTful API to serve HTTP requests for inference. Again, this can be done in any language or framework that works within the Docker environment.\nIn most Amazon SageMaker containers, serve is simply a wrapper that starts the inference server. Furthermore, Amazon SageMaker injects the model artifact produced in training into the container and unarchives it automatically.\nAmazon SageMaker uses two URLs in the container:\n/ping will receive GET requests from the infrastructure. Your program returns 200 if the container is up and accepting requests. /invocations is the endpoint that receives client inference POST requests. The format of the request and the response is up to the algorithm. If the client supplied ContentType and Accept headers, these will be passed in as well. Storing SageMaker Containers For SageMaker to run a container for training or hosting, it needs to be able to find the image hosted in the image repository, Amazon Elastic Container Registry (Amazon ECR). The three main steps to this process are building locally, tagging with the repository location, and pushing the image to the repository.\nTo build the local image, call the following command:\ndocker build \u0026lt;image name\u0026gt; This takes instructions from the Dockerfile we discussed earlier to generate an image on your local instance. After the image is built, we need to let our local instance of Docker know where to store the image so that SageMaker can find it. We do this by tagging the image with the following command:\ndocker tag \u0026lt;image name\u0026gt; \u0026lt;repository name\u0026gt; The repository name has the following structure:\n\u0026lt;account number\u0026gt;.dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;image name\u0026gt;:\u0026lt;tag\u0026gt; Without tagging the image with the repository name, Docker defaults to uploading to Docker Hub, and not Amazon ECR. Amazon SageMaker currently requires Docker images to reside in Amazon ECR. To push an image to ECR, and not the central Docker registry, you must tag it with the registry hostname.\nUnlike Docker Hub, Amazon ECR images are private by default, which is a good practice with Amazon SageMaker. If you want to share your Amazon SageMaker images publicly, you can find more information in the Amazon ECR User Guide.\nFinally, to upload the image to Amazon ECR, with the Region set in the repository name tag, call the following command:\ndocker push \u0026lt;repository name\u0026gt; One final note on Amazon SageMaker Docker containers. We have already shown you that you have the option to build one Docker container serving both training and hosting, or you can build one for each. While building two Docker images can increase storage requirements and cost due to duplicated common libraries, you might get a benefit from building a significantly smaller inference container, allowing the hosting service to scale more quickly when reacting to traffic increases. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs. You need to consider the tradeoffs when you decide if you want to build a single container or two.\n"
},
{
	"uri": "/security_for_sysops/security_overview.html",
	"title": "Security Overview",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is a powerful enabler and a key component of a data science environment, but it\u0026rsquo;s only part of what is required to build a complete secure data science environment. For more robust security you will need other AWS services such as Amazon CloudWatch, Amazon S3, and AWS VPC. To begin, let\u0026rsquo;s talk about some of the key things you will want in place, working in concert, with Amazon SageMaker.\nPrivate Network Environment Let\u0026rsquo;s begin with your Virtual Private Cloud (VPC) which will be used to host Amazon SageMaker and other components of your data science environment. An AWS VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. You will begin this workshop by creating a shared services VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or to your shared services from other VPCs will be provided using VPC endpoints powered by PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nIn these labs you will create a VPC that:\nHas no IGW or NAT gateway attached Multiple subnets are defined across Availability Zones for resiliency VPC Endpoints are configured to grant explicit access to AWS services like Amazon SageMaker APIs, Amazon S3, STS, and CloudWatch Logs Security Groups are configured to govern IP traffic and grant access to VPC endpoints DNS Hostnames are enabled on the VPC to support VPC endpoint hostname resolution Authentication and Authorization AWS Identity and Access Management (IAM) can help you create preventive controls for many aspects of your data science enviroment. They can control access to your data in Amazon S3, control who can access SageMaker resources like Notebook servers, and even be applied as VPC endpoint policies to put explicit controls around the API endpoints you create in your data science environment.\nThere are several IAM roles you will need in order to manage permissions and ensure separation of concerns at scale. Those roles are:\nData scientist user role\nGranting Console access, permissions to start/stop a Jupyter notebook, permissions to open a Jupyter notebook\nNotebook creation role\nUsed by CI/CD pipeline or Service Catalog to create a Jupyter Notebook\nNotebook execution role\nUsed by a Jupyter Notebook to access AWS resources\nTraining / Transform job execution role\nUsed by Training Jobs or Batch Transform jobs to access AWS resources like Amazon S3\nEndpoint creation role\nUsed by your CI/CD pipeline to create hosted ML model endpoints\nEndpoint hosting role\nUsed by a hosted ML model to access AWS resources such as Amazon S3\nEndpoint invocation role\nUsed by an application to call a hosted ML model endpoint\nThere are also many IAM conditions you can apply in your policies to begin to grant powerful permissions but only under certain conditions. You will learn more about these in the labs.\nData Protection In a data science environment there is the highly sensitive data you are using to train your ML models, but there is also the sensitive intellectual property you are developing in the form of algorithms, libraries, and trained models. There are many ways to protect data such as the preventive controls described above, defined as IAM policies. In addition you have the ability to encrypt data at rest using managed encryption keys.\nMany AWS services, including Amazon S3 and Amazon SageMaker, are integrated with AWS Key Management Service (KMS) to make it very easy to encrypt your data at rest. You can take advantage of these integrations to ensure that your data is encrypted in the data lake AND in the data science environment, end to end. This encryption also applies to your intellectual property as it is being developed in the many places it may be stored such as Amazon S3, EC2 EBS volumes, or AWS CodeCommit git repository.\nAuditability Using cloud services in a safe and responsible manner is good, but being able to demonstrate to others that you are operating in a governed manner is even better. Developers and security officers alike will need to see activity logs for models being trained and persons interacting with the systems. Amazon CloudWatch Logs and CloudTrail are there to help, receiving logs from many different parts of your data science environment to include:\nAmazon S3 Amazon SageMaker Notebooks Amazon SageMaker Training Jobs Amazon SageMaker Hosted Models VPC Flow Logs Let\u0026rsquo;s now dive into implementing these key areas. You will start by creating a secure, private, network environment to host shared services along with a self-service mechanism to support project administrators.\n"
},
{
	"uri": "/security_for_users/security_overview.html",
	"title": "Security Overview",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is a powerful enabler and a key component of a data science environment, but it\u0026rsquo;s only part of what is required to build a complete secure data science environment. For more robust security you will need other AWS services such as Amazon CloudWatch, Amazon S3, and AWS VPC. To begin, let\u0026rsquo;s talk about some of the key things you will want in place, working in concert, with Amazon SageMaker.\nPrivate Network Environment Let\u0026rsquo;s begin with your Virtual Private Cloud (VPC) which will be used to host Amazon SageMaker and other components of your data science environment. Your VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. We will begin this workshop by creating a VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or your own shared services will be provided using VPC endpoints and PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nIn these labs you will create a VPC that:\nHas no IGW or NAT gateway attached Multiple subnets are defined across Availability Zones for resiliency VPC Endpoints are configured to grant explicit access to Amazon SageMaker APIs, Amazon S3, STS, and CloudWatch Logs Security Groups are configured to govern IP traffic and grant access to VPC endpoints DNS Hostnames enabled on the VPC to support VPC endpoint hostname resolution Authentication and Authorization AWS Identity and Access Management (IAM) can help you create preventive controls for many aspects of your data science enviroment. They can control access to your data in Amazon S3, control who can access SageMaker resources like Notebook servers, and even be applied as VPC endpoint policies to put explicit controls around the API endpoints you create in your data science environment.\nThere are several IAM roles you will need in order to manage permissions and ensure separation of concerns at scale. Those roles are:\nData scientist user role\nGranting Console access, start/stop Jupyter notebook, open Jupyter notebook\nNotebook creation role\nUsed by CI/CD pipeline or Service Catalog to create a Jupyter Notebook\nNotebook execution role\nUsed by a Jupyter Notebook to access AWS resources\nTraining / Transform job execution role\nUsed by Training Job or Batch Transform job to access AWS resources like Amazon S3\nEndpoint creation role\nUsed by your CI/CD pipeline to create hosted ML model endpoints\nEndpoint hosting role\nUsed by a hosted ML model to access AWS resources such as Amazon S3\nEndpoint invocation role\nUsed by an application to call a hosted ML model endpoint\nThere are also many IAM conditions you can apply in your policies to begin to grant powerful permissions but only under certain conditions. You will learn more about these in the labs.\nData Protection In a data science environment there is the highly sensitive data you are using to train your ML models, but there is also the sensitive intellectual property you are developing in the form of algorithms, libraries, and trained models. There are many ways to protect data such as the preventive controls described above, defined as IAM policies. In addition you have the ability to encrypt data at rest using managed encryption keys.\nMany AWS services, including Amazon S3 and Amazon SageMaker, are integrated with AWS Key Management Service (KMS) to make it very easy to encrypt your data at rest. You can take advantage of these integrations to ensure that your data is encrypted in the data lake AND in the data science environment, end to end. This encryption also applies to your intellectual property as it is being developed in the many places it may be stored such as Amazon S3, EC2 EBS volumes, or AWS CodeCommit git repository.\nAuditability Using cloud services in a safe and responsible manner is good, but being able to demonstrate to others that you are operating in a governed manner is even better. Developers and security officers alike will need to see activity logs for models being trained and persons interacting with the systems. Amazon CloudWatch Logs and CloudTrail are there to help, receiving logs from many different parts of your data science environment to include:\nAmazon S3 Amazon SageMaker Notebooks Amazon SageMaker Training Jobs Amazon SageMaker Hosted Models VPC Flow Logs Let\u0026rsquo;s now dive into implementing these key areas. You will start by creating a secure, private, network environment.\n"
},
{
	"uri": "/personalize/lite/using.html",
	"title": "Using the Notebooks",
	"tags": [],
	"description": "",
	"content": "Start by navigating to the SageMaker serivce page by clicking the Services link in the top navigation bar of the AWS console.`\nIn the search field enter SageMaker and then click for the service when it appears, from the service page click the Notebook Instances link on the far left menu bar.\nTo get to the Jupyter interface, simply click Open JupyterLab on the far right next to your notebook instance.\nClicking the open link will take a few seconds to redirect you to the Jupyter system but once there you should see a collection of files on your left. Get started by clicking on Personalize_BuildCampaign.ipynb.\nThe rest of the lab will take place via the Jupyter notebooks, simply read each block before executing it and moving onto the next. If you have any questions about how to use the notebooks please ask your instructor or if you are working independently this is a pretty good video to get started:\nhttps://www.youtube.com/watch?v=Gzun8PpyBCo\n"
},
{
	"uri": "/personalize/lite/clean.html",
	"title": "After the Notebooks",
	"tags": [],
	"description": "",
	"content": "Once you have completed all of the work in the Notebooks and have completed the cleanup steps there as well, the last thing to do is to delete the stack you created with CloudFormation. To do that, inside the AWS Console again click the Services link at the top, and this time enter in CloudFormation and click the link for it.\nClick the Delete button on the demo stack you created:\nLastly click the Delete Stack button that shows up on the popup:\nYou\u0026rsquo;ll now notice that the stack is in progress of being deleted. Once you see Delete Completed you know that everything has been deleted and you are 100% done with this lab.\n"
},
{
	"uri": "/personalize/full/runvideorecommendation.html",
	"title": "Running the Video Recommendation App",
	"tags": [],
	"description": "",
	"content": "Running the Video Recommendation App You are now ready to run the application server! Simply execute the runmyserver script, and you should see status messages appearing quickly - these initial ones are the Load Balancer health-checks, and after a minute or so the instance should be declared healthy by the Load Balancer Target Group. Note, you will see some warnings around the psycopg2 component, but this can be ignored.\nTo execute the runmyserver script\nObtain the public DNS name from the console (Services =\u0026gt; EC2 =\u0026gt; Instances =\u0026gt; details) SSH using the key created at lab start Became root Move to app directory Execute the runmyserver script The URL of the server is your ALB followed by the /recommend/ path, although there is also an /admin/ path configured that we\u0026rsquo;ll use later. For now connect to your server - in my example the server can be found at http://TestS-Appli-ADS60FMCKPMG-1862985075.us-east-1.elb.amazonaws.com/recommend\nYou should see the following screen in your browser - no Model Precision Metrics are available, as we haven\u0026rsquo;t added any models yet to the application. You can also see that documentation for this is present, but be aware that it may not be 100% up to date with coding changes on the demo.\nIf you hit Select Random User then you\u0026rsquo;ll be taken to the main Recommendation screen, which starts by showing you a random user\u0026rsquo;s top-25 movie review titles. However, you\u0026rsquo;ll see on the Model dropdown on the left that there are no models available, and if you change the Personalize Mode to either Personal Ranking or Similar Items then it\u0026rsquo;s the same story - you can see the movie reviews, and most-popular titles in a genre, but no recommendations. We need to get the solutions and campaigns built in the notebook, then you can come back and plug in the models.\nAt this point we require the solution that is being built in the notebook to complete and the associated campaign to have been created - until that time we cannot move forward, so you may wish to get some refreshments if you are still waiting for those two steps to complete.\n"
},
{
	"uri": "/personalize/full/additionalpersonalize.html",
	"title": "Create Additional Personalize Campaigns",
	"tags": [],
	"description": "",
	"content": "If you have built the additional two Personalize models, for Item-to-Item Similarities and Personal Rankings, then you\u0026rsquo;ll need to create the associated campaigns for these solutions, as it is the campaigns that we will add to the application. If those solutions have been built then continue with these steps, but if not you can always come back to these steps later before adding them to the application.\nIn the AWS Console, go to the Amazon Personalize service console, click on Dataset groups link on the left-hand menu, and select the personalize-recs-dataset-group link, then click into the Campaigns menu item on the left. Select Create Campaign\nFirst, we want to build the campaign for the Similar Items model - enter a name for the campaign, such as similar-items-campaign, select via the drop-down that solution that you previously built in the console, similar-items-solution, and ensure that minimum TPS is set to 1. Hit Create Campaign\nNow build the campaign for the Personal Rankings model - follow the same steps as before, but this time use rankings-campaign for the campaign name and select the rankings-solution model in the drop-down control.\n"
},
{
	"uri": "/personalize/full/plugin.html",
	"title": "Plug In the Recommendation Model(s)",
	"tags": [],
	"description": "",
	"content": "Plug In the Recommendation Model(s) The application uses the Django Administration feature to define models that are available to the application. This allows multiple models of different types to be configured, and injected or removed from the application at any time. There are three modes of operation of the application:\nRecommendations - standard recommendations, allowing different 2 models to be compared at once Personal Ranking - re-ranks popular films in a genre, with a single model on-screen at once Similar Items - shows items similar to others, with a single model on-screen at once. You can optionally send this list through a Personal Ranking model if you have one definedA Each of these modes allows multiple models of their type to be used, but each mode can only show one or two different models simultaneously - however, you can choose any configured model at any time.\nBy default the admin user within Django does not exist - you need to create one. But in this case we already created one admin user for this workshop.\nusername admin password DoNotH@ckMe email anyone@email.com Login to the Django Administration site. This is at the same URL as the main application, but replace /recommend with /admin at the end of the URL. This will bring up the following screen, so login now with the credentials that you just created\nThis brings up the Site Administration screen, which show entries for Groups and Users (which we don\u0026rsquo;t need), but also a section called Recommend where you can add Personalize models to the app. Click on +Add link to begin to add a new model\nBack on the AWS Console, go to the Amazon Personalize service console, select the personalize-recs-dataset-group and then on the left-hand menu click Campaigns. This will show your personalize-lab-recs-campaign, along with the campaigns for the other two solutions if you\u0026rsquo;ve created them. If you\u0026rsquo;ve created all three then you should see something like this, but for your other two campaigns may already have been created\nClick on the personalize-lab-recs-campaign and you\u0026rsquo;ll see the Campaign ARN - copy this, and head back to the admin screen. Enter Personal Recommendations for the model name, enter the ARN where it asks, ensure that the Model type is set for recommendations and set the Model sort order to 1. Click on SAVE to save the definition.\nThe application will use the sort order field to decide how to order models in the on-screen drop-downs. Only models of the right type are shown on the relevant screen, but there is no validation that you have entered the correct model type, and if you put a SIMS model on the Rankings screen then the application will throw errors.\nIf you also have a SIMS or Personal Ranking campaign then go ahead and add them now in the same way - if they haven\u0026rsquo;t yet completed then you can come back and add them later. You can then close the admin screen and head back to the main application web page\nThe main screen now shows the three models (or maybe just one) that we\u0026rsquo;ve built - it lists the precision metrics for each one, and as you add or remove models from the Django Administration page the changes will be reflected here. Now click on the Select Random User button\nThe screen will look as before, but now if you click on the Model 1 drop-down you will see that our one Recommendation model is present - if you select it then the screen will refresh to show recommendations for this user using that model.\nYou can step through users to see how these look for different demographics of users. If you had mutiple Recommendation models defined then they would also be in the two model drop-downs, and you\u0026rsquo;d be able to show two completely different recommendation models, based upon different user demographic or item metadata, allowing you to compare and contrast different approaches.\nTry out the Personal Ranking personalize mode - this takes a list of the most popular movies in the dataset, either as a whole or in just a single genre. This will take that list and re-rank it into an order for this particular user, ensuring that the ones that are most likely to be interested in are shown first.\nFinally, try the Similar Items personalize mode - this starts with the user\u0026rsquo;s top-ranked film, and finds a list of films that people who watched this also watched. This is done without reference to the user\u0026rsquo;s preferences, and the list is generated based upon what\u0026rsquo;s in the dataset as a whole. However, if you also have a Personal Ranking model defined then the Ordering drop-down will re-rank this list into one that is more suited to the user\u0026rsquo;s preferences.\n"
},
{
	"uri": "/personalize/full/additionalcampaigns.html",
	"title": "Additional Campaigns to Build",
	"tags": [],
	"description": "",
	"content": "If you look at the embedded documentation you\u0026rsquo;ll see that it talks about 3 other models, which there isn\u0026rsquo;t time to build during this Lab. They involve the user of additional data files - a user demographic file, and a item metadata file, all of which are supplied with the Movie Lens data set in your Sagemaker Notebook. Because they required additional data-sets, you need to create each of these within their own Personalize Dataset Group, and you also need to re-import the original interactions file DEMO-movie-lens-100k.csv that you uploaded into S3 during the notebook - this is because Personalize trains solutions on all data files witin the Dataset Group.\nThe three models that you should build are as follows:\nUsing a USERS file, create a model that takes into account user\u0026rsquo;s demographic details such as age, gender and occupation Using an ITEMS metadata file, create a model that also takes into account the movie year and the top-4 genres associated with that movie as 4 separate metadata fields Using an ITEMS metadata file, create a model that also takes into account the movie year and then compounds the top-4 genres into a single metadata field Observations are that demographics are absolutely not a good indicator for movies recommendations, nor for things like book recommendations - this isn\u0026rsquo;t an issue with Amazon Personalize, rather it is a know issue with using age and gender to predict likes and dislikes of media. Also, the single, compound genre certainly seems more accurate for the first 5 or 10 responses, but for the set of 25 response as a whole the multiple genre model probably gets a better list of movies than the compound one.\n"
},
{
	"uri": "/personalize/full/closing.html",
	"title": "Cleaning up",
	"tags": [],
	"description": "",
	"content": "Terminating the Notebook Instance Open the Amazon SageMaker console and click on Notebook instances Find the notebook instance listed as [Name]-lab-notebook, select its radio button and then click the Actions dropdown.\nClick Stop to stop the Notebook Instance. This does not delete the underlying data and resources. After a few minutes the instance status will change to Stopped, and you can now click on the Actions dropdown again, but this time select Delete. Note that by selecting the name of the Notebook instance on this dialog you are taken to a more detailed information page regarding that instance, which also has Stop and Delete buttons present â€“ notebooks can also be deleted using this method.\n"
},
{
	"uri": "/airflow.html",
	"title": "Airflow Integration",
	"tags": [],
	"description": "",
	"content": "Getting Started Machine learning (ML) workflows orchestrate and automate sequences of ML tasks by enabling data collection and transformation. This is followed by training, testing, and evaluating a ML model to achieve an outcome. For example, you might want to perform a query in Amazon Athena or aggregate and prepare data in AWS Glue before you train a model on Amazon SageMaker and deploy the model to production environment to make inference calls. Automating these tasks and orchestrating them across multiple services helps build repeatable, reproducible ML workflows. These workflows can be shared between data engineers and data scientists.\nThis solution is a modified version of this blog post https://aws.amazon.com/blogs/machine-learning/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/ Thanks!\n"
},
{
	"uri": "/personalize.html",
	"title": "Amazon Personalize",
	"tags": [],
	"description": "",
	"content": "Getting Started Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\nWith Amazon Personalize, you provide an activity stream from your application â€“ clicks, page views, signups, purchases, and so forth â€“ as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize will process and examine the data, identify what is meaningful, select the right algorithms, and train and optimize a personalization model that is customized for your data. All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving personalized recommendations via a simple API call. You pay only for what you use, and there are no minimum fees and no upfront commitments.\nIn this workshop you will build your very own recommendation model that will recommend movies to users based on their past preferences. You will further improve the recommendation model to take into account a user\u0026rsquo;s interactions with movie items to provide accurate recommendations. This workshop will use the publicly available movie lens dataset.\n"
},
{
	"uri": "/cleanup/workspace.html",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Clean Up:\nDelete Glue Tables https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=tables Delete Glue Crawler https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers Delete Glue ETL https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs Delete Lambda Functions https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions Delete Cloudwatch Rule https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#rules: Delete State Machine https://console.aws.amazon.com/states/home?region=us-east-1#/statemachines Delete IAM Roles https://console.aws.amazon.com/iam/home?region=us-east-1#/roles Delete Endpoint Configurations https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig Delete Endpoints https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints Delete Inference Models https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models Terminate Notebook Instance https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances Delete Created Bucket https://s3.console.aws.amazon.com/s3/home?region=us-east-1# Remove AWSGlueServiceRole-billing-crawler-role https://console.aws.amazon.com/iam/home?region=us-east-1#/roles Remove AWSGlueServiceRole-reseller-crawler-role https://console.aws.amazon.com/iam/home?region=us-east-1#/roles Run drop table implementationdb in https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#query "
},
{
	"uri": "/personalize/full/conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Upon completion of this lab you will have performed the following:\nLaunched a Jupyter notebook from with the Amazon SageMaker service Imported external files into the notebook environment Seen how to enable Preview services within a notebook (assuming your account has been whitelisted for Preview access) Used the pandas libraries to do some pre-processing of the source data Built and deployed an ML model based upon the HRNN algorithm Tested your model via just a few lines of code Deployed your model into a live application You should now be able to embed this model from within your own application code, using any language that is supported by the AWS SDK. Happy recommending!\n"
},
{
	"uri": "/prerequisites/prerequisites.html",
	"title": "Environment",
	"tags": [],
	"description": "",
	"content": "AWS Account In order to complete this workshop you\u0026rsquo;ll need an AWS Account, and an AWS IAM user in that account with at least full permissions to the following AWS services:\nAWS IAM Amazon S3 Amazon SageMaker AWS Cloud9 Use Your Own Account: The code and instructions in this workshop assume only one student is using a given AWS account at a time. If you try sharing an account with another student, you\u0026rsquo;ll run into naming conflicts for certain resources. You can work around these by appending a unique suffix to the resources that fail to create due to conflicts, but the instructions do not provide details on the changes required to make this work. Use a personal account or create a new AWS account for this workshop rather than using an organizationâ€™s account to ensure you have full access to the necessary services and to ensure you do not leave behind any resources from the workshop.\nCosts: Some, but NOT all, of the resources you will launch as part of this workshop are eligible for the AWS free tier if your account is less than 12 months old. See the AWS Free Tier page for more details. An example of a resource that is not covered by the free tier is the ml.m4.xlarge notebook instance used in some workshops. To avoid charges for endpoints and other resources you might not need after you\u0026rsquo;ve finished a workshop, please refer to the Cleanup Module.\nAWS Region SageMaker is not available in all AWS Regions at this time. Accordingly, we recommend running this workshop in one of the following supported AWS Regions: N. Virginia, Oregon, Ohio, or Ireland.\nOnce you\u0026rsquo;ve chosen a region, you should create all of the resources for this workshop there, including a new Amazon S3 bucket and a new SageMaker notebook instance. Make sure you select your region from the dropdown in the upper right corner of the AWS Console before getting started.\nBrowser We recommend you use the latest version of Chrome or Firefox to complete this workshop.\nAWS Command Line Interface To complete certain workshop modules, you\u0026rsquo;ll need the AWS Command Line Interface (CLI) and a Bash environment. You\u0026rsquo;ll use the AWS CLI to interface with SageMaker and other AWS services.\nFor these workshops, AWS Cloud9 is used to avoid problems that can arise configuring the CLI on your machine. AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It has the AWS CLI pre-installed so you donâ€™t need to install files or configure your laptop to use the AWS CLI. For Cloud9 setup directions for these workshops, see Cloud9 Setup. Do NOT attempt to use a locally installed AWS CLI during a live workshop because there is insufficient time during a live workshop to resolve related issues.\nText Editor For any workshop module that requires use of the AWS Command Line Interface (see above), you also will need a plain text editor for writing Bash scripts. Any editor that inserts Windows or other special characters potentially will cause scripts to fail. AWS Cloud9 includes a text editor.\n"
},
{
	"uri": "/personalize/lite.html",
	"title": "Lite Version",
	"tags": [],
	"description": "",
	"content": "Getting Started Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\nWith Amazon Personalize, you provide an activity stream from your application â€“ clicks, page views, signups, purchases, and so forth â€“ as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize will process and examine the data, identify what is meaningful, select the right algorithms, and train and optimize a personalization model that is customized for your data. All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving personalized recommendations via a simple API call. You pay only for what you use, and there are no minimum fees and no upfront commitments.\nIn this workshop you will build your very own recommendation model that will recommend movies to users based on their past preferences. You will further improve the recommendation model to take into account a user\u0026rsquo;s interactions with movie items to provide accurate recommendations. This workshop will use the publicly available movie lens dataset.\n"
},
{
	"uri": "/introduction/concepts.html",
	"title": "Machine Learning",
	"tags": [],
	"description": "",
	"content": "This section describes a typical machine learning workflow and summarizes how you accomplish those tasks with Amazon SageMaker.\nIn machine learning, you \u0026ldquo;teach\u0026rdquo; a computer to make predictions, or inferences. First, you use an algorithm and example data to train a model. Then you integrate your model into your application to generate inferences in real time and at scale. In a production environment, a model typically learns from millions of example data items and produces inferences in hundreds to less than 20 milliseconds.\nThe following diagram illustrates the typical workflow for creating a machine learning model:\nAs the diagram illustrates, you typically perform the following activities:\nGenerate example data To train a model, you need example data. The type of data that you need depends on the business problem that you want the model to solve (the inferences that you want the model to generate). For example, suppose you want to create a model to predict a number given an input image of a handwritten digit. To train such a model, you need example images of handwritten numbers.\nData scientists often spend a lot of time exploring and preprocessing, or \u0026ldquo;wrangling,\u0026rdquo; example data before using it for model training. To preprocess data, you typically do the following:\nFetch the dataâ€” You might have in-house example data repositories, or you might use datasets that are publicly available. Typically, you pull the dataset(s) into a single repository.\nClean the dataâ€”To improve model training, inspect the data and clean it up as needed. For example, if your data has a country name attribute with values United States and US, you might want to edit the data to be consistent.\nPrepare or transform the dataâ€”You might perform additional data transformations to improve performance. For example, you might choose to combine attributes. If your model predicts the conditions that require de-icing an aircraft, instead of using temperature and humidity attributes separately, you might combine them into a new attribute to get a better model.\nIn Amazon SageMaker, you preprocess example data in a Jupyter notebook on your notebook instance. You use your notebook to fetch your dataset, explore it and prepare it for model training. You\u0026rsquo;ll create a Notebook Instance in the next section.\nTrain a model Model training includes both training and evaluating the model, as follows:\nTraining the modelâ€”To train a model, you need an algorithm. The algorithm you choose depends on a number of factors. In Amazon SageMaker, you have the following options for a training algorithm:\nUse an algorithm provided by Amazon SageMakerâ€”Amazon SageMaker provides training algorithms. If one of these meets your needs, it\u0026rsquo;s a great out-of-the-box solution for quick model training. For a list of algorithms provided by Amazon SageMaker, see the Amazon SageMaker documentation. You\u0026rsquo;ll use some of the built-in SageMaker algorithms in the Using Built-in Algorithms module.\nUse Apache Spark with Amazon SageMakerâ€”Amazon SageMaker provides a library that you can use in Apache Spark to train models with Amazon SageMaker. Using the library provided by Amazon SageMaker is similar to using Apache Spark MLLib. For more information, see Using Apache Spark with Amazon SageMaker.\nSubmit custom code to train with deep learning frameworksâ€”You can submit custom Python code that uses TensorFlow or Apache MXNet for model training. You\u0026rsquo;ll see an example of using Apache MXNet with Amazon SageMaker in the Using Custom Algorithms module.\nUse your own custom algorithmsâ€”Put your code together as a Docker image and specify the registry path of the image in an Amazon SageMaker CreateTrainingJob API call. For more information, see the Amazon SageMaker documentation. You\u0026rsquo;ll see an example of how to use your own algorithm in the Using Custom Algorithms module.\nEvaluating the modelâ€”After you\u0026rsquo;ve trained your model, you evaluate it to determine whether the accuracy of the inferences is acceptable. You can evaluate your model using historical data (offline) or live data:\nOffline testingâ€”Use historical, not live, data to send requests to the model for inferences. Deploy your trained model to an alpha endpoint, and use historical data to send inference requests to it. To send the requests, use a Jupyter notebook in your Amazon SageMaker notebook instance and either the AWS SDK for Python (Boto) or the high-level Python library provided by Amazon SageMaker.\nOnline testing with live dataâ€”Amazon SageMaker supports multiple models (called production variants) to a single Amazon SageMaker endpoint. You configure the production variants so that a small portion of the live traffic goes to the model that you want to validate. For example, you might choose to send 10% of the traffic to a model variant for evaluation. After you are satisfied with the model\u0026rsquo;s performance, you can route 100% traffic to the updated model.\nYou use a Jupyter notebook in your Amazon SageMaker notebook instance to train and evaluate your model.\nDeploy the model Traditionally, you do some re-engineering of a model to integrate it with your application, before deploying the model into production. With Amazon SageMaker hosting services, you can deploy your model independently, decoupling it from your application code. For more information, see Deploying a Model on Amazon SageMaker Hosting Services.\nMachine learning is a continuous cycle. After deploying a model, you monitor the inferences, then collect \u0026ldquo;ground truth,\u0026rdquo; and evaluate the model to identify drift. You then increase the accuracy of your inferences by updating your training data to include the newly collected ground truth, by retraining the model with the new dataset. As more and more example data becomes available, you continue retraining your model to increase accuracy over time.\n"
},
{
	"uri": "/prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "In this module, we\u0026rsquo;ll go through the prerequisites for the workshop, and set up a Cloud9 workspace for the workshop:\nEnvironment\nJupyter Notebooks\nCloud9 Setup\n"
},
{
	"uri": "/start.html",
	"title": "SageMaker",
	"tags": [],
	"description": "",
	"content": "Getting Started To start the workshop, let\u0026rsquo;s create a Jupyter Notebook in Sagemaker and clone the repos to it in order to continue with the first part:\nExploratory analysis ETL to prepare training data Training the model with Hyperparameter Optimization Putting â€œnew dataâ€ through a preprocessing pipeline to get it ready for prediction Batch predictions for new data "
},
{
	"uri": "/security_for_sysops/team_resources/secure_networking.html",
	"title": "Secure Networking",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker allows you to create resources attached to your AWS Virtual Private Cloud (VPC). This allows you to govern access to SageMaker resources and your data sets using familiar tools such as security groups, routing tables, and VPC endpoints. Using these network-layer tools you can create a secure network environment that allows you to explicitly control the data ingress and egress of your data science environment. Please take a few moments and read about these tools in more detail.\nPrivate Network Environment Begin with your VPC which will be used to host Amazon SageMaker and other components of your data science environment. Your VPC provides a familiar set of network-level controls to allow you to govern ingress and egress of data. You will begin this lab by creating a VPC with no Internet Gateway (IGW), therefore all subnets will be private, without Internet connectivity. Network connectivity with AWS services or your own shared services will be provided using VPC endpoints powered by PrivateLink. Security Groups will be used to control traffic between different resources, allowing you to group like resources together and manage their ingress and egress traffic.\nVirtual Private Cloud A Virtual Private Cloud (VPC) gives you a self-contained network environment that you control. When initially created the VPC does not allow network traffic into or out of the VPC. It\u0026rsquo;s only by adding VPC endpoints, Internet Gateways (IGW), or Virtual Private Gateways (VPGW) that you begin to configure your private network environment to communicate with the wider world. For the rest of these labs you can assume that no access to the internet is required and that all communication with AWS services will be done explicitly through private connectivity to the AWS APIs through VPC endpoints. It is also recommend to create multiple subnets in your VPC in multiple availability zones to support highly available deployments and resilient architectures.\nTo find out more about VPCs and VPC concepts such as routing tables, subnets, security groups, and network access control lists please visit the AWS documentation.\nVPC Endpoints A VPC endpoint allows you to establish a private, secure connection between your VPC and an AWS service without requiring you to configure an Internet Gateway, NAT device, or VPN connection. Using VPC endpoints your VPC resources can communicate with AWS services without ever leaving the AWS network. A VPC endpoint is a highly available virtual device that is managed on your behalf. As a VPC resource an endpoint is given IP addresses within your VPC and security groups assigned to the endpoint can control who can communicate with the endpoint.\nIn addition to security groups you can also apply VPC endpoint policies to an AWS service endpoint. An endpoint policy is an IAM resource policy that gets applied to a VPC endpoint and governs which APIs can be called on an AWS service through the endpoint. For example, if the following endpoint policy were applied to an Amazon S3 VPC endpoint it would only allow read access to objects in the specified S3 bucket. Actions against other buckets would be denied.\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Access-to-specific-bucket-only\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my_secure_bucket\u0026#34;, \u0026#34;arn:aws:s3:::my_secure_bucket/*\u0026#34; ] } ] } VPC endpoint policies, along with security groups, provide the ability to implement Defense-in-Depth and bring a multi-layered approach to security and who can access what resources within your VPC.\nIn this lab you will create a VPC with endpoints for the following services:\nAmazon S3, for reading and writing data Amazon SageMaker, for creating training jobs and hosted models Amazon STS, for obtaining temporary credentials Amazon CloudWatch Logs, for writing out log data from VPC-based resources Use the previously created AWS Service Catalog products to create an AWS Virtual Private Cloud, security groups, and VPC endpoints to configure a precise, secure network environment to support your data science project teams. The VPC will have no IGW or NAT gateway attached and it will have multiple subnets across 3 availability zones. VPC endpoints will be created and attached to the VPC and security groups applied to control what VPC resources can communicate with each other.\n"
},
{
	"uri": "/security_for_users/environment/secure_notebook.html",
	"title": "Secure Notebooks",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is designed to empower data scientists and developers, enabling them to build more quickly and remain focused on their machine learning project. One of the ways that SageMaker does this is by providing hosted Jupyter Notebook servers. With a single API call users can create a Jupyter Notebook server with the latest patches and kernels available. No need to install software, patch or maintain systems - it is all done for you. What the API creates for you is an EC2 instance running Jupyter Notebook server software with Python, R, Docker, and the most popular ML frameworks pre-installed. But it is not just about convenience and enablement, SageMaker is also focused on hosting these notebooks for you in a secure manner.\nSecure by default An Amazon SageMaker notebook is an EC2 instance with the open source Jupyter server installed. The SageMaker service manages the EC2 instance in order to help you maintain a level of security with little or no overhead. To do this Amazon SageMaker takes responsibility for:\nPatching and updating the system Encrypting data on the EBS volumes Limiting user access to the system Enabling you to further tailor and harden the system Patching and updating A SageMaker Jupyter notebook server will have 2 EBS volumes attached to it. The first is the system drive and is ephemeral. This hosts the operating system, Anaconda, and Jupyter server software. The second hosts your data and anything you put into /home/ec2-user/SageMaker. Over time your EC2 instance can become out of date, going unpatched. To patch your Jupyter Notebook to the latest versions simply stop the notebook and start it again. This will refresh the system drive without any maintenance required on your part. However please note that if you make any changes to files on the system drive you will need to make those changes again as they will be destroyed in the stopping and starting of the notebook server.\nTo keep a SageMaker notebook up to date and to save costs it is recommended to stop a Jupyter notebook server when it is not needed and to restart it when you need to use it.\nEncryption at rest As mentioned the EC2 instance has two EBS volumes mapped to it. Both of these volumes are encrypted using a SageMaker service-managed KMS key, although you can specify a CMK to be used to encrypt the storage volume. By doing this you can easily encrypt all of the data stored on the Jupyter Notebook server by default.\nLimiting user access The very nature of software development means that users can obtain some OS-level access to the Jupyter notebook server. By default the Jupyter Web UI will allow you to open a shell terminal. When a user access the shell, they will be logged into the EC2 instance as ec2-user. If you\u0026rsquo;re familiar with Amazon Linux this is the default username that you use to gain access to an AWS EC2 instance. This user also typically has permissions to sudo to the root user. This can be limited in the Jupyter Notebook configuration which will remove the user\u0026rsquo;s permission to assume the root user. They will still have permissions to install things like Python modules into their user environment, but they will not be able to modify the wider system of the notebook server.\nFurther tailoring In addition to the measures described above you also have the ability to specify Lifecycle Configurations. These are shell scripts which execute on system bootstrap, before the notebook is made available to users. These configuration scripts can execute on system creation, system startup, or both. Using these scripts you can ensure that monitoring agents are installed or other types of hardening are performed to ensure that the system is in a specific state before allowing users to access the system. Here we will use the lifecycle scripts to download some open source libraries from the pip mirror we created, create an sagemaker_environment.py file to keep track of variables such as the network configuration, KMS keys that can be imported directly, without giving the datascientist access to them.\nManaged and Governed Amazon SageMaker provides managed EC2-based services such as Jupyter notebooks, training jobs, and hosted ML models. SageMaker runs the infrastructure for these components using EC2 resources dedicated to yourself. These EC2 resources can be mapped to your VPC environment allowing you to apply your network level controls, such as security groups, to the notebooks, training jobs, and hosted ML models.\nAmazon SageMaker does this by creating an ENI in your specified VPC and attaching it to the EC2 infrastructure in the service account. Using this pattern the service gives you control over the network-level access of the services you run on Amazon SageMaker.\nFor Jupyter Notebooks this will mean that all Internet connectivity, Amazon S3 connectivity, or access to other systems in your VPC is governed by you and by your network configuration.\nFor training jobs and hosted models these are again governed by you. When retrieving training data from Amazon S3 the SageMaker EC2 instances will communicate with S3 through your VPC without traversing the public internet. Equally when the SageMaker EC2 instances retrieve your trained model for hosting, they will communicate with S3 through your VPC, without traversing the public internet. You maintain governance and control of the network communication of your SageMaker resources.\nAccess Control Access to a SageMaker Jupyter notebook instance is goverend by AWS IAM. In order to open a Jupyter Notebook instance users will need access to the CreatePresignedNotebookInstanceUrl API call. This API call creates a presigned URL which can be followed to obtain Web UI access to the Jupyter notebook server. To secure this interface you use IAM policy statements to wrap conditions around calling the API, for example who can invoke the API and from what IP address.\n{ \u0026#34;Id\u0026#34;:\u0026#34;notebook-example-1\u0026#34;, \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;Enable Notebook Access\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;:{ \u0026#34;ForAnyValue:StringEquals\u0026#34;:{ \u0026#34;aws:sourceVpce\u0026#34;:[ \u0026#34;vpce-111bbccc\u0026#34;, \u0026#34;vpce-111bbddd\u0026#34; ] } } } ] } The IAM policy above states that someone can only communicate with a Notebook if they do so from within a VPC and through specific VPC endpoints. Using mechanisms like the above you can explicitly control who can interact with a Notebook server. Other example IAM policies can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_id-based-policy-examples.html\nAs a best practice, we want to limit a single user to a notebook instance. As every notebook instance has an associated IAM role, by using fine grained user level IAM roles, we can limit a single user to a single notebook instance. Similarly, for cost attribution purposes we can use tagging https://docs.aws.amazon.com/IAM/latest/UserGuide/id_tags.html to link usage costs to individual users or to teams.\nVersion Control Finally, to support collaboration, SageMaker notebooks can be integrated with Git-based repositories. Git is a distributed version control system which enables project teams to manage source code, notebook files, and other artifacts related to a project. In the next lab the notebook you create will be configured to use a CodeCommit repository as a way of managing the sample project provided. In this lab you will push and tag the code to the master branch of the code repository using the CLI or Jupyter UI.\nIn this lab, as a data scientist, you will use the newly launched notebook to kickoff a new ML project.\n"
},
{
	"uri": "/security_for_users/notebook/ml_lifecycle.html",
	"title": "Security Objectives",
	"tags": [],
	"description": "",
	"content": "A secure environment is an enabler for teams, allowing them to focus on their project and the business challenge it is trying to solve. From the perspective of a project team the environment is intended to support the team in achieving the following objectives:\nCompute and Network Isolation\nIn working with the Amazon SageMaker service you will need to configure training jobs and similar resources using practices inline with security policy. This means provisioning resources in the same secure network environment in which your JupyterLab server is operating. This lab will show you how to configure SageMaker to provision resources in accordance with your policies.\nAuthentication and Access Management\nAn AWS IAM role has been provisioned for you and for the resources you create, this lab will show you how to pass that role on to Amazon SageMaker for use when acting on your behalf.\nArtifact Management\nYour team has been provided with resources and technology. Your team will now produce artifacts such as features derived from data, notebooks to explore and conduct experimentation, algorithms to produce ML models, and other forms of intellectual property. This lab will show you how to manage these assets using services like Amazon S3 and an AWS CodeCommit Git repository.\nData Encryption\nYou will learn how to ensure that all training/processing volumes are passed appropriate encryption keys to ensure data is encrypted end to end, in transit and at rest. Also we will see how to use network configurations to ensure that data is encrypted in transit and doesn\u0026rsquo;t traverse the internet.\nTraceability and Auditability\nYou will learn how to use SageMaker Experiments to easily maintain the lineage of your trained models and training jobs. SageMaker Experiments will automatically manage the metadata associated with your training jobs and make it easily accessible via a Pandas DataFrame.\nExplainability and Interpretability\nThere are different methods, tools, and techniques to explain an ML model\u0026rsquo;s inference. In this lab you will use SHAP values to derive feature importances and gain insight into how your model is weighting different inputs to produce a prediction.\nReal Time Model Monitoring\nOnce a model has been produced ensuring that it is producing predictions that are still accurate and relevant for the current state of the world is a must to avoid drift in original accuracy, F1 scores, or other objective metrics. In this lab you will learn how to monitor your production endpoints in real time to detect concept drift and alert when inputs to your model or its outputs are no longer in keeping with the model\u0026rsquo;s training sets.\nReproducibility\nFor audit reasons, you may need to reproduce your work at a later time. To ensure reproducibility, you will see how to use SageMaker experiments in combination with CodeCommit to version your code and track the entire lineage of your models.\nIn the following labs you will work through Jupyter notebooks which show you how to support the above objectives using the environment that has been provisioned for you.\nIn the first lab you will work through a notebook called 01_SageMaker-DataScientist-Workflow.ipynb which will cover a typical Data Scientist workflow and show you how to explore data, pre-process data, train an XGBoost model using an Amazon Managed container and explore feature importances for that model in a secure manner, maintaining network traffic within your team\u0026rsquo;s private VPC and enforing encryption at rest and in transit. Furthermore, you will learn how to use SageMaker Processing for running processing jobs at scale, and leverage Spot instance pricing to save on training costs.\nIn the second notebook, 02_SageMaker-DevOps-Workflow.ipynb, you will deploy the trained model from the SageMaker notebook to production and monitor the endpoint for data drift using ModelMonitor. Finally you will use SageMaker Experiments to track any model metadata, such as the Git commit ID associated with the algorithm, to trace the lineage of our models and endpoints.\n"
},
{
	"uri": "/step.html",
	"title": "Step Functions",
	"tags": [],
	"description": "",
	"content": "Implementation with Step Functions We will move to the part 2 of the workshop on out Jupyter Notebook into the folder \u0026ldquo;2-implementation-with-step-functions\u0026rdquo;\nNow that we have a satisfying machine learning model, we want to implement this as part of a process that runs every day at 3AM in the morning based on the latest transactional information that we have dumped on S3 to create forecasts for each reseller.\nThe final workflow will look like this:\n"
},
{
	"uri": "/step/upload.html",
	"title": "Upload the data to S3",
	"tags": [],
	"description": "",
	"content": "First you need to create a bucket for this experiment. Upload the data from the following public location to your own S3 bucket. To facilitate the work of the crawler use two different prefixs (folders): one for the billing information and one for reseller.\nWe can execute this on the console of the Jupyter Notebook or we can just execute it directly:\nDownload the data Your Bucket Name\nyour_bucket = \u0026#39;\u0026lt;YOUR BUCKET NAME e.g., my-sagemaker-bucket\u0026gt;\u0026#39; !wget https://github.com/mpdominguez/sagemaker-workshop/blob/master/assets/awswrangler-1.9.6-py3.6.egg !wget https://github.com/mpdominguez/sagemaker-workshop/blob/master/assets/reseller_sm.csv !wget https://github.com/mpdominguez/sagemaker-workshop/blob/master/assets/billing_sm.csv Now we upload the data to an S3 location\nimport boto3, os boto3.Session().resource(\u0026#39;s3\u0026#39;).Bucket(your_bucket).Object(os.path.join(\u0026#39;billing\u0026#39;, \u0026#39;billing_sm.csv\u0026#39;)).upload_file(\u0026#39;billing_sm.csv\u0026#39;) boto3.Session().resource(\u0026#39;s3\u0026#39;).Bucket(your_bucket).Object(os.path.join(\u0026#39;reseller\u0026#39;, \u0026#39;reseller_sm.csv\u0026#39;)).upload_file(\u0026#39;reseller_sm.csv\u0026#39;) boto3.Session().resource(\u0026#39;s3\u0026#39;).Bucket(your_bucket).Object(os.path.join(\u0026#39;python\u0026#39;, \u0026#39;awswrangler-1.9.6-py3.6.egg\u0026#39;)).upload_file(\u0026#39;awswrangler-1.9.6-py3.6.egg\u0026#39;) "
},
{
	"uri": "/step/crawler.html",
	"title": "Create the Crawler",
	"tags": [],
	"description": "",
	"content": "To use this csv information in the context of a Glue ETL, first we have to create a Glue crawler pointing to the location of each file. The crawler will try to figure out the data types of each column. The safest way to do this process is to create one crawler for each table pointing to a different location.\nGo to the AWS Console.\nSelect under Services AWS Glue.\nOr follow this link! https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers\nUnder crawlers Add Crawler and two crawlers: create one pointing to each S3 location (one to billing and one to reseller)\nCrawler Name: Billing - Next Crawler source type: Data Store - Next Add a data store: S3, Specific Path in my Account, Navigate to your bucket and your folder Billing - Next Add another data store: no - Next Choose an IAM role: create an IAM role billing-crawler-role (if exists, choose the existing) - Next Frequency: run on demand - Next Crawler\u0026rsquo;s output: Add database implementationdb - Next Finish After the crawler is created select Run it now.\nLet\u0026rsquo;s add the second crawler:\nCrawler Name: Reseller - Next Crawler source type: Data Store - Next Add a data store: S3, Specific Path in my Account, Navigate to your bucket and your folder Reseller - Next Add another data store: no - Next Choose an IAM role: create an IAM role reseller-crawler-role (if exists, choose the existing) - Next Frequency: run on demand - Next Crawlers\u0026rsquo;s output: Select database implementationdb - Next Finish After the crawler is created select Run it now.\nAfter both crawlers run you should see one table is been adeed for each. You can use Athena to inspect the tables and double check the data is been added properly. Follow this link to ckeck https://us-east-1.console.aws.amazon.com/athena/home?force\u0026amp;region=us-east-1#query And run these two queries\nselect * from billing Separately\nselect * from reseller "
},
{
	"uri": "/airflow/introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction ML workflows consist of tasks that are often cyclical and iterative to improve the accuracy of the model and achieve better results. We recently announced new integrations with Amazon SageMaker that allow you to build and manage these workflows:\nAWS Step Functions automates and orchestrates Amazon SageMaker related tasks in an end-to-end workflow. You can automate publishing datasets to Amazon S3, training an ML model on your data with Amazon SageMaker, and deploying your model for prediction. AWS Step Functions will monitor Amazon SageMaker and other jobs until they succeed or fail, and either transition to the next step of the workflow or retry the job. It includes built-in error handling, parameter passing, state management, and a visual console that lets you monitor your ML workflows as they run. Many customers currently use Apache Airflow, a popular open source framework for authoring, scheduling, and monitoring multi-stage workflows. With this integration, multiple Amazon SageMaker operators are available with Airflow, including model training, hyperparameter tuning, model deployment, and batch transform. This allows you to use the same orchestration tool to manage ML workflows with tasks running on Amazon SageMaker. This exercise shows how you can build and manage ML workflows using Amazon Sagemaker and Apache Airflow. Weâ€™ll build a recommender system to predict a customerâ€™s rating for a certain video based on the customerâ€™s historical ratings of similar videos, as well as the behavior of other similar customers. Weâ€™ll use historical star ratings from over 2 million Amazon customers on over 160,000 digital videos. Details on this dataset can be found at its AWS Open Data page.\n"
},
{
	"uri": "/security_for_sysops/team_resources/secure_environment_lab.html",
	"title": "Lab 2: Secure Environment",
	"tags": [],
	"description": "",
	"content": "A data science project team have requested a cloud environment to begin their project. As a project administrator you will use the Service Catalog portfolio, managed by the Cloud Platform Engineering team, to provision a secure VPC and related resources for the data science team. In this lab, you will use AWS Service Catalog to provision this data science environment. By following the steps below you will create an environment which contains:\nAWS VPC with no IGW VPC endpoints to Amazon S3, Amazon SageMaker, CloudWatch, STS IAM roles for the project\u0026rsquo;s data science administrator and the data scientists Store AWS resource names/identifiers in AWS SSM Parameter Store for future reference Service Catalog Portfolio of products for the data science team AWS KMS key for encrypting data at rest Amazon S3 buckets dedicated to the project Amazon CodeCommit Git repository for project source code Deploy a project environment In the previous lab you deployed a CloudFormation template which created a project administrator role. Details of this role can be found in the CloudFormation Outputs for the stack you deployed.\nAfter assuming the project administrator role, visit the AWS Service Catalog console and provision a project environment for the data science team.\nEnvironment creation will take approximately 10 minutes.\nStep-by-step instructions\rOpen the AWS CloudFormation console and select the stack you deployed in the previous lab. Click the Outputs tab on the stack\u0026rsquo;s detail page and notice the AssumeProjectAdminRole hyperlink to assume the project administrator role created by the stack. Click the hyperlink to assume the project administrator role. On the resulting screen leave the values unchanged and click Switch Role. As the project administrator:\nVisit the AWS Service Catalog console. Click the context menu button next to the product Data Science Project Environment and click Launch product. Give the product deployment a name, such as example-project-dev-environment and click Next. Provide a ProjectName such as example-project. You can leave the remaining values unchanged if you like. Click Next through the next few screens to get to the Review page. Click Launch. If you wish to see the contents of these CloudFormation templates you can view them on the CloudFormation console or copy them locally for review using a command such as the below.\naws s3 sync s3://sagemaker-workshop-cloudformation-us-east-1/quickstart ./sagemaker-workshop-cloudformation Review base environment After the CloudFormation stack has been successfully created review the Resources tab of the CloudFormation stack and the resources that were created. You\u0026rsquo;ll notice that it has provisioned:\nProject-specific Service Catalog Portfolio\nA service catalog portfolio and products have been configured to give the data science team a tailored set of products they can deploy easily.\nIAM roles\nRoles and permissions have been created so that the data science teams can manage themselves and create the resources they need.\nSSM parameters\nA collection of parameters have been stored so they can be referenced by the data science teams. Visit the console, what parameters have been created?\nKMS key\nA KMS key to encrypt data at rest in the data science environment. Visit the console, what is the KMS key being used to encrypt?\nVirtual Private Cloud (VPC)\nThe template has created a VPC with no Internet connectivity but with VPC endpoints for accessing AWS services like Amazon S3 and Amazon SageMaker. Visit the console, what services are accessible from within the VPC? Do any endpoints have Endpoint Policies governing them?\nYou have now created a secure environment for the data science team. Now hand things over to the project team and let them support themselves.\n"
},
{
	"uri": "/step/gluejob.html",
	"title": "Create the Glue Job",
	"tags": [],
	"description": "",
	"content": "Now we are going to create a GLUE ETL job in python 3.6. In this job, we can combine both the ETL from Notebook #2 and the Preprocessing Pipeline from Notebook #4.\nNote that, instead of reading from a csv file, we are going to use Athena to read from the resulting tables of the Glue Crawler.\nGlue is a serverless service so the processing power assigned is meassured in (Data Processing Units) DPUs. Each DPU is equivalent to 16GB of RAM and 4vCPU.\nOpen the AWS Console\nUnder Services go to AWS Glue\nOr follow this link https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs\nUnder Jobs, add new job\nName: etlandpipeline Role: Create a role named Glueadmin with AdministratorAccess (this is because we are testing) Type: Python Shell Glue version: Python3 (Glue Version 1.0) Select A New Script Authored By you Under Security Configuration, Select Python library path and browse to the location where you have the egg of the aws wrangler Library (your bucket in thr folder python) Under Maximum Capacity: 1 - Next Then hit \u0026ldquo;Save Job and Edit Script\u0026rdquo; In the Script tab copy and paste the following script adapted to Glue from the previous notebooks.\nRemember to modify the bucket to yours. Bucket: Line 18\nimport pandas as pd import numpy as np import datetime import pandas as pd from datetime import date import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import LabelEncoder import boto3 import pickle import io from io import StringIO import awswrangler df_r = awswrangler.athena.read( \u0026#34;implementationdb\u0026#34;, \u0026#34;select * from reseller\u0026#34; ) df = awswrangler.athena.read( \u0026#34;implementationdb\u0026#34;, \u0026#34;select * from billing\u0026#34; ) bucket = \u0026#39;blackb-mggaska-implementation\u0026#39; df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#39;date\u0026#39;]) print(\u0026#39;dataframe\u0026#39;,df.shape) print(\u0026#39;dataframer\u0026#39;,df_r.shape) #---FUNCTIONS------------------------------- def write_dataframe_to_csv_on_s3(dataframe, bucket, filename): \u0026#34;\u0026#34;\u0026#34; Write a dataframe to a CSV on S3 \u0026#34;\u0026#34;\u0026#34; # Create buffer csv_buffer = StringIO() # Write dataframe to buffer dataframe.to_csv(csv_buffer, sep=\u0026#34;,\u0026#34;, header=None,index=None) # Create S3 object s3_resource = boto3.resource(\u0026#34;s3\u0026#34;) # Write buffer to S3 object s3_resource.Object(bucket, filename).put(Body=csv_buffer.getvalue()) print(\u0026#34;Writing {} records to {}\u0026#34;.format(len(dataframe), filename)) #-------------------------------------------------- # ### Filter the last 4 months of data max_date = df[\u0026#39;date\u0026#39;].max() min_date = max_date - pd.to_timedelta(120, unit=\u0026#39;d\u0026#39;) df = df[df[\u0026#39;date\u0026#39;] \u0026gt; min_date] def completeItem(dfItem): min_date = dfItem[\u0026#39;date\u0026#39;].min() max_date = dfItem[\u0026#39;date\u0026#39;].max() if min_date == max_date: #only one data point return r = pd.date_range(start=min_date, end=max_date) dfItemNew = dfItem.set_index(\u0026#39;date\u0026#39;).reindex(r).rename_axis(\u0026#39;date\u0026#39;).reset_index() dfItemNew[\u0026#39;mean-last-30\u0026#39;] = dfItemNew[\u0026#39;bill\u0026#39;].rolling(30,min_periods=1).mean().reset_index()[\u0026#39;bill\u0026#39;] dfItemNew[\u0026#39;mean-last-7\u0026#39;] = dfItemNew[\u0026#39;bill\u0026#39;].rolling(7,min_periods=1).mean().reset_index()[\u0026#39;bill\u0026#39;] dfItemNew[\u0026#39;std-last-30\u0026#39;] = dfItemNew[\u0026#39;bill\u0026#39;].rolling(30,min_periods=1).std().reset_index()[\u0026#39;bill\u0026#39;] dfItemNew[\u0026#39;bill\u0026#39;] = dfItemNew[\u0026#39;bill\u0026#39;].fillna(0) dfItemNew[\u0026#39;id_reseller\u0026#39;] = dfItem[\u0026#39;id_reseller\u0026#39;].max() dfItemNew[\u0026#39;std-last-30\u0026#39;].fillna(method=\u0026#39;ffill\u0026#39;,inplace=True) dfItemNew[\u0026#39;mean-last-7\u0026#39;].fillna(method=\u0026#39;ffill\u0026#39;,inplace=True) dfItemNew[\u0026#39;std-last-30\u0026#39;].fillna(method=\u0026#39;ffill\u0026#39;,inplace=True) resp = [] counter = 0 for index,row in dfItemNew.iterrows(): resp.append(counter) if row[\u0026#39;bill\u0026#39;] == 0: counter += 1 else: counter = 0 dfItemNew[\u0026#39;days_without_purchase\u0026#39;] = pd.Series(resp) return dfItemNew i = 0 dfCompletedList = [] for nid,item in df.groupby(\u0026#39;id_reseller\u0026#39;): i = i+1 if i%200 == 0: print (\u0026#39;processed {} resellers\u0026#39;.format(str(i))) dfCompletedList.append(completeItem(item)) df = pd.concat(dfCompletedList).copy() del dfCompletedList df[\u0026#39;weekday\u0026#39;] = df[\u0026#39;date\u0026#39;].dt.weekday_name # ### Compute next bill # In[11]: df[\u0026#39;next_bill\u0026#39;] = df.replace(0,np.nan).groupby(\u0026#39;id_reseller\u0026#39;)[\u0026#39;bill\u0026#39;].fillna(method=\u0026#39;bfill\u0026#39;) # ## Compute last bill # In[12]: df[\u0026#39;last_bill\u0026#39;] = df.replace(0,np.nan).groupby(\u0026#39;id_reseller\u0026#39;)[\u0026#39;bill\u0026#39;].fillna(method=\u0026#39;ffill\u0026#39;).copy() different_zero = df[\u0026#39;last_bill\u0026#39;].shift(1) df.loc[df[\u0026#39;bill\u0026#39;] != 0,\u0026#39;last_bill\u0026#39;] = np.nan df[\u0026#39;last_bill\u0026#39;] = df[\u0026#39;last_bill\u0026#39;].fillna(different_zero) # In[13]: df = df.merge(df_r,how=\u0026#39;inner\u0026#39;,on=\u0026#39;id_reseller\u0026#39;) # In[14]: df = df.dropna() # ## Deal with categorical variables # # To deal with categorical variables (reseller\u0026#39;s cluster and reseller\u0026#39;s zone), we will use a combination of sklearn\u0026#39;s Label Encoder, a preprocessing module that transforms strings in numeric lables, and One Hot Encoder, that takes this numerical variables and creates dummy (0/1 state) variables. # # This modules are python objects that keep in their internal variables the information necessary to transform new data. So, in the Glue ETL we are going to store this objects in pkl format # # In[17]: le_cluster = LabelEncoder() ohe_cluster = OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;) df_cluster = pd.DataFrame(ohe_cluster.fit_transform(le_cluster.fit_transform(df[\u0026#39;cluster\u0026#39;].fillna(\u0026#39;\u0026#39;)).reshape(-1, 1)).todense()) df_cluster = df_cluster.add_prefix(\u0026#39;cluster_\u0026#39;) # In[18]: le_zone = LabelEncoder() ohe_zone = OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;) df_zone = pd.DataFrame(ohe_zone.fit_transform(le_zone.fit_transform(df[\u0026#39;zone\u0026#39;].fillna(\u0026#39;\u0026#39;)).reshape(-1, 1)).todense()) df_zone = df_zone.add_prefix(\u0026#39;zone_\u0026#39;) # In[19]: le_weekday = LabelEncoder() ohe_weekday = OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;) df_weekday = pd.DataFrame(ohe_weekday.fit_transform(le_weekday.fit_transform(df[\u0026#39;weekday\u0026#39;]).reshape(-1, 1)).todense()) df_weekday = df_weekday.add_prefix(\u0026#39;weekday_\u0026#39;) # In[20]: client = boto3.client(\u0026#39;s3\u0026#39;) client.put_object(Body=pickle.dumps(le_cluster), Bucket=bucket, Key=\u0026#39;preprocessing/le_cluster.pkl\u0026#39;); # In[21]: client.put_object(Body=pickle.dumps(ohe_cluster), Bucket=bucket, Key=\u0026#39;preprocessing/ohe_cluster.pkl\u0026#39;) client.put_object(Body=pickle.dumps(le_zone), Bucket=bucket, Key=\u0026#39;preprocessing/le_zone.pkl\u0026#39;) client.put_object(Body=pickle.dumps(ohe_zone), Bucket=bucket, Key=\u0026#39;preprocessing/ohe_zone.pkl\u0026#39;) client.put_object(Body=pickle.dumps(le_weekday), Bucket=bucket, Key=\u0026#39;preprocessing/le_weekday.pkl\u0026#39;) client.put_object(Body=pickle.dumps(ohe_weekday), Bucket=bucket, Key=\u0026#39;preprocessing/ohe_weekday.pkl\u0026#39;); # ## Write to S3 resulting ETL # # Now we have to write to S3 all the relevant columns. We will perform a train/validation split of the customers so we can train on a group and get relevant metrics on the other. # In[29]: df = df[[\u0026#39;next_bill\u0026#39;, \u0026#39;bill\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;id_reseller\u0026#39;, \u0026#39;mean-last-30\u0026#39;, \u0026#39;mean-last-7\u0026#39;, \u0026#39;std-last-30\u0026#39;, \u0026#39;days_without_purchase\u0026#39;, \u0026#39;weekday\u0026#39;, \u0026#39;last_bill\u0026#39;, \u0026#39;zone\u0026#39;, \u0026#39;cluster\u0026#39;]] df = pd.concat([df,df_cluster,df_zone,df_weekday],axis=1) #Take a random 10% sample of the resellers val_resellers = list(pd.Series(df[\u0026#39;id_reseller\u0026#39;].unique()).sample(frac=0.1)) df_train = df[~df[\u0026#39;id_reseller\u0026#39;].isin(val_resellers)].sample(frac=1) df_validation = df[df[\u0026#39;id_reseller\u0026#39;].isin(val_resellers)].sample(frac=1) df_train.drop([\u0026#39;date\u0026#39;,\u0026#39;id_reseller\u0026#39;,\u0026#39;bill\u0026#39;,\u0026#39;zone\u0026#39;,\u0026#39;cluster\u0026#39;,\u0026#39;weekday\u0026#39;],axis=1,inplace=True) df_validation.drop([\u0026#39;date\u0026#39;,\u0026#39;id_reseller\u0026#39;,\u0026#39;bill\u0026#39;,\u0026#39;zone\u0026#39;,\u0026#39;cluster\u0026#39;,\u0026#39;weekday\u0026#39;],axis=1,inplace=True) write_dataframe_to_csv_on_s3(df_validation, bucket, \u0026#39;validation/validation.csv\u0026#39;) write_dataframe_to_csv_on_s3(df_train, bucket, \u0026#39;train/train.csv\u0026#39;) ##### # Preprocessing Pipeline ##### df_r = awswrangler.athena.read( \u0026#34;implementationdb\u0026#34;, \u0026#34;select * from reseller\u0026#34; ) df = awswrangler.athena.read( \u0026#34;implementationdb\u0026#34;, \u0026#34;select * from billing\u0026#34; ) df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#39;date\u0026#39;]) max_date = df[\u0026#39;date\u0026#39;].max() min_date = max_date - pd.Timedelta(days=30) df = df[(df[\u0026#39;date\u0026#39;] \u0026gt; min_date)] def completeItem(dfItem,max_date,min_date): r = pd.date_range(start=min_date, end=max_date) dfItemNew = dfItem.set_index(\u0026#39;date\u0026#39;).reindex(r).fillna(0.0).rename_axis(\u0026#39;date\u0026#39;).reset_index() dfItemNew[\u0026#39;id_reseller\u0026#39;] = dfItem[\u0026#39;id_reseller\u0026#39;].max() return dfItemNew dfCompletedList = [] for nid,item in df.groupby(\u0026#39;id_reseller\u0026#39;): dfCompletedList.append(completeItem(item,max_date,min_date)) dfCompleted = pd.concat(dfCompletedList).copy() df = dfCompleted del dfCompleted del dfCompletedList def complete_info(group): weekday = (max_date + pd.Timedelta(days=1)).weekday_name mean_last_30 = group[\u0026#39;bill\u0026#39;].replace(0,np.nan).mean() std_last_30 = group[\u0026#39;bill\u0026#39;].replace(0,np.nan).std() date_last_bill = group[group[\u0026#39;bill\u0026#39;] != 0][\u0026#39;date\u0026#39;].max() days_without_purchase = (max_date + pd.Timedelta(days=1) - date_last_bill).days mean_last_7 = group[(group[\u0026#39;date\u0026#39;] \u0026gt;= max_date - pd.Timedelta(days=6))][\u0026#39;bill\u0026#39;].replace(0,np.nan).mean() last_bill = group[group[\u0026#39;bill\u0026#39;] \u0026gt; 0].sort_values(\u0026#39;date\u0026#39;,ascending=False).head(1)[\u0026#39;bill\u0026#39;].values[0] return {\u0026#39;weekday\u0026#39;:weekday,\u0026#39;mean-last-30\u0026#39;:mean_last_30, \u0026#39;std-last-30\u0026#39;:std_last_30,\u0026#39;mean-last-7\u0026#39;:mean_last_7,\u0026#39;last_bill\u0026#39;:last_bill, \u0026#39;id_reseller\u0026#39;:group[\u0026#39;id_reseller\u0026#39;].max(), \u0026#39;days_without_purchase\u0026#39;:days_without_purchase} features = [] for index,group in df.groupby(\u0026#39;id_reseller\u0026#39;): features.append(complete_info(group)) df_features = pd.DataFrame(features) df_features = df_features.merge(df_r,how=\u0026#39;inner\u0026#39;,on=\u0026#39;id_reseller\u0026#39;) pipe_list = [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday] df_cluster = pd.DataFrame( pipe_list[1].transform(pipe_list[0].transform(df_features[\u0026#39;cluster\u0026#39;]).reshape(-1, 1)).todense() ) df_cluster = df_cluster.add_prefix(\u0026#39;cluster_\u0026#39;) df_zone = pd.DataFrame( pipe_list[3].transform(pipe_list[2].transform(df_features[\u0026#39;zone\u0026#39;]).reshape(-1, 1)).todense() ) df_zone = df_zone.add_prefix(\u0026#39;zone_\u0026#39;) df_weekday = pd.DataFrame( pipe_list[5].transform(pipe_list[4].transform(df_features[\u0026#39;weekday\u0026#39;]).reshape(-1, 1)).todense() ) df_weekday = df_weekday.add_prefix(\u0026#39;weekday_\u0026#39;) df_to_predict = pd.concat([df_features,df_cluster,df_zone,df_weekday],axis=1) df_to_predict_feats = df_to_predict[[\u0026#39;mean-last-30\u0026#39;, \u0026#39;mean-last-7\u0026#39;, \u0026#39;std-last-30\u0026#39;, \u0026#39;days_without_purchase\u0026#39;, \u0026#39;last_bill\u0026#39;, \u0026#39;cluster_0\u0026#39;, \u0026#39;cluster_1\u0026#39;, \u0026#39;cluster_2\u0026#39;, \u0026#39;cluster_3\u0026#39;, \u0026#39;cluster_4\u0026#39;, \u0026#39;zone_0\u0026#39;, \u0026#39;zone_1\u0026#39;, \u0026#39;zone_2\u0026#39;, \u0026#39;zone_3\u0026#39;, \u0026#39;zone_4\u0026#39;, \u0026#39;zone_5\u0026#39;, \u0026#39;zone_6\u0026#39;, \u0026#39;zone_7\u0026#39;, \u0026#39;zone_8\u0026#39;, \u0026#39;zone_9\u0026#39;, \u0026#39;zone_10\u0026#39;, \u0026#39;zone_11\u0026#39;, \u0026#39;zone_12\u0026#39;, \u0026#39;zone_13\u0026#39;, \u0026#39;zone_14\u0026#39;, \u0026#39;zone_15\u0026#39;, \u0026#39;zone_16\u0026#39;, \u0026#39;zone_17\u0026#39;, \u0026#39;zone_18\u0026#39;, \u0026#39;zone_19\u0026#39;, \u0026#39;zone_20\u0026#39;, \u0026#39;zone_21\u0026#39;, \u0026#39;zone_22\u0026#39;, \u0026#39;zone_23\u0026#39;, \u0026#39;zone_24\u0026#39;, \u0026#39;zone_25\u0026#39;, \u0026#39;zone_26\u0026#39;, \u0026#39;zone_27\u0026#39;, \u0026#39;zone_28\u0026#39;, \u0026#39;zone_29\u0026#39;, \u0026#39;zone_30\u0026#39;, \u0026#39;zone_31\u0026#39;, \u0026#39;zone_32\u0026#39;, \u0026#39;zone_33\u0026#39;, \u0026#39;zone_34\u0026#39;, \u0026#39;zone_35\u0026#39;, \u0026#39;zone_36\u0026#39;, \u0026#39;zone_37\u0026#39;, \u0026#39;zone_38\u0026#39;, \u0026#39;zone_39\u0026#39;, \u0026#39;zone_40\u0026#39;, \u0026#39;zone_41\u0026#39;, \u0026#39;zone_42\u0026#39;, \u0026#39;zone_43\u0026#39;, \u0026#39;zone_44\u0026#39;, \u0026#39;zone_45\u0026#39;, \u0026#39;zone_46\u0026#39;, \u0026#39;zone_47\u0026#39;, \u0026#39;zone_48\u0026#39;, \u0026#39;zone_49\u0026#39;, \u0026#39;zone_50\u0026#39;, \u0026#39;zone_51\u0026#39;, \u0026#39;weekday_0\u0026#39;, \u0026#39;weekday_1\u0026#39;, \u0026#39;weekday_2\u0026#39;, \u0026#39;weekday_3\u0026#39;, \u0026#39;weekday_4\u0026#39;, \u0026#39;weekday_5\u0026#39;, \u0026#39;weekday_6\u0026#39;]] write_dataframe_to_csv_on_s3(df_to_predict_feats,bucket,\u0026#39;to_predict.csv\u0026#39;) write_dataframe_to_csv_on_s3(df_to_predict[[\u0026#39;id_reseller\u0026#39;]],bucket,\u0026#39;id_reseller_to_predict.csv\u0026#39;) Hit Save and Run job, no Parameters. "
},
{
	"uri": "/airflow/highlevel.html",
	"title": "High-Level Solution",
	"tags": [],
	"description": "",
	"content": "Weâ€™ll start by exploring the data, transforming the data, and training a model on the data. Weâ€™ll fit the ML model using an Amazon SageMaker managed training cluster. Weâ€™ll then deploy to an endpoint to perform batch predictions on the test data set. All of these tasks will be plugged into a workflow that can be orchestrated and automated through Apache Airflow integration with Amazon SageMaker.\nThe following diagram shows the ML workflow weâ€™ll implement for building the recommender system.\nThe workflow performs the following tasks:\nData pre-processing: Extract and pre-process data from Amazon S3 to prepare the training data. Prepare training data: To build the recommender system, weâ€™ll use the Amazon SageMaker built-in algorithm, Factorization machines. The algorithm expects training data only in recordIO-protobuf format with Float32 tensors. In this task, pre-processed data will be transformed to RecordIO Protobuf format. Training the model: Train the Amazon SageMaker built-in factorization machine model with the training data and generate model artifacts. The training job will be launched by the Airflow Amazon SageMaker operator. Tune the model hyperparameters: A conditional/optional task to tune the hyperparameters of the factorization machine to find the best model. The hyperparameter tuning job will be launched by the Amazon SageMaker Airflow operator. Batch inference: Using the trained model, get inferences on the test dataset stored in Amazon S3 using the Airflow Amazon SageMaker operator. "
},
{
	"uri": "/airflow/airflowconcepts.html",
	"title": "Airflow Concepts",
	"tags": [],
	"description": "",
	"content": "Before implementing the solution, letâ€™s get familiar with Airflow concepts. If you are already familiar with Airflow concepts, skip to the Airflow Amazon SageMaker operators section.\nApache Airflow is an open-source tool for orchestrating workflows and data processing pipelines. Airflow allows you to configure, schedule, and monitor data pipelines programmatically in Python to define all the stages of the lifecycle of a typical workflow management.\nAirflow nomenclature DAG (Directed Acyclic Graph): DAGs describe how to run a workflow by defining the pipeline in Python, that is configuration as code. Pipelines are designed as a directed acyclic graph by dividing a pipeline into tasks that can be executed independently. Then these tasks are combined logically as a graph. Operators: Operators are atomic components in a DAG describing a single task in the pipeline. They determine what gets done in that task when a DAG runs. Airflow provides operators for common tasks. It is extensible, so you can define custom operators. Airflow Amazon SageMaker operators are one of these custom operators contributed by AWS to integrate Airflow with Amazon SageMaker. Task: After an operator is instantiated, itâ€™s referred to as a â€œtask.â€ Task instance: A task instance represents a specific run of a task characterized by a DAG, a task, and a point in time. Scheduling: The DAGs and tasks can be run on demand or can be scheduled to be run at a certain frequency defined as a cron expression in the DAG. Airflow architecture The following diagram shows the typical components of Airflow architecture.\nScheduler: The scheduler is a persistent service that monitors DAGs and tasks, and triggers the task instances whose dependencies have been met. The scheduler is responsible for invoking the executor defined in the Airflow configuration. Executor: Executors are the mechanism by which task instances get to run. Airflow by default provides different types of executors and you can define custom executors, such as a Kubernetes executor. Broker: The broker queues the messages (task requests to be executed) and acts as a communicator between the executor and the workers. Workers: The actual nodes where tasks are executed and that return the result of the task. Web server: A web server to render the Airflow UI. Configuration file: Configure settings such as executor to use, airflow metadata database connections, DAG, and repository location. You can also define concurrency and parallelism limits, etc. Metadata database: Database to store all the metadata related to the DAGS, DAG runs, tasks, variables, and connections. Airflow Amazon SageMaker operators Amazon SageMaker operators are custom operators available with Airflow installation allowing Airflow to talk to Amazon SageMaker and perform the following ML tasks:\nSageMakerTrainingOperator: Creates an Amazon SageMaker training job. SageMakerTuningOperator: Creates an AmazonSageMaker hyperparameter tuning job. SageMakerTransformOperator: Creates an Amazon SageMaker batch transform job. SageMakerModelOperator: Creates an Amazon SageMaker model. SageMakerEndpointConfigOperator: Creates an Amazon SageMaker endpoint config. SageMakerEndpointOperator: Creates an Amazon SageMaker endpoint to make inference calls. "
},
{
	"uri": "/step/orchestration.html",
	"title": "Orchestration",
	"tags": [],
	"description": "",
	"content": "Now it\u0026rsquo;s time to create all the necesary steps to schedule the training, deployment and inference with the model. For this, we are going to use an architecture similar to the serverless sagemaker orchestration but adapted to our specific problem.\nFirst, we need to create lambda functions capables of:\nTraining the model Awaiting for training Deploying the model Awaiting for deploy Predict, save predictions and delete the endpoint So, let\u0026rsquo;s do it\n"
},
{
	"uri": "/airflow/airflowsetup.html",
	"title": "Airflow Setup",
	"tags": [],
	"description": "",
	"content": "We will set up a simple Airflow architecture with a scheduler, worker, and web server running on a single instance. Typically, you will not use this setup for production workloads. We will use AWS CloudFormation to launch the AWS services required to create the components in this blog post. The following diagram shows the configuration of the architecture to be deployed.\nThe stack includes the following:\nAn Amazon Elastic Compute Cloud (EC2) instance to set up the Airflow components. An Amazon Relational Database Service (RDS) Postgres instance to host the Airflow metadata database. An Amazon Simple Storage Service (S3) bucket to store the Amazon SageMaker model artifacts, outputs, and Airflow DAG with ML workflow. The template will prompt for the S3 bucket name. AWS Identity and Access Management (IAM) roles and Amazon EC2 security groups to allow Airflow components to interact with the metadata database, S3 bucket, and Amazon SageMaker. The prerequisite for running this CloudFormation script is to set up an Amazon EC2 Key Pair to log in to manage Airflow, for example, if you want to troubleshoot or add custom operators. Airflow 1.10.12 RDS\nAirflow 1.10.12 Aurora Serverless\nAirflow 2.0.2 RDS\nAirflow 2.0.2 Aurora Serverless\nIt might take up to 10 minutes for the CloudFormation stack to create the resources. After the resource creation is completed, you should be able to log in to Airflow web UI. The Airflow web server runs on port 8080 by default. To open the Airflow web UI, open any browser, and type in the URL here http://ec2-public-dns-name:8080. The public DNS name of the EC2 instance can be found on the Outputs tab of CloudFormation stack on the AWS CloudFormation console. (if you are an Amazon employee, disconnect the VPN to try the URL)\n"
},
{
	"uri": "/step/createrole.html",
	"title": "Create Role",
	"tags": [],
	"description": "",
	"content": "Create a role with SageMaker and S3 access To execute this lambdas we are going to need a role SageMaker and S3 permissions.\nFollow this link:\nhttps://console.aws.amazon.com/iam/home?region=us-east-1#/roles$new?step=type\nSelect Lambda - Next In Permissions: Select AmazonSageMakerFullAccess and AmazonS3FullAccess In Tags: Next In Name: workshop-role - Create role Make sure AmazonSageMakerFullAccess and AmazonS3FullAccess are in place Go to the Trust relationships tab and click Edit trust relationship Delete the Policy Document and paste: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;sagemaker.amazonaws.com\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Click on \u0026ldquo;Update Trust Policy\u0026rdquo; "
},
{
	"uri": "/airflow/buildingworkflow.html",
	"title": "Building an ML Workflow",
	"tags": [],
	"description": "",
	"content": "In this section, weâ€™ll create a ML workflow using Airflow operators, including Amazon SageMaker operators to build the recommender. You can download the companion Jupyter notebook to look at individual tasks used in the ML workflow. Weâ€™ll highlight the most important pieces here.\nData preprocessing As mentioned earlier, the dataset contains ratings from over 2 million Amazon customers on over 160,000 digital videos. More details on the dataset are here. After analyzing the dataset, we see that there are only about 5 percent of customers who have rated 5 or more videos, and only 25 percent of videos have been rated by 9+ customers. Weâ€™ll clean this long tail by filtering the records. After cleanup, we transform the data into sparse format by giving each customer and video their own sequential index indicating the row and column in our ratings matrix. We store this cleansed data in an S3 bucket for the next task to pick up and process. The following PythonOperator snippet in the Airflow DAG calls the preprocessing function: # preprocess the data preprocess_task = PythonOperator( task_id=\u0026#39;preprocessing\u0026#39;, dag=dag, provide_context=False, python_callable=preprocess.preprocess, op_kwargs=config[\u0026#34;preprocess_data\u0026#34;]) NOTE: For this blog post, the data preprocessing task is performed in Python using the Pandas package. The task gets executed on the Airflow worker node. This task can be replaced with the code running on AWS Glue or Amazon EMR when working with large data sets.\nData Preparation We are using the Amazon SageMaker implementation of Factorization Machines (FM) for building the recommender system. The algorithm expects Float32 tensors in recordIO protobuf format. The cleansed data set is a Pandas DataFrame on disk. As part of data preparation, the Pandas DataFrame will be transformed to a sparse matrix with one-hot encoded feature vectors with customers and videos. Thus, each sample in the data set will be a wide Boolean vector with only two values set to 1 for the customer and the video. The following steps are performed in the data preparation task: Split the cleaned data set into train and test data sets. Build a sparse matrix with one-hot encoded feature vectors (customer + videos) and a label vector with star ratings. Convert both the sets to protobuf encoded files. Copy the prepared files to an Amazon S3 bucket for training the model. The following PythonOperator snippet in the Airflow DAG calls the data preparation function. # prepare the data for training prepare_task = PythonOperator( task_id=\u0026#39;preparing\u0026#39;, dag=dag, provide_context=False, python_callable=prepare.prepare, op_kwargs=config[\u0026#34;prepare_data\u0026#34;] ) Model training and tuning Weâ€™ll train the Amazon SageMaker Factorization Machine algorithm by launching a training job using Airflow Amazon SageMaker Operators. There are couple of ways we can train the model. Use SageMakerTrainingOperator to run a training job by setting the hyperparameters known to work for your data. # train_config specifies SageMaker training configuration train_config = training_config( estimator=fm_estimator, inputs=config[\u0026#34;train_model\u0026#34;][\u0026#34;inputs\u0026#34;]) # launch sagemaker training job and wait until it completes train_model_task = SageMakerTrainingOperator( task_id=\u0026#39;model_training\u0026#39;, dag=dag, config=train_config, aws_conn_id=\u0026#39;airflow-sagemaker\u0026#39;, wait_for_completion=True, check_interval=30 ) Use SageMakerTuningOperator to run a hyperparameter tuning job to find the best model by running many jobs that test a range of hyperparameters on your dataset. # create tuning config tuner_config = tuning_config( tuner=fm_tuner, inputs=config[\u0026#34;tune_model\u0026#34;][\u0026#34;inputs\u0026#34;]) tune_model_task = SageMakerTuningOperator( task_id=\u0026#39;model_tuning\u0026#39;, dag=dag, config=tuner_config, aws_conn_id=\u0026#39;airflow-sagemaker\u0026#39;, wait_for_completion=True, check_interval=30 ) Conditional tasks can be created in the Airflow DAG that can decide whether to run the training job directly or run a hyperparameter tuning job to find the best model. These tasks can be run in synchronous or asynchronous mode. branching = BranchPythonOperator( task_id=\u0026#39;branching\u0026#39;, dag=dag, python_callable=lambda: \u0026#34;model_tuning\u0026#34; if hpo_enabled else \u0026#34;model_training\u0026#34;) The progress of the training or tuning job can be monitored in the Airflow Task Instance logs. Model inference Using the Airflow SageMakerTransformOperator, create an Amazon SageMaker batch transform job to perform batch inference on the test dataset to evaluate performance of the model.\n# create transform config transform_config = transform_config_from_estimator( estimator=fm_estimator, task_id=\u0026#34;model_tuning\u0026#34; if hpo_enabled else \u0026#34;model_training\u0026#34;, task_type=\u0026#34;tuning\u0026#34; if hpo_enabled else \u0026#34;training\u0026#34;, **config[\u0026#34;batch_transform\u0026#34;][\u0026#34;transform_config\u0026#34;] ) # launch sagemaker batch transform job and wait until it completes batch_transform_task = SageMakerTransformOperator( task_id=\u0026#39;predicting\u0026#39;, dag=dag, config=transform_config, aws_conn_id=\u0026#39;airflow-sagemaker\u0026#39;, wait_for_completion=True, check_interval=30, trigger_rule=TriggerRule.ONE_SUCCESS ) We can further extend the ML workflow by adding a task to validate model performance by comparing the actual and predicted customer ratings before deploying the model in production environment. In the next section, weâ€™ll see how all these tasks are stitched together to form a ML workflow in an Airflow DAG.\n"
},
{
	"uri": "/step/trainmodel.html",
	"title": "Train Model Lambda",
	"tags": [],
	"description": "",
	"content": " Go to the AWS Console and under Services, select Lambda Go to the Functions Pane and select Create Function Author from scratch Or follow this link https://console.aws.amazon.com/lambda/home?region=us-east-1#/create?firstrun=true Parameters for the function: * Name: lambdaModelTrain * Runtime Python 3.6 * Executing role: Use an existing role and select the role you created in the previous step (workshop-role) - Create function This lambda function doesn\u0026rsquo;t need to receive any parameters, but it should return the resulting hyperparameter tunning optimization job name.\nWe will use that job name in the next lambda function to check status, the container it used and that the status is now \u0026ldquo;In Progress\u0026rdquo;.\nLet\u0026rsquo;s copy this code into the editor.\nTake special care of:\nLine 9: role (the Role ARN of workshop-role, also in this link https://console.aws.amazon.com/iam/home?region=us-east-1#/roles/workshop-role) Line 12: bucket_path (your bucket for the results) import json import boto3 import copy from time import gmtime, strftime region = boto3.Session().region_name smclient = boto3.Session().client(\u0026#39;sagemaker\u0026#39;) role = \u0026#39;arn:aws:iam::1111111111:role/role/workshop-role\u0026#39; bucket_path=\u0026#39;s3://bucket-name\u0026#39; prefix = \u0026#34;invoice-forecast\u0026#34; container = \u0026#39;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#39; def lambda_handler(event, context): tuning_job_config = \\ { \u0026#34;ParameterRanges\u0026#34;: { \u0026#34;CategoricalParameterRanges\u0026#34;: [], \u0026#34;ContinuousParameterRanges\u0026#34;: [ { \u0026#34;MaxValue\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;MinValue\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;eta\u0026#34; }, { \u0026#34;MaxValue\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;MinValue\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;alpha\u0026#34; }, { \u0026#34;MaxValue\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;MinValue\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;min_child_weight\u0026#34; } ], \u0026#34;IntegerParameterRanges\u0026#34;: [ { \u0026#34;MaxValue\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;MinValue\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;max_depth\u0026#34; } ] }, \u0026#34;ResourceLimits\u0026#34;: { \u0026#34;MaxNumberOfTrainingJobs\u0026#34;: 5, \u0026#34;MaxParallelTrainingJobs\u0026#34;: 5 }, \u0026#34;Strategy\u0026#34;: \u0026#34;Bayesian\u0026#34;, \u0026#34;HyperParameterTuningJobObjective\u0026#34;: { \u0026#34;MetricName\u0026#34;: \u0026#34;validation:mae\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Minimize\u0026#34; } } training_job_definition = \\ { \u0026#34;AlgorithmSpecification\u0026#34;: { \u0026#34;TrainingImage\u0026#34;: container, \u0026#34;TrainingInputMode\u0026#34;: \u0026#34;File\u0026#34; }, \u0026#34;RoleArn\u0026#34;: role, \u0026#34;OutputDataConfig\u0026#34;: { \u0026#34;S3OutputPath\u0026#34;: bucket_path + \u0026#34;/\u0026#34;+ prefix + \u0026#34;/xgboost\u0026#34; }, \u0026#34;ResourceConfig\u0026#34;: { \u0026#34;InstanceCount\u0026#34;: 2, \u0026#34;InstanceType\u0026#34;: \u0026#34;ml.m4.xlarge\u0026#34;, \u0026#34;VolumeSizeInGB\u0026#34;: 5 }, \u0026#34;StaticHyperParameters\u0026#34;: { \u0026#34;objective\u0026#34;: \u0026#34;reg:linear\u0026#34;, \u0026#34;num_round\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;subsample\u0026#34;:\u0026#34;0.7\u0026#34;, \u0026#34;eval_metric\u0026#34;:\u0026#34;mae\u0026#34; }, \u0026#34;StoppingCondition\u0026#34;: { \u0026#34;MaxRuntimeInSeconds\u0026#34;: 86400 }, \u0026#34;InputDataConfig\u0026#34;: [ { \u0026#34;ChannelName\u0026#34;: \u0026#34;train\u0026#34;, \u0026#34;DataSource\u0026#34;: { \u0026#34;S3DataSource\u0026#34;: { \u0026#34;S3DataType\u0026#34;: \u0026#34;S3Prefix\u0026#34;, \u0026#34;S3Uri\u0026#34;: bucket_path + \u0026#39;/train/\u0026#39;, \u0026#34;S3DataDistributionType\u0026#34;: \u0026#34;FullyReplicated\u0026#34; } }, \u0026#34;ContentType\u0026#34;: \u0026#34;text/csv\u0026#34;, \u0026#34;CompressionType\u0026#34;: \u0026#34;None\u0026#34; }, { \u0026#34;ChannelName\u0026#34;: \u0026#34;validation\u0026#34;, \u0026#34;DataSource\u0026#34;: { \u0026#34;S3DataSource\u0026#34;: { \u0026#34;S3DataType\u0026#34;: \u0026#34;S3Prefix\u0026#34;, \u0026#34;S3Uri\u0026#34;: bucket_path + \u0026#39;/validation/\u0026#39;, \u0026#34;S3DataDistributionType\u0026#34;: \u0026#34;FullyReplicated\u0026#34; } }, \u0026#34;ContentType\u0026#34;: \u0026#34;text/csv\u0026#34;, \u0026#34;CompressionType\u0026#34;: \u0026#34;None\u0026#34; } ] } tuning_job_name = prefix + strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;, gmtime()) event[\u0026#34;container\u0026#34;] = container event[\u0026#34;stage\u0026#34;] = \u0026#34;Training\u0026#34; event[\u0026#34;status\u0026#34;] = \u0026#34;InProgress\u0026#34; event[\u0026#39;name\u0026#39;] = tuning_job_name smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name, HyperParameterTuningJobConfig = tuning_job_config, TrainingJobDefinition = training_job_definition) # output return event Hit Save Test the lambda function\nCreate a test event with no parameters save and test. In your AWS console under SageMaker\u0026gt;Hyperparamter tunning jobs you should see the HPO runnning.\nhttps://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs\n"
},
{
	"uri": "/step/awaitmodel.html",
	"title": "Await Model Lambda",
	"tags": [],
	"description": "",
	"content": " Go to the AWS Console and under Services, select Lambda Go to the Functions Pane and select Create Function Author from scratch Or follow this link https://console.aws.amazon.com/lambda/home?region=us-east-1#/create?firstrun=true Parameters for the function: Name: lambdaModelAwait Runtime: Python 3.6 Executing role: Use an existing role and select the role you created in the previous step (workshop-role) - Create function This lambda function, now receives the output of the previous step and allows us to check if the process is done or not. If it\u0026rsquo;s done, it returns the name of the best training job.\nIn the code editor paste the following code:\nimport boto3 import os sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) def lambda_handler(event, context): stage = event[\u0026#39;stage\u0026#39;] if stage == \u0026#39;Training\u0026#39;: name = event[\u0026#39;name\u0026#39;] training_details = describe_training_job(name) print(training_details) status = training_details[\u0026#39;HyperParameterTuningJobStatus\u0026#39;] if status == \u0026#39;Completed\u0026#39;: s3_output_path = training_details[\u0026#39;TrainingJobDefinition\u0026#39;][\u0026#39;OutputDataConfig\u0026#39;][\u0026#39;S3OutputPath\u0026#39;] model_data_url = os.path.join(s3_output_path, training_details[\u0026#39;BestTrainingJob\u0026#39;][\u0026#39;TrainingJobName\u0026#39;], \u0026#39;output/model.tar.gz\u0026#39;) event[\u0026#39;message\u0026#39;] = \u0026#39;HPO tunning job \u0026#34;{}\u0026#34; complete. Model data uploaded to \u0026#34;{}\u0026#34;\u0026#39;.format(name, model_data_url) event[\u0026#39;model_data_url\u0026#39;] = model_data_url event[\u0026#39;best_training_job\u0026#39;] = training_details[\u0026#39;BestTrainingJob\u0026#39;][\u0026#39;TrainingJobName\u0026#39;] elif status == \u0026#39;Failed\u0026#39;: failure_reason = training_details[\u0026#39;FailureReason\u0026#39;] event[\u0026#39;message\u0026#39;] = \u0026#39;Training job failed. {}\u0026#39;.format(failure_reason) elif stage == \u0026#39;Deployment\u0026#39;: name = \u0026#39;demobb-invoice-prediction\u0026#39; endpoint_details = describe_endpoint(name) status = endpoint_details[\u0026#39;EndpointStatus\u0026#39;] if status == \u0026#39;InService\u0026#39;: event[\u0026#39;message\u0026#39;] = \u0026#39;Deployment completed for endpoint \u0026#34;{}\u0026#34;.\u0026#39;.format(name) elif status == \u0026#39;Failed\u0026#39;: failure_reason = endpoint_details[\u0026#39;FailureReason\u0026#39;] event[\u0026#39;message\u0026#39;] = \u0026#39;Deployment failed for endpoint \u0026#34;{}\u0026#34;. {}\u0026#39;.format(name, failure_reason) elif status == \u0026#39;RollingBack\u0026#39;: event[\u0026#39;message\u0026#39;] = \u0026#39;Deployment failed for endpoint \u0026#34;{}\u0026#34;, rolling back to previously deployed version.\u0026#39;.format(name) event[\u0026#39;status\u0026#39;] = status return event def describe_training_job(name): \u0026#34;\u0026#34;\u0026#34; Describe SageMaker training job identified by input name. Args: name (string): Name of SageMaker training job to describe. Returns: (dict) Dictionary containing metadata and details about the status of the training job. \u0026#34;\u0026#34;\u0026#34; try: response = sagemaker.describe_hyper_parameter_tuning_job( HyperParameterTuningJobName=name ) except Exception as e: print(e) print(\u0026#39;Unable to describe hyperparameter tunning job.\u0026#39;) raise(e) return response def describe_endpoint(name): \u0026#34;\u0026#34;\u0026#34; Describe SageMaker endpoint identified by input name. Args: name (string): Name of SageMaker endpoint to describe. Returns: (dict) Dictionary containing metadata and details about the status of the endpoint. \u0026#34;\u0026#34;\u0026#34; try: response = sagemaker.describe_endpoint( EndpointName=name ) except Exception as e: print(e) print(\u0026#39;Unable to describe endpoint.\u0026#39;) raise(e) return response Previous response (for using in the test event):\nThe name of the training job can be found here: https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs\n{ \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Training\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast11111111111111\u0026#34; } First, you will see a response like\nResponse: { \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Training\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast20190702190151\u0026#34; } Once the training is completed, the state is going to change and you\u0026rsquo;ll see the new status and the name of the best training job.\n{ \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Training\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Completed\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast20190702190151\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;HPO tunning job \\\u0026#34;invoice-forecast20190702190151\\\u0026#34; complete. Model data uploaded to \\\u0026#34;s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\\\u0026#34;\u0026#34;, \u0026#34;model_data_url\u0026#34;: \u0026#34;s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\u0026#34;, \u0026#34;best_training_job\u0026#34;: \u0026#34;invoice-forecast20190702190151-005-dde9844e\u0026#34; } "
},
{
	"uri": "/step/deploymodel.html",
	"title": "Deploy Model Lambda",
	"tags": [],
	"description": "",
	"content": "Deploy Model In SageMaker: Lambda Function\nIn this lambda function, we are going to need to use the best training job from the previous step to deploy a predictor.\nGo to the AWS Console and under Services, select Lambda Go to the Functions Pane and select Create Function Author from scratch Or follow this link https://console.aws.amazon.com/lambda/home?region=us-east-1#/create?firstrun=true Parameters for the function: Name: lambdaModelDeploy Runtime: Python 3.6 Executing role: Use an existing role and select the role you created in the previous step (workshop-role) - Create function The ARN for the variable EXECUTION_ROLE can be found here: https://console.aws.amazon.com/iam/home?region=us-east-1#/roles/workshop-role\nimport boto3 import os sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) EXECUTION_ROLE = \u0026#39;arn:aws:iam::1111111111112:role/service-role/AmazonSageMaker-ExecutionRole-111111111111\u0026#39; INSTANCE_TYPE = \u0026#39;ml.m4.xlarge\u0026#39; container = \u0026#39;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#39; def lambda_handler(event, context): best_training_job = event[\u0026#39;best_training_job\u0026#39;] endpoint = \u0026#39;demobb-invoice-prediction\u0026#39; model_data_url = event[\u0026#39;model_data_url\u0026#39;] print(\u0026#39;Creating model resource from training artifact...\u0026#39;) create_model(best_training_job, container, model_data_url) print(\u0026#39;Creating endpoint configuration...\u0026#39;) create_endpoint_config(best_training_job) print(\u0026#39;There is no existing endpoint for this model. Creating new model endpoint...\u0026#39;) create_endpoint(endpoint, best_training_job) event[\u0026#39;stage\u0026#39;] = \u0026#39;Deployment\u0026#39; event[\u0026#39;status\u0026#39;] = \u0026#39;Creating\u0026#39; event[\u0026#39;message\u0026#39;] = \u0026#39;Started deploying model \u0026#34;{}\u0026#34; to endpoint \u0026#34;{}\u0026#34;\u0026#39;.format(best_training_job, endpoint) return event def create_model(name, container, model_data_url): \u0026#34;\u0026#34;\u0026#34; Create SageMaker model. Args: name (string): Name to label model with container (string): Registry path of the Docker image that contains the model algorithm model_data_url (string): URL of the model artifacts created during training to download to container Returns: (None) \u0026#34;\u0026#34;\u0026#34; try: sagemaker.create_model( ModelName=name, PrimaryContainer={ \u0026#39;Image\u0026#39;: container, \u0026#39;ModelDataUrl\u0026#39;: model_data_url }, ExecutionRoleArn=EXECUTION_ROLE ) except Exception as e: print(e) print(\u0026#39;Unable to create model.\u0026#39;) raise(e) def create_endpoint_config(name): \u0026#34;\u0026#34;\u0026#34; Create SageMaker endpoint configuration. Args: name (string): Name to label endpoint configuration with. Returns: (None) \u0026#34;\u0026#34;\u0026#34; try: sagemaker.create_endpoint_config( EndpointConfigName=name, ProductionVariants=[ { \u0026#39;VariantName\u0026#39;: \u0026#39;prod\u0026#39;, \u0026#39;ModelName\u0026#39;: name, \u0026#39;InitialInstanceCount\u0026#39;: 1, \u0026#39;InstanceType\u0026#39;: INSTANCE_TYPE } ] ) except Exception as e: print(e) print(\u0026#39;Unable to create endpoint configuration.\u0026#39;) raise(e) def create_endpoint(endpoint_name, config_name): \u0026#34;\u0026#34;\u0026#34; Create SageMaker endpoint with input endpoint configuration. Args: endpoint_name (string): Name of endpoint to create. config_name (string): Name of endpoint configuration to create endpoint with. Returns: (None) \u0026#34;\u0026#34;\u0026#34; try: sagemaker.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=config_name ) except Exception as e: print(e) print(\u0026#39;Unable to create endpoint.\u0026#39;) raise(e) On your SageMaker console you should see an endpoint with status creating. Once you test the output it should look like this: You can configure the test with this parameters, taking care of changing the parameters name , model_data_url and best_training_job received in the output of the test of lambdaModelAwait\n{ \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Creating\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast20190702190151\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Started deploying model \\\u0026#34;invoice-forecast20190702190151-005-dde9844e\\\u0026#34; to endpoint \\\u0026#34;demobb-invoice-prediction\\\u0026#34;\u0026#34;, \u0026#34;model_data_url\u0026#34;: \u0026#34;s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\u0026#34;, \u0026#34;best_training_job\u0026#34;: \u0026#34;invoice-forecast20190702190151-005-dde9844e\u0026#34; } This is a good chance to test the Await function this time with Deployment stage. If you create a new test event on the lambdaAwaitModel you should see a response like this: Response:\n{ \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Creating\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast20190702190151\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Started deploying model \\\u0026#34;invoice-forecast20190702190151-005-dde9844e\\\u0026#34; to endpoint \\\u0026#34;demobb-invoice-prediction\\\u0026#34;\u0026#34;, \u0026#34;model_data_url\u0026#34;: \u0026#34;s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\u0026#34;, \u0026#34;best_training_job\u0026#34;: \u0026#34;invoice-forecast20190702190151-005-dde9844e\u0026#34; } And you can check the progress in the AWS Console: https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints\nAfter the model is in service you should see something like this:\n{ \u0026#34;container\u0026#34;: \u0026#34;811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InService\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;invoice-forecast20190702190151\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Deployment completed for endpoint \\\u0026#34;demobb-invoice-prediction\\\u0026#34;.\u0026#34;, \u0026#34;model_data_url\u0026#34;: \u0026#34;s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\u0026#34;, \u0026#34;best_training_job\u0026#34;: \u0026#34;invoice-forecast20190702190151-005-dde9844e\u0026#34; } "
},
{
	"uri": "/airflow/puttingalltogether.html",
	"title": "Putting it all together",
	"tags": [],
	"description": "",
	"content": "Airflow DAG integrates all the tasks weâ€™ve described as a ML workflow. Airflow DAG is a Python script where you express individual tasks with Airflow operators, set task dependencies, and associate the tasks to the DAG to run on demand or at a scheduled interval. The Airflow DAG script is divided into following sections.\nSet DAG with parameters such as schedule interval, concurrency, etc. dag = DAG( dag_id=\u0026#39;sagemaker-ml-pipeline\u0026#39;, default_args=args, schedule_interval=None, concurrency=1, max_active_runs=1, user_defined_filters={\u0026#39;tojson\u0026#39;: lambda s: JSONEncoder().encode(s)} ) Set up training, tuning, and inference configurations for each operator using Amazon SageMaker Python SDK for Airflow Create individual tasks with Airflow operators that define trigger rules and associate them with the DAG object. Refer to the previous section for defining these individual tasks. Specify task dependencies. init.set_downstream(preprocess_task) preprocess_task.set_downstream(prepare_task) prepare_task.set_downstream(branching) branching.set_downstream(tune_model_task) branching.set_downstream(train_model_task) tune_model_task.set_downstream(batch_transform_task) train_model_task.set_downstream(batch_transform_task) batch_transform_task.set_downstream(cleanup_task) After the DAG is ready, deploy it to the Airflow DAG repository using CI/CD pipelines. If you followed the setup outlined in Airflow setup, the CloudFormation stack deployed to install Airflow components will add the Airflow DAG to the repository on the Airflow instance that has the ML workflow for building the recommender system.\nAfter triggering the DAG on demand or on a schedule, you can monitor the DAG in multiple ways: tree view, graph view, Gantt chart, task instance logs, etc.\n"
},
{
	"uri": "/airflow/cleanup.html",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Now to the final step, cleaning up the resources.\nTo avoid unnecessary charges on your AWS account do the following:\nDestroy all of the resources created by the CloudFormation stack in Airflow set up by deleting the stack after youâ€™re done experimenting with it. You can follow the steps here to delete the stack. You have to manually delete the S3 bucket created by the CloudFormation stack because AWS CloudFormation canâ€™t delete a non-empty Amazon S3 bucket. "
},
{
	"uri": "/step/usemodel.html",
	"title": "Use Model to Predict",
	"tags": [],
	"description": "",
	"content": "In this lambda function, we are going to use the deployed model to predict.\nGo to the AWS Console and under Services, select Lambda Go to the Functions Pane and select Create Function Author from scratch Or follow this link https://console.aws.amazon.com/lambda/home?region=us-east-1#/create?firstrun=true Parameters for the function: Name: lambdaModelPredict Runtime: Python 3.6 Executing role: Use an existing role and select the role you created in the previous step (workshop-role) - Create function This last lambda function doesn\u0026rsquo;t take any parameters, but in this case we need to touch the default parameters of the lambda to configure Max Memory in 1024 MB and Timeout in 15 Mins.\nChange the variable bucket for your bucket name\nimport os import io import boto3 import json import csv from io import StringIO # grab static variables sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) ENDPOINT_NAME = \u0026#39;demobb-invoice-prediction\u0026#39; runtime= boto3.client(\u0026#39;runtime.sagemaker\u0026#39;) bucket = \u0026#39;blackb-mggaska-implementation\u0026#39; s3 = boto3.client(\u0026#39;s3\u0026#39;) key = \u0026#39;to_predict.csv\u0026#39; def lambda_handler(event, context): response = s3.get_object(Bucket=bucket, Key=key) content = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) results = [] for line in content.splitlines(): response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=\u0026#39;text/csv\u0026#39;, Body=line) result = json.loads(response[\u0026#39;Body\u0026#39;].read().decode()) results.append(result) i = 0 multiLine = \u0026#34;\u0026#34; for item in results: if (i \u0026gt; 0): multiLine = multiLine + \u0026#39;\\n\u0026#39; multiLine = multiLine + str(item) i+=1 file_name = \u0026#34;predictions.csv\u0026#34; s3_resource = boto3.resource(\u0026#39;s3\u0026#39;) s3_resource.Object(bucket, file_name).put(Body=multiLine) event[\u0026#39;status\u0026#39;] = \u0026#39;Processed records \u0026#39; + str(len(results)) # Deleting Endpoint sagemaker.delete_endpoint(EndpointName=ENDPOINT_NAME) return event "
},
{
	"uri": "/airflow/conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, you have seen that building an ML workflow involves quite a bit of preparation but it helps improve the rate of experimentation, engineering productivity, and maintenance of repetitive ML tasks. Airflow Amazon SageMaker Operators provide a convenient way to build ML workflows and integrate with Amazon SageMaker.\nYou can extend the workflows by customizing the Airflow DAGs with any tasks that better fit your ML workflows, such as feature engineering, creating an ensemble of training models, creating parallel training jobs, and retraining models to adapt to the data distribution changes.\n"
},
{
	"uri": "/step/createstep.html",
	"title": "Create Step Function",
	"tags": [],
	"description": "",
	"content": "Now we have to create a step function to orchestrate the execution of all the previous step.\nFollow this link: https://console.aws.amazon.com/states/home?region=us-east-1#/statemachines/create\nThis is the resulting diagram:\nFor step function we use the ASL markup language. We have to change all the arn from 1111111111111 to our account number.\n{ \u0026#34;StartAt\u0026#34;: \u0026#34;ETL\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ETL\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::glue:startJobRun.sync\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;JobName\u0026#34;: \u0026#34;etlandpipeline\u0026#34; }, \u0026#34;Next\u0026#34;: \u0026#34;StartTrainingJob\u0026#34; }, \u0026#34;StartTrainingJob\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:1111111111111:function:lambdaModelTrain\u0026#34;, \u0026#34;ResultPath\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckStatusTraining\u0026#34; }, \u0026#34;CheckStatusTraining\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:1111111111111:function:lambdaModelAwait\u0026#34;, \u0026#34;ResultPath\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckTrainingBranch\u0026#34; }, \u0026#34;CheckTrainingBranch\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Or\u0026#34;: [{ \u0026#34;Variable\u0026#34;: \u0026#34;$.status\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Completed\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;StartDeployment\u0026#34; }, { \u0026#34;Or\u0026#34;: [{ \u0026#34;Variable\u0026#34;: \u0026#34;$.status\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;InProgress\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;WaitStatusTraining\u0026#34; } ] }, \u0026#34;WaitStatusTraining\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 60, \u0026#34;Next\u0026#34;: \u0026#34;CheckStatusTraining\u0026#34; }, \u0026#34;StartDeployment\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:1111111111111:function:lambdaModelDeploy\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckStatusDeployment\u0026#34; }, \u0026#34;CheckStatusDeployment\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:1111111111111:function:lambdaModelAwait\u0026#34;, \u0026#34;ResultPath\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckDeploymentBranch\u0026#34; }, \u0026#34;CheckDeploymentBranch\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Or\u0026#34;: [{ \u0026#34;Variable\u0026#34;: \u0026#34;$.status\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Creating\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;WaitStatusDeployment\u0026#34; }, { \u0026#34;Or\u0026#34;: [{ \u0026#34;Variable\u0026#34;: \u0026#34;$.status\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;InService\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;StartPrediction\u0026#34; } ] }, \u0026#34;WaitStatusDeployment\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 60, \u0026#34;Next\u0026#34;: \u0026#34;CheckStatusDeployment\u0026#34; }, \u0026#34;StartPrediction\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:1111111111111:function:lambdaModelPredict\u0026#34;, \u0026#34;End\u0026#34;: true } } } "
},
{
	"uri": "/step/schedule.html",
	"title": "Schedule the State Machine",
	"tags": [],
	"description": "",
	"content": "Well! we have all greens on our state machine, time to put the cron in order to forget about this\nWe are going to use Cloudwacth Event Rules to create a scheduled task in order to execute this once a day.\nFollow this link: https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#rules:action=create\nAnd we fill the form like this:\nEvent Source: Schedule Cron expression: 0 10 * * ? * (every day at 10 AM UTC) Targets: Step Functions state machine State machine: the name of your state machine Create a new role for this specific resource - Configure details Name: MLcron - Create rule And that\u0026rsquo;s it! the rule will be triggered in the specific time every day. More examples of cron expressions here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html#CronExpressions\n"
},
{
	"uri": "/step/cloudformation.html",
	"title": "Cloudformation Template",
	"tags": [],
	"description": "",
	"content": "In case that you want to deploy the stack without going step by step, here is the Cloudformation stack:\nhttps://console.aws.amazon.com/cloudformation/home#/stacks/new?stackName=LabStack\u0026amp;region=us-east-1\u0026amp;templateURL=https://mrtdomshare.s3.amazonaws.com/sagemakerworkshop-step/sagemaker-step-functions.yml\nThe only step that you have to make manually is uploading the data to S3 (the Billing and Reseller folder with the csv files)\nThen you go to step functions, and execute it manually to see the whole process\n"
},
{
	"uri": "/start/create.html",
	"title": "Create the Notebook",
	"tags": [],
	"description": "",
	"content": "Follow this link to create a notebook instance:\nhttps://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances/create\nWe\u0026rsquo;ll give a name to the notebook instance and in \u0026ldquo;Notebook instance type\u0026rdquo; we\u0026rsquo;ll select\nml.m4.4xlarge Create an IAM role with access to:\nAny S3 bucket Athena Full Access. On the Git repositories tab, choose the option \u0026ldquo;Clone a public Git Repository to this notebook instance only\u0026rdquo; an paste the following repo url: https://github.com/githubmg/buildon-workshop\nThe rest of the data, let\u0026rsquo;s leave it by default and click \u0026ldquo;Create notebook instance\u0026rdquo;.\nNow, we wait until the notebook instance is spinned up and click on the \u0026ldquo;Open Jupyter\u0026rdquo; link.\n"
},
{
	"uri": "/personalize/full.html",
	"title": "Full Version",
	"tags": [],
	"description": "",
	"content": "Getting Started Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\nWith Amazon Personalize, you provide an activity stream from your application â€“ clicks, page views, signups, purchases, and so forth â€“ as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize will process and examine the data, identify what is meaningful, select the right algorithms, and train and optimize a personalization model that is customized for your data. All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving personalized recommendations via a simple API call. You pay only for what you use, and there are no minimum fees and no upfront commitments.\nIn this workshop you will build your very own recommendation model that will recommend movies to users based on their past preferences. You will further improve the recommendation model to take into account a user\u0026rsquo;s interactions with movie items to provide accurate recommendations. This workshop will use the publicly available movie lens dataset.\n"
},
{
	"uri": "/prerequisites/jupyter.html",
	"title": "Jupyter Notebooks",
	"tags": [],
	"description": "",
	"content": "Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualization, machine learning, and much more. With respect to code, it can be thought of as a web-based IDE that executes code on the server it is running on instead of locally.\nThere are two main types of \u0026ldquo;cells\u0026rdquo; in a notebook: code cells, and \u0026ldquo;markdown\u0026rdquo; cells with explanatory text. You will be running the code cells. These are distinguished by having \u0026ldquo;In\u0026rdquo; next to them in the left margin next to the cell, and a greyish background. Markdown cells lack \u0026ldquo;In\u0026rdquo; and have a white background. In the screenshot below, the upper cell is a markdown cell, while the lower cell is a code cell:\nTo run a code cell, simply click in it, then either click the Run Cell button in the notebook\u0026rsquo;s toolbar, or use Control+Enter from your computer\u0026rsquo;s keyboard. It may take a few seconds to a few minutes for a code cell to run. You can determine whether a cell is running by examining the In[]: indicator in the left margin next to each cell: a cell will show In [*]: when running, and In [a number]: when complete.\nPlease run each code cell in order, and only once, to avoid repeated operations. For example, running the same training job cell twice might create two training jobs, possibly exceeding your service limits.\n"
},
{
	"uri": "/security_for_users/environment/env_deploy_lab.html",
	"title": "Lab 01: Deploy the environment",
	"tags": [],
	"description": "",
	"content": "In the following steps you will use AWS CloudFormation and AWS Service Catalog to create a self-service mechanism to create secure data science environments. You will first deploy a CloudFormation template which provisions a shared service environment which hosts a PyPI mirror along with a detective control to enforce Amazon SageMaker resources being attached to a VPC. The template will also create a product portfolio in AWS Service Catalog which enables users with appropriate permissions to create a data science environment dedicated to a single project. Once this environment is created you will move on to the next lab which will use AWS Service Catalog to provision a SageMaker notebook.\nUsing one of the buttons below deploy the Secure Data Science quickstart into your AWS account. This will provision a shared services VPC which hosts a PyPI mirror. The CloudFormation stack will also create an AWS Service Catalog to allow project administrators to create data science environments using the self-service mechanisms of the AWS Service Catalog.\nAfter the CloudFormation stack has been deployed visit the Outputs tab of the CloudFormation console. You will notice a hyperlink which will allow you to the assume the role of a Project Administrator. Click this link and then navigate to the Service Catalog console and launch a Data Science Project Environment.\nRegion Launch Template Oregon (us-west-2) Deploy to AWS Oregon\rOhio (us-east-2) Deploy to AWS Ohio\rN. Virginia (us-east-1) Deploy to AWS N. Virginia\rIreland (eu-west-1) Deploy to AWS Ireland\rLondon (eu-west-2) Deploy to AWS London\rSydney (ap-southeast-2) Deploy to AWS Sydney\rStep-by-Step instructions\rVisit the CloudFormation console and select the Outputs tab for the stack you just deployed. Use the provided link to assume the role of Project Administrator. Navigate to the Service Catalog console and launch a Data Science Project Environment. Click the context menu button in the upper-right of the Data Science Project Environment tile and select Launch product. Give the provisioned product a name, such as project-abc-environment and click Next. Use a ProjectName such as project-abc and click Next. Note S3 bucket names need to be globally unique, so don\u0026rsquo;t use project-abc verbatim but replace \u0026ldquo;abc\u0026rdquo; with something unique such as \u0026ldquo;yourname-12345\u0026rdquo; etc. Click Next without entering any Tag Options. Click Next without checking any Notifications. On the Review page click Launch. The product will take approximately 10 minutes to launch.\nAt this point, the cloud platform engineering team has created the underlying infrastructure to host secure data science environments. The project administrators have provisioned a data science environment for your project team and they have provided you with an AWS Service Catalog to enable you to provision SageMaker notebooks. In the next lab you will use these resources to provision a Jupyter notebook and start developing your ML solution.\n"
},
{
	"uri": "/security_for_users/notebook/notebook_lab_01.html",
	"title": "Lab 03: Data Science Workflow",
	"tags": [],
	"description": "",
	"content": "The ML lifecylce has many stages and steps, and often requires revisiting previous steps as you tune your model. This lab is intended to highlight how your project team can work through the ML lifecycle and achive the objectives outlined earlier through supporting services such as experiment tracking.\nThe notebook explained 01_SageMaker-DataScientist-Workflow.ipynb will cover a typical Data Scientist workflow of data exploration, model training, extracting model feature importances and committing your code to Git.\nYou will look at a credit card dataset to predict whether a customer will default on their credit card payments based on prior payment data. This is a binary classification problem and you will train a XGBoost model using an Amazon managed container and explore feature importances for that model.\nThroughout this notebook you will keep network traffic within your private VPC and enforce encryption at rest and in transit. Furthermore, you will learn how to use SageMaker Processing for running processing jobs at scale, and leverage Spot instance pricing to save up to 90% on training costs.\nFinally you will commit the code to our CodeCommit repository and demonstrate code versioning, tagging and source control.\nAfter you have completed the notebook please proceed to Lab 04: DevOps Workflow where you will deploy and monitor the trained model.\n"
},
{
	"uri": "/security_for_sysops/best_practice/service_catalog.html",
	"title": "Self-Service with Guard Rails",
	"tags": [],
	"description": "",
	"content": "Allowing users in the cloud to self-service and provision cloud resources on-demand is a powerful enabler for project teams but can be a concern from an operational risk perspective. However, if you can empower your developers to self-service, while enforcing guard rails and best practice, then the operational and security teams will also benefit. With enforced guard rails and best practice you can be confident that, while developers are creating resources they need, they are doing so in a manner that is inline with your policies and requirements.\nAWS Service Catalog AWS Service Catalog allows you to create and manage collections of logical IT products and services that you have configured and parameterized as templates. These IT template products and services can include everything from virtual machines, software deployment, and database creation to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage commonly deployed IT services, and helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.\nAWS Service Catalog works by managing CloudFormation templates you provide, allowing users in your AWS environment to deploy the templates. You control the templates themselves along with who can deploy or manage the templates and the resulting deployments. With AWS Service Catalog you can control which IT services and versions are available, the configuration of the available services, and permission access by individual, group, department, or cost center.\nTo get started you begin by creating a portfolio which represents a collection of products and configuration information. It allows you to specify who can access the products you define and how they can use them.\nOnce you\u0026rsquo;ve created a portfolio you then begin to define products which describe a product or service that your users can deploy onto AWS. A product will consist of one or more AWS resources like an Amazon EC2 instance, databases, or Jupyter notebooks. These products are described through a CloudFormation template and can be tailored by the user at deployment time based upon the values of parameters you define for the user to configure.\nWith resources deployed on the user\u0026rsquo;s behalf you will then want to ensure that users can access those resources in an appropriate manner without being able to undo the best practice configurations deployed by Service Catalog. To do this you can implement preventive controls in the form of identity and access management policies.\nIdentity \u0026amp; Accesss Management AWS services are interacted with via a RESTful API. Every call into this API is authorized by the AWS Identity \u0026amp; Access Managment (IAM) service. You control AWS IAM and grant users in your environment explicit permissions to use various AWS services. You grant explicit permissions through IAM policy documents which specify the principal (who), the actions (API calls), and the resources (such as S3 objects) that are allowed, as well as under what conditions such access is granted.\n{ \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;arn:aws:s3:::my_corporate_bucket/home/${aws:userid}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;10.1.100.0/24\u0026#34; } } } ] } The example policy document above grants a user permissions to read or write to an S3 bucket, but only to a sub-folder that matches the user\u0026rsquo;s user ID, and only from an IP address on the 10.1.100.0 network. We can see that the Effect explicitly allows the Actions on a single Resource.\nAmazon SageMaker has a comprehensive set of conditions and actions which you can grant to users in your environment. You can grant users the ability to start, stop, or access notebook servers, create training jobs, or host models in production for example. You can also use an array of conditions such as:\nsagemaker:VolumeKmsKey, to require that an encryption key is specified sagemaker:VpcSecurityGroupIds, to require that a VPC configuration is provided sagemaker:DirectInternetAccess, to ensure that notebooks do not have Internet access sagemaker:NetworkIsolation, to ensure that models or training jobs cannot communicate with the network In this lab you will create an IAM role for the project administrators. You will also create a Service Catalog portfolio and product, granting project administrators the ability to deploy data science environments using this templated product in Service Catalog. You will also create a detective control (covered later) to detect any Amazon SageMaker resources that are run in a non-compliant manner. Lastly you will create a PyPI mirror to provide private access to approved Python packages from the data science environments.\n"
},
{
	"uri": "/start/clone.html",
	"title": "Clone the Service Repos",
	"tags": [],
	"description": "",
	"content": "Now, let\u0026rsquo;s open a Terminal in our notebook by going to \u0026ldquo;New/Terminal\u0026rdquo; and perform the following commands\ncd ~/SageMaker git clone https://github.com/githubmg/buildon-workshop If we reload the Notebook, we\u0026rsquo;ll see a new folder \u0026ldquo;buildon-workshop\u0026rdquo; and inside another one called \u0026ldquo;pure-sagemaker-workshop\u0026rdquo;\n"
},
{
	"uri": "/security_for_sysops/best_practice/best_practice_lab.html",
	"title": "Lab 1: Best Practice as Code",
	"tags": [],
	"description": "",
	"content": "Before you can begin creating templates for deployment by the Project Administration team you will need a shared services VPC to host a Python package mirror (PyPI) for use by data science teams. The mirror will host a collection of approved Python packages. The concept of a shared services VPC or PyPI mirror is not something that is detailed in this workshop, and is partially assumed as common practice among many AWS customers. After you have created a shared services VPC and PyPI mirror you will then, as the Cloud Platform Engineering Team, create a Service Catalog Portfolio which the project administrators can use to easily deploy data science environments in support of new projects.\nThis lab assumes other recommended security practices such as enabling AWS CloudTrail and capturing VPC Flow Logs. The contents of this lab focus soley on controls and guard rails directly related to data science resources.\nShared Services architecture In this section you will quickly get started by deploying a shared PyPI mirror for use by data science project teams. In addition to deploying a shared service this template will also create an IAM role for use by the AWS Service Catalog and for use by project administrators who are responsible for creating data science project environments.\nThe shared PyPI mirror will be hosted in a shared services VPC and exposed to project environments using a PrivateLink-powered endpoint. The mirror will host approved Python packages and can be used by all internal Python applications, such as machine learning code running on SageMaker.\nThe resulting architecture will look like this:\nDeploy your shared service As a cloud platform engineering team member, deploy the CloudFormation template linked below to provision a shared service VPC and IAM roles.\nRegion Launch Template Oregon (us-west-2) Deploy to AWS Oregon\rOhio (us-east-2) Deploy to AWS Ohio\rN. Virginia (us-east-1) Deploy to AWS N. Virginia\rIreland (eu-west-1) Deploy to AWS Ireland\rLondon (eu-west-2) Deploy to AWS London\rSydney (ap-southeast-2) Deploy to AWS Sydney\rDeployment should take around 5 minutes.\nStep-by-Step instructions\rClick the button above which is appropriate for the AWS region in which you want to build. This will take you to the CloudFormation console to create a CloudFormation stack using the reference CloudFormation template. Check the Stack name and the name you want to use for your shared service resources. You can accept the default values if you wish. Under Capabilities click the 2 check boxes indicating you understand that the CloudFormation template will create IAM resources. Click Create stack Create Project Portfolio With the shared services VPC online and available you now need to provide the project administration team with a configured Service Catalog to provision data science project environments. To start, visit the AWS Service Catalog console and create a Portfolio. Grant the DataScienceAdmin role permissions to access the portfolio adn then use the appropriate CloudFormation template linked below to create a Data Science Environment product. Ensure that the product has a constraint applied to it that uses the IAM role ServiceCatalogLaunchRole to launch the product upon request. This will give the Service Catalog service the permissions needed to create a Data Science Environment.\nService Catalog Product Templates by Region:\nRegion ap-southeast-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-ap-southeast-2/quickstart/ds_environment.yaml Region eu-west-1, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-1/quickstart/ds_environment.yaml Region eu-west-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-2/quickstart/ds_environment.yaml Region us-east-1, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-east-1/quickstart/ds_environment.yaml Region us-east-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-east-2/quickstart/ds_environment.yaml Region us-west-2, https://s3.amazonaws.com/sagemaker-workshop-cloudformation-us-west-2/quickstart/ds_environment.yaml Step-by-Step instructions\rAccess the AWS Service Catalog console Click Portfolios on the left Click Create portfolio Enter a Portfolio name of Data Science Project Portfolio Enter a Owner of Cloud Operations Team Click the link for your new portfolio to view the portfolio\u0026rsquo;s details Click the Groups, roles, and users tab Click Add groups, roles, users Click Roles and type DataScienceAdmin into the search field Tick the box next to your DataScienceAdministrator role and click Add access Click Products Click Upload new product Enter a Product name of Data Science Environment For Owner enter Cloud Operations Team Click Use a CloudFormation template For the CloudFormation template URL enter the appropriate URL from the list below: For Version title enter v1 Click Review and Create product Click the radio button next to the new product and from the Actions drop down select Add product to portfolio Click the radio button for your product portfolio and click Add Product to Portfolio Return to the list of Portfolios by clicking Portfolios on the left Click the link for the data science portfolio Click the Constraints tab in the portfolio detail page Click Create constraint From the Product drop down select your product Select Launch for the Constraint type Under Launch Constraint click Select IAM role From the IAM role drop down select ServiceCatalogLaunchRole Click Create Review team resources In addition to the Service Catalog Portfolio and product you have also created the following AWS resources to support the project administration team. Please take a moment and review these resources and their configuration.\nIAM roles\nAWS IAM roles\nThe IAM roles for the project administration team and the Service Catalog have been created. Visit the AWS IAM console and review the permissions granted to these two roles.\nAWS Lambda Detective Control\nAn AWS Lambda has been deployed and configured to execute whenever an Amazon SageMaker resource is deployed. The Lambda function will act as a detective control, inspecting launched resources to ensure that the resource is configured correctly. To inspect the Lambda function and its triggers visit the AWS Lambda console. Can you determine exactly what types of events will cause the Lambda function to execute?\nParameters added to Parameter Store\nA collection of parameters have been added to Parameter Store. Can you see what parameters have been added? How would you use these values?\nShared Services VPC\nThe template has created a VPC that will house our shared applications. Visit the console and see what services are accessible from within the VPC?\nPyPI Mirror Service\nA Python package mirror has been deployed as a containerised service in the Shared Services VPC. This service is running on a cluster managed by Amazon Elastic Container Service (ECS) Fargate which means there are no Amazon EC2 servers for you to manage. Visit the ECS console to check whether the service is up and running. You can also see the task logs from the container through the ECS console to check its status.\nWith these resources created you can now move on to Lab 2 where you will, as a project administrator, deploy a secure data science environment for a new project team.\n"
},
{
	"uri": "/builtin.html",
	"title": "Built-in Algorithms",
	"tags": [],
	"description": "",
	"content": "The focus of this module is on SageMaker\u0026rsquo;s built-in algorithms. These algorithms are ready-to-use, scalable, and provide many other conveniences. The module shows how to use SageMaker\u0026rsquo;s built-in algorithms via hosted Jupyter notebooks, the AWS CLI, and the SageMaker console. You\u0026rsquo;ll also see different strategies to distribute your data when training your models across a cluster of machines. To proceed to this module you need to have completed the Cloud9 Setup and Creating a Notebook Instance sections in the previous module.\n"
},
{
	"uri": "/prerequisites/cloud9.html",
	"title": "Cloud9 Setup",
	"tags": [],
	"description": "",
	"content": "AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes pre-packaged with essential tools for popular programming languages and the AWS Command Line Interface (CLI) pre-installed so you donâ€™t need to install files or configure your laptop for this workshop. Your Cloud9 environment will have access to the same AWS resources as the user with which you logged into the AWS Management Console.\nSetup the Cloud9 Development Environment Go to the AWS Management Console, click Services then select Cloud9 under Developer Tools.\nClick Create environment.\nEnter workshop into Name and optionally provide a Description.\nClick Next step.\nYou may leave Environment settings at their defaults of launching a new t2.micro EC2 instance which will be paused after 30 minutes of inactivity.\nClick Next step.\nReview the environment settings and click Create environment. It will take several minutes for your environment to be provisioned and prepared.\nOnce ready, your IDE will open to a welcome screen. The central panel of the IDE has two parts: a text/code editor in the upper half, and a terminal window in the lower half. Below the welcome screen in the editor, you should see a terminal prompt similar to the following (you may need to scroll down below the welcome screen to see it):\nYou can run AWS CLI commands in here just like you would on your local computer. Verify that your user is logged in by running aws sts get-caller-identity as follows at the terminal prompt: aws sts get-caller-identity Youâ€™ll see output indicating your account and user information: Admin:~/environment $ aws sts get-caller-identity { \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;AKIAI44QH8DHBEXAMPLE\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/Alice\u0026#34; } To create a new text/code file, just click the + symbol in the tabs section of the editor part of the IDE. You can do that now, and close the wecome screen by clicking the x symbol in the welcome screen tab.\nKeep your AWS Cloud9 IDE opened in a browser tab throughout this workshop as weâ€™ll use it for activities like using the AWS CLI and running Bash scripts.\nTips Keep an open scratch pad in Cloud9 or a text editor on your local computer for notes. When the step-by-step directions tell you to note something such as an ID or Amazon Resource Name (ARN), copy and paste that into the scratch pad.\n"
},
{
	"uri": "/security_for_users/notebook/notebook_lab_02.html",
	"title": "Lab 04: DevOps Workflow",
	"tags": [],
	"description": "",
	"content": "In notebook 02_SageMaker-DevOps-Workflow.ipynb, you will complete the machine learning lifecycle and deliver a model into production. As a DevOps Engineer you will pick up the model trained by the data scientists and deploy it as an endpoint. You will also set up model monitoring on the endpoint for detecting data drift. This notebook is primarily focused on two aspects of running machine learning models in production, monitoring the model\u0026rsquo;s performance and being able to demonstrate the model\u0026rsquo;s lineage.\nModel Monitoring When a model is operating in production issues can arise that can be difficult to detect. For instance invalid data inputs can be sent to your model for inference, if those invalid data inputs have an invalid data type that can be easily detected, but if the value is a numeric value that is greater than any value seen previously in training, how would you detect that? Another issue could be what\u0026rsquo;s called concept drift, meaning that the patterns the model was trained to detect are no longer relevant for the world in which the model is operating. For instance, if a model was trained to detect fraud but the patterns of fraud have shifted in response to increased detection, the model would no longer be as accurate as it once was. Amazon SageMaker offers a monitoring capability to detect erroneous inputs and concept drift. In this notebook you will use Model Monitor to capture the inputs and outputs of your model and to provide reports on its observations of your model.\nAmazon SageMaker Model Monitor can detect the following violations:\nInvalid input or output data types Detection of uncharacteristic levels of null values Data distribution of input or output data is outside of baseline data distribution Missing or extra input values Unknown categorical values For more information about the types of violations and their meanings please see the Model Monitor documentation.\nNote that the Model Monitor is scheduled to execute periodically. As a result you may have to give the monitor up to an hour to execute itself as you work through the notebook. Below you will find example screen captures of monitor output, similar to what your monitoring job will show after time.\nThese graphics are generated using SageMaker Studio UI.\nThe image below shows a sample Pandas dataframe showing some of the violations which Model Monitor can detect. Note, these may not be the exact final violations you see. For example, below we have also modified the Marriage column.\nThis same dataframe is shown below as a graph. You can see that there is a large drift in the MARRIAGE column, but over time the service also detects some drift in the Label distribution. SageMaker Studio can also directly plot the actual drift as shown in the BILL_AMT figure instead of comparing the baseline to the observed values.\nBy setting thresholds for acceptable drift, you can decide when to retrain your models.\nModel Lineage As a best practice, you will commit the notebook to a git repo that is already provisioned, and call SageMaker Experiments APIs to track any model metadata, container artifacts, data and code lineage from the raw code to the production endpoint.\nAs you near the end of the notebook you will reach Part 8: Lineage. Here you will generate a dataframe that contains the lineage of your trained model, tagging the experiment with relevant Git commit ids and details.\nCongratulations, you have now taken raw data and an algorithm to produce a trained model for your business challenge. You have deployed this model and monitored it for drift, with the ability to demonstrate the lineage of the model in terms of when it was trained, which version of the code was used, etc.\n"
},
{
	"uri": "/security_for_sysops/secure_notebook/secure_notebook.html",
	"title": "Secure Notebooks",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker is designed to empower data scientists and developers, enabling them to build more quickly and remain focused on their machine learning project. One of the ways that SageMaker does this is by providing hosted Jupyter Notebook servers. With a single API call users can create a Jupyter Notebook server with the latest patches and kernels available. No need to install software, patch or maintain systems - it is all done for you. What the API creates for you is an EC2 instance running Jupyter Notebook server software with Python, R, Docker, and the most popular ML frameworks pre-installed. But it is not just about convenience and enablement, SageMaker is also focused on hosting these notebooks for you in a secure manner.\nSecure by default An Amazon SageMaker notebook is an EC2 instance with the open source Jupyter server installed. The SageMaker service manages the EC2 instance in order to help you maintain a level of security with little or no overhead. To do this Amazon SageMaker takes responsibility for:\nPatching and updating the system Encrypting data on the EBS volumes Limiting user access to the system Enabling you to further tailor and harden the system Patching and updating A SageMaker Jupyter notebook server will have 2 EBS volumes attached to it. The first is the system drive and is ephemeral. This hosts the operating system, Anaconda, and Jupyter server software. The second hosts your data and anything you put into /home/ec2-user/SageMaker. Over time your EC2 instance can become out of date, going unpatched. To patch your Jupyter Notebook to the latest versions simply stop the notebook and start it again. This will refresh the system drive without any maintenance required on your part. However please note that if you make any changes to files on the system drive you will need to make those changes again as they will be destroyed in the stopping and starting of the notebook server.\nTo keep a SageMaker notebook up to date and to save costs it is recommended to stop a Jupyter notebook server when it is not needed and to restart it when you need to use it.\nEncryption at rest As mentioned the EC2 instance has two EBS volumes mapped to it. Both of these volumes are encrypted using a SageMaker service-managed KMS key, although you can specify a CMK to be used to encrypt the storage volume. By doing this you can easily encrypt all of the data stored on the Jupyter Notebook server by default.\nLimiting user access The very nature of software development means that users can obtain some OS-level access to the Jupyter notebook server. By default the Jupyter Web UI will allow you to open a shell terminal. When a user access the shell, they will be logged into the EC2 instance as ec2-user. If you\u0026rsquo;re familiar with Amazon Linux this is the default username that you use to gain access to an AWS EC2 instance. This user also typically has permissions to sudo to the root user. This can be limited in the Jupyter Notebook configuration which will remove the user\u0026rsquo;s permission to assume the root user. They will still have permissions to install things like Python modules into their user environment, but they will not be able to modify the wider system of the notebook server.\nFurther tailoring In addition to the measures described above you also have the ability to specify Lifecycle Configurations. These are shell scripts which execute on system bootstrap, before the notebook is made available to users. These configuration scripts can execute on system creation, system startup, or both. Using these scripts you can ensure that monitoring agents are installed or other types of hardening are performed to ensure that the system is in a specific state before allowing users to access the system. Here we will use the lifecycle scripts to download some open source libraries from the pip mirror we created, create an sagemaker_environment.py file to keep track of variables such as the network configuration, KMS keys that can be imported directly, without giving the datascientist access to them.\nManaged and Governed Amazon SageMaker provides managed EC2-based services such as Jupyter notebooks, training jobs, and hosted ML models. SageMaker runs the infrastructure for these components using EC2 resources dedicated to yourself. These EC2 resources can be mapped to your VPC environment allowing you to apply your network level controls, such as security groups, to the notebooks, training jobs, and hosted ML models.\nAmazon SageMaker does this by creating an ENI in your specified VPC and attaching it to the EC2 infrastructure in the service account. Using this pattern the service gives you control over the network-level access of the services you run on Amazon SageMaker.\nFor Jupyter Notebooks this will mean that all Internet connectivity, Amazon S3 connectivity, or access to other systems in your VPC is governed by you and by your network configuration.\nFor training jobs and hosted models these are again governed by you. When retrieving training data from Amazon S3 the SageMaker EC2 instances will communicate with S3 through your VPC. Equally when the SageMaker EC2 instances retrieve your trained model for hosting, they will communicate with S3 through your VPC. You maintain governance and control of the network communication of your SageMaker resources.\nAccess Control Access to a SageMaker Jupyter notebook instance is goverend by AWS IAM. In order to open a Jupyter Notebook instance users will need access to the CreatePresignedNotebookInstanceUrl API call. This API call creates a presigned URL which can be followed to obtain Web UI access to the Jupyter notebook server. To secure this interface you use IAM policy statements to wrap conditions around calling the API, for example who can invoke the API and from what IP address.\n{ \u0026#34;Id\u0026#34;:\u0026#34;notebook-example-1\u0026#34;, \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;Enable Notebook Access\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;:[ \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34; ], \u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;:{ \u0026#34;ForAnyValue:StringEquals\u0026#34;:{ \u0026#34;aws:sourceVpce\u0026#34;:[ \u0026#34;vpce-111bbccc\u0026#34;, \u0026#34;vpce-111bbddd\u0026#34; ] } } } ] } The IAM policy above states that someone can only communicate with a Notebook if they do so from within a VPC and through specific VPC endpoints. Using mechanisms like the above you can explicitly control who can interact with a Notebook server.\nVersion Control Finally, to support collaboration, SageMaker notebooks can be integrated with Git-based repositories. Git is a distributed version control system which enables project teams to manage source code, notebook files, and other artifacts related to a project. In the next lab the notebook you create will be configured to use a CodeCommit repository as a way of managing the sample project provided. In this lab you will push and tag the code to the master branch of the code repository using the CLI or Jupyter UI.\nIn this lab, as a data scientist, you will use a product in a portfolio defined by the Project Administrators and create a Jupyter notebook for yourself following best practice.\n"
},
{
	"uri": "/security_for_users/environment/secure_notebook_lab.html",
	"title": "Lab 02: Deploy a Notebook",
	"tags": [],
	"description": "",
	"content": "Your project team has been presented with IAM roles and a Service Catalog Portfolio to allow your team to self service and obtain resources to support your efforts. Following the steps below use this portfolio to create a Jupyter notebook instance for yourself.\nLaunch the notebook product Click into the details for the data science environment product you provisioned in the last step and find the link to assume the role of a data science user. After assuming this role return to the Service Catalog console and launch an Amazon SageMaker Jupyter notebook.\nStep-by-Step instructions\rFrom the Service Catalog console open the details for the data science project environment you provisoined as a Project Administrator. Locate the link to assume the role of a Data Science User and assume the role. Now, as a Data Science User, return to the Service Catalog console and launch a SageMaker Notebook product. Click the context menu button in the upper right corner of the SageMaker Notebook product and click Launch Product. Give the product a name such as my-sagemaker-nb and click Next. Provide a NotebookOwnerEmail to be associated with any git commits. Enter the ProjectName that was used in the previous lab, such as project-abc. Enter a NotebookOwnerUsername to be associated with any git commits and click Next. Enter no Tag Options and click Next. Do not configure Notifications and click Next. On the Review page click Launch. You will land at a Provisioned Product page while the Service Catalog creates your Jupyter Notebook. Periodically click Refresh until the Status reads Succeeded. This should take about 5 minutes to launch your notebook.\nIAM Permissions Every SageMaker notebook has permissions granted to it to be able to access, create, and delete AWS resources and APIs. These permissions are granted through an IAM role associated with the Jupyter notebook\u0026rsquo;s EC2 instance. An example of the permissions associated with the notebook are highlighted in the next set of labs.\nThe IAM role assigned to your notebook has been created just for your notebook and represents an association of the Jupyter notebook to both yourself and your project. This will be represented in audit logs, identifying that actions were taken by your Jupyter notebook and allow for easy tracking of which notebook, for which project, belonging to which project team member performed an action on AWS resources.\nAccess the notebook After the notebook product has finished provisioning you can open it by clicking the NotebookUrl link provided as an Output in the provisioned product detail. With your Jupyter notebook open, familiarize yourself with the web interface and open the Notebook kernel named 01_SageMaker-DataScientist-Workflow.ipynb. Don\u0026rsquo;t forget to reference back to the Jupyter cheat sheet for a quick reference if you need one.\nStep-by-step instructions\rFrom the Service Catalog console open the link for the notebook product you provisioned. Under Outputs, click the link for NotebookUrl to launch the Jupyter notebook interface. With the notebook open, launch the Notebook kernel listed on the left hand side named 01_SageMaker-DataScientist-Workflow.ipynb. When its open the Notebook kernel should use the conda_python3 kernel. If Jupyter asks you to set the kernel select conda_python3 and if Jupyter displays an error, reload the Jupyter page by clicking your browser Refresh button.\nYou have now created a Jupyter notebook dedicated to yourself as a member of the project team. With the notebook open you are ready to take on the next set of labs where you will use the notebook and the data science environment to engineer a feature set, train a model, deploy the model, and then monitor the model for performance or drift.\n"
},
{
	"uri": "/security_for_sysops/secure_notebook/secure_notebook_lab.html",
	"title": "Lab 3: Deploy a Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "At this point, the cloud platform engineering team has built a self-service mechanism to provision secure environments to host data science projects. The project administrators have provisioned resources using the self-service mechanism for your team to work, and they have provided you with a self-service mechanism to enable you to provision SageMaker notebooks. Now, use these resources to provision a Jupyter notebook and start developing your ML solution.\nLaunch the notebook product Navigate to the AWS Service Catalog console and on go to the detail page for the recently provisioned data science environment. Use the hyperlink labeled AssumeProjectUserRole under Outputs to assume the role of a data science user. Assume the role and visit the Service Catalog product listing. Using the notebook product defined for you by the project administrators, launch a Notebook product using the same project name that was used to create the environment.\nStep-by-step instructions\rOpen the Service Catalog provisioned products list. Click on the provisioned data science project and locate the hyperlinks in the Output section of the provisioned product detail. Click on the AssumeProjectUserRole hyperlink. Click Assume Role on the next screen. Return to the Service Catalog product listing. Open the menu for the SageMaker notebook product and click Launch Product Give the notebook product a name such as YOUR-NAME-ds-notebook and click Next Use the same project name as earlier in the labs Enter an email address for NotebookOwnerEmail and a username for NotebookOwnerUsername Click Next and on the next 2 screens click Next On the Review screen click Launch You will land at a Provisioned Product page while the Service Catalog creates your Jupyter Notebook. Periodically click Refresh until the Status reads Succeeded. This should take about 5 minutes to launch your notebook.\nAccess the notebook After the notebook has launched successfully you can open it by clicking the NotebookUrl hyperlink in the Outputs section of the provisoined notebook details page. With your Jupyter notebook open, familiarize yourself with the web interface and open the Notebook kernel named 00_SageMaker-SysOps-Workflow. Don\u0026rsquo;t forget to reference back to the Jupyter cheat sheet for a quick reference if you need one.\nWhen its open the Notebook kernel should use the conda_python3 kernel. If Jupyter asks you to set the kernel select conda_python3 and if Jupyter displays an error, reload the Jupyter page by clicking your browser Refresh button.\nYou will now learn about methods for implementing detective and corrective controls on AWS. In the next lab you will return to the Jupyter notebook to test the detective controls.\n"
},
{
	"uri": "/introduction/notebook.html",
	"title": "Create a Notebook Instance",
	"tags": [],
	"description": "",
	"content": "SageMaker provides hosted Jupyter notebooks that require no setup, so you can begin processing your training data sets immediately. With a few clicks in the SageMaker console, you can create a fully managed notebook instance, pre-loaded with useful libraries for machine learning. You need only add your data.\nYou\u0026rsquo;ll start by creating an Amazon S3 bucket that will be used throughout the workshop. You\u0026rsquo;ll then create a SageMaker notebook instance, which you will use for the other workshop modules.\nCreate a S3 Bucket SageMaker typically uses S3 as storage for data and model artifacts. In this step you\u0026rsquo;ll create a S3 bucket for this purpose. To begin, sign into the AWS Management Console, https://console.aws.amazon.com/.\nKeep in mind that your bucket\u0026rsquo;s name must be globally unique across all regions and customers. We recommend using a name like smworkshop-firstname-lastname. If you get an error that your bucket name already exists, try adding additional numbers or characters until you find an unused name.\nIn the AWS Management Console, choose Services then select S3 under Storage.\nChoose Create Bucket\nProvide a globally unique name for your bucket such as \u0026lsquo;smworkshop-firstname-lastname\u0026rsquo;.\nSelect the Region you\u0026rsquo;ve chosen to use for this workshop from the dropdown.\nChoose Create in the lower left of the dialog without selecting a bucket to copy settings from.\nLaunch the Notebook Instance In the upper-right corner of the AWS Management Console, confirm you are in the desired AWS region. Select N. Virginia, Oregon, Ohio, or Ireland.\nClick on Amazon SageMaker from the list of all services. This will bring you to the Amazon SageMaker console homepage.\nTo create a new notebook instance, go to Notebook instances, and click the Create notebook instance button at the top of the browser window.\nType smworkshop-[First Name]-[Last Name] into the Notebook instance name text box, and select ml.m4.xlarge for the Notebook instance type.\nFor IAM role, choose Create a new role, and in the resulting pop-up modal, select Specific S3 buckets under S3 Buckets you specify â€“ optional. In the text field, paste the name of the S3 bucket you created above, AND the following bucket name separated from the first by a comma: gdelt-open-data. The combined field entry should look similar to smworkshop-john-smith, gdelt-open-data. Click Create role.\nYou will be taken back to the Create Notebook instance page. Click Create notebook instance.\nAccess the Notebook Instance Wait for the server status to change to InService. This will take several minutes, possibly up to ten but likely less.\nClick Open. You will now see the Jupyter homepage for your notebook instance.\n"
},
{
	"uri": "/custom.html",
	"title": "Custom Algorithms",
	"tags": [],
	"description": "",
	"content": "This module focuses on using your own training algorithm and your own inference code. You can use the deep learning containers provided by Amazon SageMaker for model training and your own inference code. You provide a script written for the deepV learning framework, such as Apache MXNet or TensorFlow. To proceed to this module you need to have completed the Cloud9 Setup and Creating a Notebook Instance sections in the previous module.\n"
},
{
	"uri": "/security_for_sysops/detective/detective_controls.html",
	"title": "Detective Controls",
	"tags": [],
	"description": "",
	"content": "AWS recommends a defense-in-depth approach to security, applying security at every level of your application and environment. In this section we will focus on the concept of detective controls and incident response in the form of a corrective control. A detective control is responsible for identifying potential security threats or incidents while a corrective control is responsible for limiting the potential damage of the threat or incident after detection.\nOne example of a form of detective control is the use of internal auditing to ensure that an environment and user practice is inline with your policies and requirements. These controls can help your organization identify and understand the scope of anomalous activity.\nMany AWS services are available to help you implement detective controls. AWS CloudTrail records AWS API calls, AWS Config provides a detailed inventory of your AWS resources and configuration. Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior. Amazon CloudWatch is a monitoring service for AWS resources which can trigger CloudWatch Events to automate security responses.\nEven with detective controls, you should still put processes in place to respond to and mitigate the potential impact of security incidents.\nAmazon CloudWatch Events allows you to create rules that trigger automated responses such as the execution of AWS Lambda.\nAmazon CloudWatch Events Amazon CloudWatch recieves metrics, logs, and events and can, in near-real time, respond to this information on your behalf. You can use log entries from applications or AWS services to automatically notify a member of your team when an error has occurred. You can use metrics to scale up or scale out your infrastructure when a CPU is exceeding a utilization threshold. Or you can inspect a launching EC2 instance that is the result of an API call into AWS.\nLogs, metrics, and events can trigger a number of responses such as emailing alerts or executing a Lambda function in response.\nA list of AWS services and the events that they send to CloudWatch can be found in the documentation. Within Amazon CloudWatch you can configure an events rule which can detect anomalous activity and trigger a response.\nThe response that is triggered can be any of a number of activities to include executing an AWS Lambda function, an Amazon ECS task, or an SSM RunCommand. This gives you flexibility to automate responses to incidents in your environment.\nThe detective and corrective controls made possible by CloudWatch Events and AWS Lambda allow you to inspect SageMaker training jobs, EC2 instance security groups, and much more. By detecting resources that are not in line with your standards you can quickly respond to and correct the resource using automated incident response.\n"
},
{
	"uri": "/security_for_sysops/detective/detective_lab.html",
	"title": "Lab 4: Detective and Corrective Controls",
	"tags": [],
	"description": "",
	"content": "In this lab you will test a remediating detective control that was deployed by the cloud platform engineering team in Lab 1. The control is designed to detect the creation of Amazon SageMaker training jobs outside of the secure data science VPC and terminate them. To do this, you will go through the Jupyter notebook kernel 00_SageMaker-SysOps-Workflow and execute the cells up to and including the creation of a training job. You\u0026rsquo;ll notice, as the training job is provisioned and begins to execute, that it is terminated by the corrective aspect of the control in the environment.\nStart a training job As the data scientist, read through and execute the cells in the Jupyter Notebook kernel named 00_SageMaker-SysOps-Workflow.ipynb. These various cells will:\nset variables to reference the data and model Amazon S3 buckets that the administrators created for your team copy training data to your local Jupyter notebook preprocess the data using SageMaker Processing push processed training data to your data S3 bucket configure a training job to be executed by you Use SageMaker Experiments to track metadata of your training jobs When you reach the cell titled Train without a VPC configured, execute the cell and take note of the output. After a few minutes you should notice that the training job was terminated. The output should resemble the below which indicates that the training job did not complete its bootstrap.\nStep-by-step instructions\rBegin at the JupyterLabs interface of your notebook instance and execute the first cells of the notebook kernel. Continue executing the cells through Sections A and B, until you reach Section C, Part 5. When you reach the cells titled Train without a VPC configured, pause - the following cells should fail after a period when you execute them. Watch the output of the training as it executes, and notice the job does not complete its bootstrap. 2019-10-11 13:50:37 Starting - Starting the training job... 2019-10-11 13:51:01 Starting - Launching requested ML instances... 2019-10-11 13:51:35 Starting - Preparing the instances for training...... 2019-10-11 13:52:29 Downloading - Downloading input data 2019-10-11 13:52:29 Stopping - Stopping the training job 2019-10-11 13:52:29 Stopped - Training job stopped ..Training seconds: 1 Billable seconds: 1 training completed. Detective control explained The training job was terminated by an AWS Lambda function that was executed in response to a CloudWatch Event that was triggered when the training job was created. The Lambda function inspected the training job, saw that it was NOT attached to a VPC and stopped the training job from executing.\nAssume the role of the Data Science Administrator and review the code of the AWS Lambda function SagemakerTrainingJobVPCEnforcer. Also review the CloudWatch Event rule SagemakerTrainingJobVPCEnforcementRule and take note of the event which triggers execution of the Lambda function.\nStart a compliant training job To succesfully run your training job you will need to configure the training job to run within your VPC. To do this you will pass a collection of subnet IDs and security groups that we imported earlier.\nThe following sample code shows how these can be specified:\nTensorFlow(entry_point=\u0026#39;predictor.py\u0026#39;, ..., train_instance_count=1, train_instance_type=instance_type, subnets = [\u0026#39;subnet-0fc1ed6b334bd4cfd\u0026#39;,\u0026#39;subnet-0f398485e991f8333\u0026#39;], security_group_ids = [\u0026#39;sg-0da87d40633b8f922\u0026#39;], ... ) Execute the cell below the failed training job deployment titled Traing with a VPC, the training job should complete successfully, producing output similar to the following:\n2019-10-16 19:57:54 Starting - Starting the training job... 2019-10-16 19:57:56 Starting - Launching requested ML instances...... 2019-10-16 19:58:59 Starting - Preparing the instances for training... 2019-10-16 19:59:46 Downloading - Downloading input data... 2019-10-16 20:00:25 Training - Training image download completed. Training in progress.. 2019-10-16 20:00:25,711 INFO - root - running container entrypoint 2019-10-16 20:00:25,711 INFO - root - starting train task 2019-10-16 20:00:25,727 INFO - container_support.training - Training starting In this lab, you experienced a remediating detective control deployed by the cloud platform engineering team and reconfigured the SageMaker training job to run connected to your VPC. But waiting minutes to find out that your training job is going to error out is a slow and painful way to iterate during development.\nIn the next lab you will look into what preventive controls can be put in place to enhance your defense in depth and provide a better developer experience for the project team members.\n"
},
{
	"uri": "/security_for_sysops.html",
	"title": "Build Secure Environments",
	"tags": [],
	"description": "",
	"content": "In this module we will introduce you to the recommended practices for building a secure data science environment powered by Amazon SageMaker. Like many other AWS services, Amazon SageMaker is secure by default. In these labs you will learn how to combine multiple secure by default services to enforce secure configurations and create a data science environment that meets common security requirements for many customers. You will cover various security topics and work through hands-on lab materials to exercise and explore the many security features available with Amazon Web Services.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "/security_for_sysops/best_practice.html",
	"title": "Codify Best Practice",
	"tags": [],
	"description": "",
	"content": "This lab introduces services to help you create a way for the project administration teams and the data science teams to self-service while adhering to desired, secure practices. In this lab you will learn about AWS Identity and Access Management as well as AWS Service Catalog. You will then use what you learned to enable your teams.\nLet\u0026rsquo;s begin.\n"
},
{
	"uri": "/security_for_sysops/team_resources.html",
	"title": "Project Environment",
	"tags": [],
	"description": "",
	"content": "This lab will explore the tools available to you to create a secure environment, starting with the network. By establishing a secure network environment you can begin to use familiar IP-level controls to manage the flow of data and access to systems. You will learn some concepts and terms specific to AWS but should get a sense of how to create a private environment for your data science teams. You will also create resources to encrypt data as it flows from storage in Amazon S3, into the VPC on EBS volumes, and back out to Amazon S3.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "/security_for_sysops/secure_notebook.html",
	"title": "Working Securely",
	"tags": [],
	"description": "",
	"content": "This lab introduces some of the key security features of the Amazon SageMaker service and how you can use them to keep your data scientists operating in a secure manner. After introducing the concepts and features of the service this lab will then allow you to create a SageMaker Jupyter Notebook in a self-service manner.\nLet\u0026rsquo;s jump in.\n"
},
{
	"uri": "/security_for_sysops/detective.html",
	"title": "Detection and Remediation",
	"tags": [],
	"description": "",
	"content": "This lab will briefly introduce the concept of detective and corrective controls. It will also introduce some of the AWS services which can be used to create detective and corrective controls. During the lab you will see these controls in action as they safeguard you and your environment, keeping you within your security policies.\nLet\u0026rsquo;s get to it.\n"
},
{
	"uri": "/security_for_sysops/preventive.html",
	"title": "Prevention",
	"tags": [],
	"description": "",
	"content": "This lab will dive deeper into AWS Identity and Access Management and how it can be applied to Amazon SageMaker to create preventive controls. In the lab you will also update existing Service Catalog products to enforce your new best practice.\nLet\u0026rsquo;s dive in.\n"
},
{
	"uri": "/security_for_sysops/preventive/preventive_controls.html",
	"title": "Preventive Controls",
	"tags": [],
	"description": "",
	"content": "Amazon SageMaker, like other services, is goverened by Identity and Access Management (IAM). You configure IAM using policy documents which explicitly grant permissions to your environment.\nAWS IAM is based upon the concepts of Principals, Actions, Resources, and Conditions (PARC). This allows you to specify, using IAM policies who (principals) can do what (actions) to which resources and under what circumstances (conditions). Conditions are a powerful part of IAM policies and gives you the ability to control aspects of the actions being taken by principals in your environment.\nUsing the language of principals, actions, resources, and conditions you have a rich language to configure preventive controls that govern your environment. Preventive controls can stop an action from ever succeeding. However, as part of Defense-in-Depth, you should also create corrective and detective controls.\nAmazon SageMaker provides numerous conditions that allow you to control the aspects of the many SageMaker actions your users will need. In this lab you will use some of these controls to stop data scientists from creating non-compliant training jobs. We won\u0026rsquo;t go into the full list of conditions which Amazon SageMaker makes available, but let\u0026rsquo;s take a look at some of the conditions available in the context of the machine learning lifecycle.\nAccessing a SageMaker notebook To access an Amazon SageMaker Jupyter notebook instance you need permissions to call CreatePresignedNotebookInstanceUrl. This action creates a pre-signed URL which grants access to the notebook instance. Both the SageMaker API call and the resulting URL are governed by the IAM conditions associated with the action. This means that you can leverage an IAM policy such as the following to restrict from which IP address someone in your environment can access the notebook instance.\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:StartNotebookInstance\u0026#34;, \u0026#34;sagemaker:StopNotebookInstance\u0026#34;, \u0026#34;sagemaker:DescribeNotebookInstance\u0026#34;, \u0026#34;sagemaker:CreatePresignedNotebookInstanceUrl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;192.0.2.0/24\u0026#34;, \u0026#34;203.0.113.0/24\u0026#34; ] }, \u0026#34;StringLike\u0026#34;: { \u0026#34;sagemaker:ResourceTag/owner\u0026#34;: \u0026#34;${aws:userid}\u0026#34; } } } The snippet above will allow someone to start, stop, and access a SageMaker Jupyter notebook as long as they do so from a specific IP CIDR range and if the notebook has been tagged with their user as the owner.\nRequiring VPC attachment Actions for creating SageMaker resources such as notebooks, training jobs, and machine learning models can also have IAM conditions associated with them. This gives you the ability to require that principals create these resources with a VPC configuration.\n\u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringEqualsIfExists\u0026#34;: { \u0026#34;sagemaker:VpcSubnets\u0026#34;: [ \u0026#34;subnet-0de4766fbf8d2ea38\u0026#34;, \u0026#34;subnet-0c974f191edc71a33\u0026#34; ], \u0026#34;sagemaker:VpcSecurityGroupIds\u0026#34;: [ \u0026#34;sg-02dc127e01d6b1407\u0026#34; ] } } The condition statement above specifies that IF an action has subnets and security groups configured the values provided must be within an approved set of values. In particular someone creating a Jupyter notebook instance, for example, would have to specify at least one of the subnets above and the security group specified. If any provided values were outside of the specified set the condition would evaluate to False and the permission be denied.\nAnother condition is to ensure that if a VPC configuration CAN be provided that it IS provided. You can do this using the Null condition.\n\u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;sagemaker:VpcSubnets\u0026#34;: \u0026#34;true\u0026#34; } } The condition above evaluates to True if there is no subnet configuration specified for an API call. This will enforce that any principal invoking the SageMaker action provides a VPC configuration and the previous condition will ensure that the configuration is within an acceptable set of values.\nA complete list of conditions and the SageMaker actions they are associated with can be found in the Amazon SageMaker documentation.\nVPC Endpoint policies In addition to associating IAM policies with users, groups, or roles you can also use the same policy language to create VPC endpoint policies. Endpoint policies control what actions can be taken using a VPC endpoint. Amazon SageMaker is accessible via VPC endpoints and so you can further enforce policies at the endpoint level to govern how users in your environments can access the SageMaker APIs. This adds to your ability to create defense in depth and ensure best practice in your environment.\nNow that you understand IAM policies and the conditions you can use to control user permissions, work through this lab to use IAM policy language to create a preventive control that will lead to an improved developer experience. Recall that as a data scientist you were waiting for the training job to start and eventually be stopped by the remediating detective control. On repeat this could be a slow and unpleasant way to discover you made an error. Use a preventive control to provide clear and immediate feedback to a developer to let them know when a mistake has been made.\n"
},
{
	"uri": "/security_for_users.html",
	"title": "Use Secure Environments",
	"tags": [],
	"description": "",
	"content": "In this module you will be introduced to the recommended practices for using Amazon SageMaker in a secure data science environment. Like many other AWS services, Amazon SageMaker is secure by default. Throughout this workshop you will see how you can work in a secured data science environment. You will cover the many stages of the machine learning lifecycle and be provided with Jupyter notebooks to step through that lifecycle while maintaining a high bar for security.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "/security_for_users/environment.html",
	"title": "Deploy a Secure Environment",
	"tags": [],
	"description": "",
	"content": "In this module you will use AWS services to quickly deploy a secure environment for a data science project. After deploying the environment you will also create a SageMaker notebook for use throughout the remainder of this module.\nPlease know that this module creates the same environment that was created in the Building Secure Environments module. If you have worked through that module already and have a secured data science environment, you can reuse that environment here and can skip ahead to Secure Data Science Notebooks.\nLet\u0026rsquo;s get building.\n"
},
{
	"uri": "/security_for_sysops/preventive/preventive_lab.html",
	"title": "Lab 5: Preventive Controls",
	"tags": [],
	"description": "",
	"content": "In this lab, you will implement a preventive control that will stop a training job from starting if it\u0026rsquo;s not launched within a VPC. In the interest of defence in depth you will implement the preventive control to complement the detective control exercised in the previous lab.\nThe preventive control you will deploy here is a modification to the IAM policy which grants a user\u0026rsquo;s notebook the permission to launch a training job. Every user\u0026rsquo;s notebook has an IAM role created for it and an IAM policy attached to this role. Updating these policies individually is not feasible at scale so use the ability of AWS Service Catalog to do the heavy lifting for you.\nCreate an updated version of the Jupyter notebook product to deliver the updated IAM policy to user\u0026rsquo;s notebooks. Resume the role of the Project Administration team and visit the AWS Service Catalog console. Drill into the SageMaker notebook product and provide an updated teamplate for this product using the appropriate URL from the list below. When you have a new version of the SageMaker notebook product created return to the role of the project team member and update your notebook to the latest revision.\nIreland (eu-west-1)\nhttps://s3.eu-west-1.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-1/quickstart/ds_notebook_v2.yaml\nLondon (eu-west-2)\nhttps://s3.eu-west-2.amazonaws.com/sagemaker-workshop-cloudformation-eu-west-2/quickstart/ds_notebook_v2.yaml\nSydney (ap-southeast-2)\nhttps://s3.ap-southeast-2.amazonaws.com/sagemaker-workshop-cloudformation-ap-southeast-2/quickstart/ds_notebook_v2.yaml\nOregon (us-west-2)\nhttps://s3.us-west-2.amazonaws.com/sagemaker-workshop-cloudformation-us-west-2/quickstart/ds_notebook_v2.yaml\nN. Virginia (us-east-1)\nhttps://s3.us-east-1.amazonaws.com/sagemaker-workshop-cloudformation-us-east-1/quickstart/ds_notebook_v2.yaml\nOhio (us-east-2)\nhttps://s3.us-east-2.amazonaws.com/sagemaker-workshop-cloudformation-us-east-2/quickstart/ds_notebook_v2.yaml\nStep-by-step instructions\rThe preventive control will be a modified IAM policy associated with the exeuction role of the SageMaker notebook instance. To modify the role you first need to assume the role of the Data Science Administrator. Next click Products under Administration on the left-hand navigation bar. Click the link for the SageMaker notebook product you want to update. On the product detail page click Create new version. Click Use a CloudFormation Template Paste the appropriate URL for your region below into the CloudFormation template field. Type Version 2 for the Version title. Click Create product version. Navigate to Provisioned products Resume the role of a data science project team member and navigate to Service Catalog\u0026rsquo;s Provisioned Products console Click the context menu next to your notebook and select Update provisioned product Click the radio button next to Version 2 and click Next. Click Next and then Update and wait for the notebook\u0026rsquo;s permissions to be updated. After the product has been successfully updated revist the Jupyter notebook kernel and execute the cell titled Train Without VPC Configured. You should now quickly receive an Access Denied exception similar to the below:\nClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::012348485732:assumed-role/SageMakerExecRole-ml-product-team/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:eu-west-1:012348485732:training-job/sagemaker-tensorflow-2019-10-16-22-14-30-880 with an explicit deny In this lab you modified the permissions granted to the instances of the data science team\u0026rsquo;s Jupyter notebooks. This altered their permissions so that they could only perform actions like creating a training job if that action met the security requirement of specifying a VPC configuration. Visit the defined products using the Service Catalog console to review the CloudFormation template and the changes it made to the permissions. Or use the IAM console to review the role you created and the policy attached to it. What conditions are on the IAM policy controlling access to the SageMaker API?\n"
},
{
	"uri": "/security_for_users/notebook.html",
	"title": "Secure Data Science Notebooks",
	"tags": [],
	"description": "",
	"content": "You are now connected to a Jupyter notebook server running within a secured data science environment. From the notebook you have access to a centralized, hosted PyPI mirror and select buckets in Amazon S3. There is no open access to the Internet or unnecessary AWS services to safeguard against data exfiltration. Using this environment you will now work through the various stages of the machine learning lifecycle, from data exploration, through training, and finally to hosting and monitoring.\nLet\u0026rsquo;s get started.\n"
},
{
	"uri": "/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleanup "
},
{
	"uri": "/conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Conclusion "
},
{
	"uri": "/security_for_sysops/summary.html",
	"title": "Summary",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s look back on what you\u0026rsquo;ve accomplished during the labs in this section.\nIn this series of labs you used many AWS services to support your machine learning process. As the Cloud Platform Engineering team you created a secure network environment with corrective controls for the data science administration team. You also created a self-service product that codified best practices and allowed the Data Science Administration team to support the data science project teams without needing expansive permissions in AWS.\nAs the Data Science Administration team you used the enviornment and product created by the CPE to support the Data Science project team. You created an IAM role for the data science team, an encryption key, and Amazon S3 buckets for the project team to store their data and models. Finally you created a self-service product that the data science team could use to provision Jupyter notebooks for themselves, also without needing extensive permissions into AWS.\nThen, as a data scientist and a member of the project team, you created a Jupyter notebook instance and trained a machine learning model in accordance with your security best practice. This demonstrated the corrective control deployed and managed by the Cloud Platform Engineering team as well as the preventive control later deployed by the Data Science Administration team.\nThis module relied on the following key services that you should now be familiar with:\nAWS Service Catalog AWS Key Management Service (KMS) Amazon S3 Amazon SageMaker AWS Identity and Access Management (IAM) AWS CloudFormation Amazon Virtual Private Cloud (VPC) To find out more about any of these services please visit their documentation or visit AWS Security Workshops for more great security-focused workshops.\n"
},
{
	"uri": "/security_for_users/summary.html",
	"title": "Summary",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s look back on what you\u0026rsquo;ve accomplished during the labs in this section.\nIn this series of labs you used many AWS services to support your machine learning process. As the Cloud Platform Engineering team you created a secure network environment with corrective controls for the data science administration team. You also created a self-service product that codified best practices and allowed the Data Science Administration team to support the data science project teams without needing expansive permissions in AWS.\nAs the Data Science Administration team you used the enviornment and product created by the CPE to support the Data Science project team. You created an IAM role for the data science team, an encryption key, and Amazon S3 buckets for the project team to store their data and models. Finally you created a self-service product that the data science team could use to provision Jupyter notebooks for themselves, also without needing extensive permissions into AWS.\nThen, as a data scientist and a member of the project team, you created a Jupyter notebook instance and trained a machine learning model in accordance with your security best practice. This demonstrated the corrective control deployed and managed by the Cloud Platform Engineering team as well as the preventive control later deployed by the Data Science Administration team.\nThis module relied on the following key services that you should now be familiar with:\nAWS Service Catalog AWS Key Management Service (KMS) Amazon S3 Amazon SageMaker AWS Identity and Access Management (IAM) AWS CloudFormation Amazon Virtual Private Cloud (VPC) To find out more about any of these services please visit their documentation or visit AWS Security Workshops for more great security-focused workshops.\n"
},
{
	"uri": "/security_for_sysops/faq.html",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "The following is a compiled list of questions or issues you may face while going through these labs. If you encounter any issues, hopefully you will find helpful guidance below.\nQ: I am trying to execute a cell in Jupyter Notebook but nothing happens.\nA: Check whether the kernel has finished loading. You can see kernel status in the upper right corner of the Notebook. For the purpose of this workshop, the kernel should be conda_tensorflow_p27. If the kernel has not loaded, give it a couple of minutes to load. If it still says No Kernel, then navigate to the Jupyter menu bar and select Kernel â†’ Restart Kernel. Select conda_tensorflow_p27 if prompted for the kernel type.\nQ: Training job completes successfully, but I see all sorts of errors and warnings in the logs such as â€œNo response body. Response code: 404â€ and â€œIf the signature check failed. This could be because of a time skew. Attempting to adjust the signer.â€\nA: You may ignore these informational log messages for the purpose of this workshop.\nQ: Training job or hosting endpoint deployment terminate in error â€œResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit \u0026lsquo;ml.m4.xlarge for endpoint usage\u0026rsquo; is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.â€\nA: Try to launch a new training job with another instance type (see SageMaker instance types https://aws.amazon.com/sagemaker/pricing/instance-types/). If error persists, contact workshop support staff or your AWS account team.\nQ: In Lab 2, we use AWS Service Catalog to create a team IAM role and CloudFormation to deploy the rest of the team resources including S3 buckets, KMS key, etc. Why not do everything in Service Catalog?\nA: One reason is educational, to get exposure to both services. Another reason is that creating an IAM role requires privileged permissions to IAM service. Service Catalog provides a great option for delegating IAM role creation in a safe manner without having to grant the Data Science Admin permission to the underlying IAM service.\n"
},
{
	"uri": "/security_for_users/faq.html",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "The following is a compiled list of questions or issues you may face while going through these labs. If you encounter any issues, hopefully you will find helpful guidance below.\nQ: I am trying to execute a cell in Jupyter Notebook but nothing happens.\nA: Check whether the kernel has finished loading. You can see kernel status in the upper right corner of the Notebook. For the purpose of this workshop, the kernel should be conda_tensorflow_p27. If the kernel has not loaded, give it a couple of minutes to load. If it still says No Kernel, then navigate to the Jupyter menu bar and select Kernel â†’ Restart Kernel. Select conda_tensorflow_p27 if prompted for the kernel type.\nQ: Training job completes successfully, but I see all sorts of errors and warnings in the logs such as â€œNo response body. Response code: 404â€ and â€œIf the signature check failed. This could be because of a time skew. Attempting to adjust the signer.â€\nA: You may ignore these informational log messages for the purpose of this workshop.\nQ: Training job or hosting endpoint deployment terminate in error â€œResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit \u0026lsquo;ml.m4.xlarge for endpoint usage\u0026rsquo; is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.â€\nA: Try to launch a new training job with another instance type (see SageMaker instance types https://aws.amazon.com/sagemaker/pricing/instance-types/). If error persists, contact workshop support staff or your AWS account team.\nQ: In Lab 2, we use AWS Service Catalog to create a team IAM role and CloudFormation to deploy the rest of the team resources including S3 buckets, KMS key, etc. Why not do everything in Service Catalog?\nA: One reason is educational, to get exposure to both services. Another reason is that creating an IAM role requires privileged permissions to IAM service. Service Catalog provides a great option for delegating IAM role creation in a safe manner without having to grant the Data Science Admin permission to the underlying IAM service.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/more_resources.html",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": "Discover more AWS resources for building and running your application on AWS:\nMore Workshops Amazon EKS Workshop - This workshop guides you through the process of setting up and using a Kubernetes cluster on AWS Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop. Amplify Web App Workshop - Build a Photo-Sharing Web App with AWS Amplify and AWS AppSync Cloud Development Kit Workshop - Build a Photo-Sharing Web App with AWS Amplify and AWS AppSync Tools for AWS Sagemaker AWS Samples - Samples and more workshops "
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]